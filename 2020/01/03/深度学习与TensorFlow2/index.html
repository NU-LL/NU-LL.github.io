<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  <title>深度学习与TensorFlow2 | 无名小卒</title>
  <meta name="keywords" content=" TensorFlow2 , 深度学习 ">
  <meta name="description" content="深度学习与TensorFlow2 | 无名小卒">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="关于">
<meta property="og:type" content="website">
<meta property="og:title" content="about">
<meta property="og:url" content="http://NU-LL.github.io/about/index.html">
<meta property="og:site_name" content="无名小卒">
<meta property="og:description" content="关于">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-07-21T14:30:10.192Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="about">
<meta name="twitter:description" content="关于">


<link rel="icon" href="/img/avatar.jpg">

<link href="/css/style.css?v=1" rel="stylesheet">

<link href="/css/hl_theme/github.css?v=1" rel="stylesheet">

<link href="//cdn.bootcss.com/animate.css/3.5.2/animate.min.css" rel="stylesheet">
<link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="/js/jquery.autocomplete.min.js?v=1"></script>

<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>



<script src="//cdn.bootcss.com/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>

<script src="/js/iconfont.js?v=1"></script>

</head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="false">
  <input class="theme_blog_path" value>
</div>

<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/" class="avatar_target">
    <img class="avatar" src="/img/avatar.jpg" />
</a>
<div class="author">
    <span>NU-LL</span>
</div>

<div class="icon">
    
        
    
        
        <a title="github" href="https://github.com/NU-LL" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-github"></use>
                </svg>
            
        </a>
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
        <a title="csdn" href="https://blog.csdn.net/CSDN_JZ_" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-csdn"></use>
                </svg>
            
        </a>
        
    
        
    
        
    
        
        <a title="email" href="mailto:1125934312@qq.com" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-email"></use>
                </svg>
            
        </a>
        
    
        
        <a title="qq" href="http://wpa.qq.com/msgrd?v=3&uin=1125934312&site=qq&menu=yes" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-qq"></use>
                </svg>
            
        </a>
        
    
        
    
        
        <a title="neteasemusic" href="https://music.163.com/#/user/home?id=323374922" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-neteasemusic"></use>
                </svg>
            
        </a>
        
    
</div>




<ul>
    <li><div class="all active">全部文章<small>(44)</small></div></li>
    
        
            
            <li><div data-rel="编程语言">编程语言<small>(6)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="MCU">MCU<small>(3)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="Linux">Linux<small>(25)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="工具">工具<small>(4)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="人工智能">人工智能<small>(3)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="感悟与总结">感悟与总结<small>(2)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="算法">算法<small>(1)</small></div>
                
            </li>
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
    
    
    
    
    </div>
    <div><a style="border-right: 1px solid #fff; width: 49%"  class="about site_url"  href="/about">关于</a><a style="width: 50%"  class="friends">友链</a></div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="44">
<input type="hidden" id="yelog_site_word_count" value="288.7k">
<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="back-title-list"></i>
    </div>
    <div class="friends-content">
        <ul>
            
            <li><a target="_blank" href="https://github.com/NU-LL">NU-LL</a></li>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <form onkeydown="if(event.keyCode==13){return false;}">
        <input class="search" type="text" placeholder="Search..." autocomplete="off"id="local-search-input" >
        <i class="cross"></i>
        <span>
            <label for="tagswitch">Tags:</label>
            <input id="tagswitch" type="checkbox" style="display: none" />
            <i id="tagsWitchIcon"></i>
        </span>
    </form>
    <div class="tags-list">
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">GO</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color1">ESP32</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color2">Docker</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">DM9000C</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">网卡移植</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">ESP8266</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color1">Latex</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">Ctex</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color1">IIC驱动</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">Linux服务</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">Linux内核</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color1">文件描述符</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">LiCheePi Zero</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">Kconfig语法</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">Mininet</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color1">NanoPi Neo Core</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">SVM</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color2">TensorFlow2</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">Markdown</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">Xmanager</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">远程链接</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">字符驱动</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">数据分析</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">等待队列</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">wait_queue_head_t</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">wait_queue_t</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">network namespace</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">markdownlint</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">git</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color1">Kubernetes</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color2">python</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">爬虫</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">python实战</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">存储器</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">二维数组</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">指针</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color2">u-boot</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">IAP</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color1">BootLoader</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">dual bank</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">按键驱动</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color2">poll机制</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color2">异步通知机制</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color1">根文件系统</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">快速排序</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">设备树</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color2">网卡驱动框架</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">虚拟网卡</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color1">输入子系统</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">Go语言入门经典</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color1">驱动API</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">深度学习</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color2">廖雪峰python教程</a>
    </li>
    
    <div class="clearfix"></div>
</div>

    
    <nav id="title-list-nav">
        
        <a  class="编程语言 "
           href="/2020/05/09/C和GO的语法对比/"
           data-tag="GO"
           data-author="" >
            <span class="post-title" title="C和GO的语法对比">C和GO的语法对比</span>
            <span class="post-date" title="2020-05-09 19:55:39">2020/05/09</span>
        </a>
        
        <a  class="MCU "
           href="/2020/05/16/ESP32/"
           data-tag="ESP32"
           data-author="" >
            <span class="post-title" title="ESP8266">ESP8266</span>
            <span class="post-date" title="2020-05-16 10:30:54">2020/05/16</span>
        </a>
        
        <a  class="Linux "
           href="/2020/01/16/Docker/"
           data-tag="Docker"
           data-author="" >
            <span class="post-title" title="Docker">Docker</span>
            <span class="post-date" title="2020-01-16 20:14:51">2020/01/16</span>
        </a>
        
        <a  class="Linux "
           href="/2019/08/04/DM9000C网卡移植/"
           data-tag="DM9000C,网卡移植"
           data-author="" >
            <span class="post-title" title="DM9000C网卡移植">DM9000C网卡移植</span>
            <span class="post-date" title="2019-08-04 23:07:22">2019/08/04</span>
        </a>
        
        <a  class="MCU "
           href="/2020/04/28/ESP8266/"
           data-tag="ESP8266"
           data-author="" >
            <span class="post-title" title="ESP8266">ESP8266</span>
            <span class="post-date" title="2020-04-28 14:27:54">2020/04/28</span>
        </a>
        
        <a  class="工具 "
           href="/2019/08/19/Latex排版全解/"
           data-tag="Latex,Ctex"
           data-author="" >
            <span class="post-title" title="Latex排版全解">Latex排版全解</span>
            <span class="post-date" title="2019-08-19 16:02:27">2019/08/19</span>
        </a>
        
        <a  class="Linux "
           href="/2019/08/05/IIC驱动/"
           data-tag="IIC驱动"
           data-author="" >
            <span class="post-title" title="IIC驱动">IIC驱动</span>
            <span class="post-date" title="2019-08-05 14:39:10">2019/08/05</span>
        </a>
        
        <a  class="Linux "
           href="/2020/03/28/Linux上常见服务搭建/"
           data-tag="Linux服务"
           data-author="" >
            <span class="post-title" title="Linux上常见服务搭建">Linux上常见服务搭建</span>
            <span class="post-date" title="2020-03-28 01:27:58">2020/03/28</span>
        </a>
        
        <a  class="Linux "
           href="/2019/08/13/Linux内核启动流程/"
           data-tag="Linux内核"
           data-author="" >
            <span class="post-title" title="Linux内核启动流程">Linux内核启动流程</span>
            <span class="post-date" title="2019-08-13 16:13:46">2019/08/13</span>
        </a>
        
        <a  class="Linux "
           href="/2019/07/21/Linux编程--文件描述符fd/"
           data-tag="文件描述符"
           data-author="" >
            <span class="post-title" title="Linux编程--文件描述符fd">Linux编程--文件描述符fd</span>
            <span class="post-date" title="2019-07-21 10:13:47">2019/07/21</span>
        </a>
        
        <a  class="Linux "
           href="/2019/10/18/LiCheePi_Zero底层开发/"
           data-tag="LiCheePi Zero"
           data-author="" >
            <span class="post-title" title="LiCheePi_Zero底层开发">LiCheePi_Zero底层开发</span>
            <span class="post-date" title="2019-10-18 12:25:42">2019/10/18</span>
        </a>
        
        <a  class="Linux "
           href="/2019/08/15/Kconfig文件语法分析/"
           data-tag="Kconfig语法"
           data-author="" >
            <span class="post-title" title="Kconfig文件语法分析">Kconfig文件语法分析</span>
            <span class="post-date" title="2019-08-15 19:38:40">2019/08/15</span>
        </a>
        
        <a  class="Linux "
           href="/2020/02/13/Mininet/"
           data-tag="Mininet"
           data-author="" >
            <span class="post-title" title="Mininet">Mininet</span>
            <span class="post-date" title="2020-02-13 18:37:12">2020/02/13</span>
        </a>
        
        <a  class="Linux "
           href="/2019/10/18/NanoPi_Neo_Core底层开发/"
           data-tag="NanoPi Neo Core"
           data-author="" >
            <span class="post-title" title="NanoPi_Neo_Core底层开发">NanoPi_Neo_Core底层开发</span>
            <span class="post-date" title="2019-10-18 12:25:42">2019/10/18</span>
        </a>
        
        <a  class="人工智能 "
           href="/2020/06/05/SVM/"
           data-tag="SVM"
           data-author="" >
            <span class="post-title" title="SVM（线性模型）数学推导">SVM（线性模型）数学推导</span>
            <span class="post-date" title="2020-06-05 21:12:53">2020/06/05</span>
        </a>
        
        <a  class="人工智能 "
           href="/2020/05/03/TensorFlow2/"
           data-tag="TensorFlow2"
           data-author="" >
            <span class="post-title" title="TensorFlow2入门与实践">TensorFlow2入门与实践</span>
            <span class="post-date" title="2020-05-03 00:52:53">2020/05/03</span>
        </a>
        
        <a  class="工具 "
           href="/2019/07/12/Markdown 语法整理/"
           data-tag="Markdown"
           data-author="" >
            <span class="post-title" title="Markdown 语法整理">Markdown 语法整理</span>
            <span class="post-date" title="2019-07-12 13:57:24">2019/07/12</span>
        </a>
        
        <a  class="Linux "
           href="/2019/11/08/Xmanager远程Ubuntu系统图像化界面/"
           data-tag="Xmanager,远程链接"
           data-author="" >
            <span class="post-title" title="Xmanager远程Ubuntu系统图像化界面">Xmanager远程Ubuntu系统图像化界面</span>
            <span class="post-date" title="2019-11-08 22:58:53">2019/11/08</span>
        </a>
        
        <a  class="Linux "
           href="/2019/08/07/RTC驱动分析/"
           data-tag="字符驱动"
           data-author="" >
            <span class="post-title" title="RTC驱动分析">RTC驱动分析</span>
            <span class="post-date" title="2019-08-07 13:37:33">2019/08/07</span>
        </a>
        
        <a  class="编程语言 "
           href="/2020/03/10/Python数据分析与展示/"
           data-tag="数据分析"
           data-author="" >
            <span class="post-title" title="Python数据分析与展示">Python数据分析与展示</span>
            <span class="post-date" title="2020-03-10 15:10:50">2020/03/10</span>
        </a>
        
        <a  class="Linux "
           href="/2019/07/21/linux等待队列wait_queue_head_t和wait_queue_t/"
           data-tag="等待队列,wait_queue_head_t,wait_queue_t"
           data-author="" >
            <span class="post-title" title="linux等待队列wait_queue_head_t和wait_queue_t">linux等待队列wait_queue_head_t和wait_queue_t</span>
            <span class="post-date" title="2019-07-21 18:18:54">2019/07/21</span>
        </a>
        
        <a  class="Linux "
           href="/2020/02/10/linux 网络虚拟化： network namespace 简介/"
           data-tag="network namespace"
           data-author="" >
            <span class="post-title" title="linux 网络虚拟化： network namespace 简介">linux 网络虚拟化： network namespace 简介</span>
            <span class="post-date" title="2020-02-10 14:24:09">2020/02/10</span>
        </a>
        
        <a  class="工具 "
           href="/2019/09/17/markdownlint规则详细介绍/"
           data-tag="markdownlint"
           data-author="" >
            <span class="post-title" title="VSC插件之markdownlint规则详细介绍">VSC插件之markdownlint规则详细介绍</span>
            <span class="post-date" title="2019-09-17 15:00:13">2019/09/17</span>
        </a>
        
        <a  class="工具 "
           href="/2019/09/17/git/"
           data-tag="git"
           data-author="" >
            <span class="post-title" title="git">git</span>
            <span class="post-date" title="2019-09-17 09:59:56">2019/09/17</span>
        </a>
        
        <a  class="Linux "
           href="/2020/03/15/kubernetes/"
           data-tag="Kubernetes"
           data-author="" >
            <span class="post-title" title="Kubernetes">Kubernetes</span>
            <span class="post-date" title="2020-03-15 13:48:53">2020/03/15</span>
        </a>
        
        <a  class="编程语言 "
           href="/2020/02/18/python爬虫/"
           data-tag="python,爬虫"
           data-author="" >
            <span class="post-title" title="python爬虫">python爬虫</span>
            <span class="post-date" title="2020-02-18 23:40:18">2020/02/18</span>
        </a>
        
        <a  class="编程语言 "
           href="/2020/01/29/python实战/"
           data-tag="python实战"
           data-author="" >
            <span class="post-title" title="python实战">python实战</span>
            <span class="post-date" title="2020-01-29 21:36:32">2020/01/29</span>
        </a>
        
        <a  class="感悟与总结 "
           href="/2019/08/11/各种存储器的区别/"
           data-tag="存储器"
           data-author="" >
            <span class="post-title" title="各种存储器的区别">各种存储器的区别</span>
            <span class="post-date" title="2019-08-11 10:57:19">2019/08/11</span>
        </a>
        
        <a  class="感悟与总结 "
           href="/2019/08/06/二维数组与指针的一些问题/"
           data-tag="二维数组,指针"
           data-author="" >
            <span class="post-title" title="二维数组与指针的一些问题">二维数组与指针的一些问题</span>
            <span class="post-date" title="2019-08-06 13:11:26">2019/08/06</span>
        </a>
        
        <a  class="Linux "
           href="/2019/08/12/u-boot分析与使用/"
           data-tag="u-boot"
           data-author="" >
            <span class="post-title" title="u-boot分析与使用">u-boot分析与使用</span>
            <span class="post-date" title="2019-08-12 19:16:50">2019/08/12</span>
        </a>
        
        <a  class="MCU "
           href="/2019/11/12/基于STM32L476的IAP升级/"
           data-tag="IAP,BootLoader,dual bank"
           data-author="" >
            <span class="post-title" title="基于STM32L476的IAP升级">基于STM32L476的IAP升级</span>
            <span class="post-date" title="2019-11-12 22:47:40">2019/11/12</span>
        </a>
        
        <a  class="Linux "
           href="/2019/07/21/按键驱动——poll机制/"
           data-tag="按键驱动,poll机制"
           data-author="" >
            <span class="post-title" title="按键驱动——poll机制">按键驱动——poll机制</span>
            <span class="post-date" title="2019-07-21 20:30:38">2019/07/21</span>
        </a>
        
        <a  class="Linux "
           href="/2019/07/22/按键驱动：异步通知机制/"
           data-tag="按键驱动,异步通知机制"
           data-author="" >
            <span class="post-title" title="按键驱动：异步通知机制">按键驱动：异步通知机制</span>
            <span class="post-date" title="2019-07-22 19:04:24">2019/07/22</span>
        </a>
        
        <a  class="Linux "
           href="/2019/08/14/构造根文件系统/"
           data-tag="根文件系统"
           data-author="" >
            <span class="post-title" title="构造根文件系统">构造根文件系统</span>
            <span class="post-date" title="2019-08-14 20:18:45">2019/08/14</span>
        </a>
        
        <a  class="Linux "
           href="/2019/08/07/字符驱动设备的另一种写法/"
           data-tag="字符驱动"
           data-author="" >
            <span class="post-title" title="字符驱动设备的另一种写法">字符驱动设备的另一种写法</span>
            <span class="post-date" title="2019-08-07 12:20:30">2019/08/07</span>
        </a>
        
        <a  class="算法 "
           href="/2019/07/29/快速排序/"
           data-tag="快速排序"
           data-author="" >
            <span class="post-title" title="快速排序">快速排序</span>
            <span class="post-date" title="2019-07-29 22:27:34">2019/07/29</span>
        </a>
        
        <a  class="Linux "
           href="/2019/10/13/设备树/"
           data-tag="设备树"
           data-author="" >
            <span class="post-title" title="设备树">设备树</span>
            <span class="post-date" title="2019-10-13 15:53:24">2019/10/13</span>
        </a>
        
        <a  class="Linux "
           href="/2019/08/04/网卡驱动程序/"
           data-tag="网卡驱动框架,虚拟网卡"
           data-author="" >
            <span class="post-title" title="网卡驱动程序">网卡驱动程序</span>
            <span class="post-date" title="2019-08-04 21:24:07">2019/08/04</span>
        </a>
        
        <a  class="Linux "
           href="/2019/07/25/输入子系统/"
           data-tag="按键驱动,输入子系统"
           data-author="" >
            <span class="post-title" title="输入子系统">输入子系统</span>
            <span class="post-date" title="2019-07-25 00:05:15">2019/07/25</span>
        </a>
        
        <a  class="编程语言 "
           href="/2019/10/23/Go语言/"
           data-tag="Go语言入门经典"
           data-author="" >
            <span class="post-title" title="Go语言">Go语言</span>
            <span class="post-date" title="2019-10-23 21:07:25">2019/10/23</span>
        </a>
        
        <a  class="Linux "
           href="/2020/04/06/Linux驱动常用API整理/"
           data-tag="驱动API"
           data-author="" >
            <span class="post-title" title="Linux驱动常用API整理">Linux驱动常用API整理</span>
            <span class="post-date" title="2020-04-06 22:48:09">2020/04/06</span>
        </a>
        
        <a  class="Linux "
           href="/2019/08/12/u-boot分析与使用/u-boot分析与使用 - 老版本备份/"
           data-tag="u-boot"
           data-author="" >
            <span class="post-title" title="u-boot分析与使用">u-boot分析与使用</span>
            <span class="post-date" title="2019-08-12 19:16:50">2019/08/12</span>
        </a>
        
        <a  class="人工智能 "
           href="/2020/01/03/深度学习与TensorFlow2/"
           data-tag="TensorFlow2,深度学习"
           data-author="" >
            <span class="post-title" title="深度学习与TensorFlow2">深度学习与TensorFlow2</span>
            <span class="post-date" title="2020-01-03 21:04:26">2020/01/03</span>
        </a>
        
        <a  class="编程语言 "
           href="/2020/01/10/python/"
           data-tag="廖雪峰python教程"
           data-author="" >
            <span class="post-title" title="Python">Python</span>
            <span class="post-date" title="2020-01-10 20:18:52">2020/01/10</span>
        </a>
        
    </nav>
</div>
    </div>
    <div class="hide-list">
        <div class="semicircle">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div class="post">
    <div class="pjax">
        <article id="post-深度学习与TensorFlow2" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">深度学习与TensorFlow2</h1>
    
    <div class="article-meta">
        
        
        
        <span class="book">
            
                <a href="javascript:" data-rel="人工智能">人工智能</a>
            
        </span>
        
        
        <span class="tag">
            
            <a href="javascript:" class="color2">TensorFlow2</a>
            
            <a href="javascript:" class="color5">深度学习</a>
            
        </span>
        
    </div>
    <div class="article-meta">
        
        创建时间:<time class="date" title='更新时间: 2020-05-13 16:30:26'>2020-01-03 21:04</time>
        
    </div>
    <div class="article-meta">
        
        <span>字数:32.9k</span>
        
        
        <span id="busuanzi_container_page_pv">
            阅读:<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#深度学习与TensorFlow2"><span class="toc-text">深度学习与TensorFlow2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#TensorFlow2基础操作"><span class="toc-text">TensorFlow2基础操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、数据类型"><span class="toc-text">1、数据类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-数值类型"><span class="toc-text">1.数值类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-字符串类型"><span class="toc-text">2.字符串类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-布尔类型"><span class="toc-text">3.布尔类型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、数值精度"><span class="toc-text">2、数值精度</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-读取精度"><span class="toc-text">1.读取精度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-类型转换"><span class="toc-text">2.类型转换</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、待优化张量"><span class="toc-text">3、待优化张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4、创建张量"><span class="toc-text">4、创建张量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-从数组、列表中创建"><span class="toc-text">1.从数组、列表中创建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-创建全0全1张量"><span class="toc-text">2.创建全0全1张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-创建自定义数值张量"><span class="toc-text">3.创建自定义数值张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-创建已知分布的张量"><span class="toc-text">4.创建已知分布的张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-创建序列"><span class="toc-text">5.创建序列</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5、张量的典型用途"><span class="toc-text">5、张量的典型用途</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6、索引与切片"><span class="toc-text">6、索引与切片</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-索引"><span class="toc-text">1.索引</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-切片"><span class="toc-text">2.切片</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7、维度变换"><span class="toc-text">7、维度变换</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-改变视图-reshape"><span class="toc-text">1.改变视图 reshape</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-增删维度"><span class="toc-text">2.增删维度</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#增加维度"><span class="toc-text">增加维度</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#删除维度"><span class="toc-text">删除维度</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-交换维度"><span class="toc-text">3.交换维度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-复制数据"><span class="toc-text">4.复制数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8、Broadcasting"><span class="toc-text">8、Broadcasting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9、数学运算"><span class="toc-text">9、数学运算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-加减乘除"><span class="toc-text">1.加减乘除</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-乘方"><span class="toc-text">2.乘方</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-指数和对数"><span class="toc-text">3.指数和对数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-矩阵乘法"><span class="toc-text">4.矩阵乘法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TensorFlow2进阶操作"><span class="toc-text">TensorFlow2进阶操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、合并与分割"><span class="toc-text">1、合并与分割</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-合并"><span class="toc-text">1.合并</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#拼接"><span class="toc-text">拼接</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#堆叠"><span class="toc-text">堆叠</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-分割"><span class="toc-text">2.分割</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、数据统计"><span class="toc-text">2、数据统计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-向量范数"><span class="toc-text">1.向量范数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-最值、均值、和"><span class="toc-text">2.最值、均值、和</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、张量比较"><span class="toc-text">3、张量比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4、复制和填充"><span class="toc-text">4、复制和填充</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-填充"><span class="toc-text">1.填充</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-复制"><span class="toc-text">2.复制</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5、数据限幅"><span class="toc-text">5、数据限幅</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6、高级操作"><span class="toc-text">6、高级操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-tf-gather"><span class="toc-text">1.tf.gather</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-tf-gather-nd"><span class="toc-text">2.tf.gather_nd</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-tf-boolean-mask"><span class="toc-text">3.tf.boolean_mask</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-tf-where"><span class="toc-text">4.tf.where</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#例子"><span class="toc-text">例子</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-scatter-nd"><span class="toc-text">5.scatter_nd</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-meshgrid"><span class="toc-text">6.meshgrid</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7、经典数据集加载"><span class="toc-text">7、经典数据集加载</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-随机打散"><span class="toc-text">1.随机打散</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-批训练"><span class="toc-text">2.批训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-预处理"><span class="toc-text">3.预处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-循环训练"><span class="toc-text">4.循环训练</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8、MNIST实战"><span class="toc-text">8、MNIST实战</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络"><span class="toc-text">神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、感知机"><span class="toc-text">1、感知机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、全连接层"><span class="toc-text">2、全连接层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-张量方式实现"><span class="toc-text">1.张量方式实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-层方式实现"><span class="toc-text">2.层方式实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、神经网络"><span class="toc-text">3、神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-张量方式实现-1"><span class="toc-text">1.张量方式实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-层方式实现-1"><span class="toc-text">2.层方式实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-优化目标"><span class="toc-text">3.优化目标</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4、激活函数"><span class="toc-text">4、激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Sigmoid"><span class="toc-text">1.Sigmoid</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-ReLU"><span class="toc-text">2.ReLU</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-LeakyReLU"><span class="toc-text">3.LeakyReLU</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-Tanh"><span class="toc-text">4.Tanh</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5、输出层设计"><span class="toc-text">5、输出层设计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-普通实数空间"><span class="toc-text">1.普通实数空间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-0-1-区间"><span class="toc-text">2.[0,1]区间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-0-1-区间，和为1"><span class="toc-text">3.[0,1]区间，和为1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1-区间"><span class="toc-text">4.[-1,1]区间</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6、误差计算"><span class="toc-text">6、误差计算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-均方差误差函数"><span class="toc-text">1.均方差误差函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-交叉熵误差函数"><span class="toc-text">2.交叉熵误差函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7、神经网络类型"><span class="toc-text">7、神经网络类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-卷积神经网络"><span class="toc-text">1.卷积神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-循环神经网络"><span class="toc-text">2.循环神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-注意力（机制）网络"><span class="toc-text">3.注意力（机制）网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-图卷积神经网络"><span class="toc-text">4.图卷积神经网络</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Keras高层接口"><span class="toc-text">Keras高层接口</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、常见功能模块"><span class="toc-text">1、常见功能模块</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-常见网络层类"><span class="toc-text">1.常见网络层类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-网络容器"><span class="toc-text">2.网络容器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、模型装配、训练与测试"><span class="toc-text">2、模型装配、训练与测试</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-模型装配"><span class="toc-text">1.模型装配</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-模型训练"><span class="toc-text">2.模型训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-模型测试"><span class="toc-text">3.模型测试</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、模型保存与加载"><span class="toc-text">3、模型保存与加载</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-张量方式"><span class="toc-text">1.张量方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-网络方式"><span class="toc-text">2.网络方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-SavedModel方式"><span class="toc-text">3.SavedModel方式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4、自定义网络"><span class="toc-text">4、自定义网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-自定义网络层"><span class="toc-text">1.自定义网络层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-自定义网络"><span class="toc-text">2.自定义网络</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5、模型乐园"><span class="toc-text">5、模型乐园</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-加载模型"><span class="toc-text">1.加载模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6、测量工具"><span class="toc-text">6、测量工具</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-新建测量器"><span class="toc-text">1.新建测量器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-写入数据"><span class="toc-text">2.写入数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-读取统计信息"><span class="toc-text">3.读取统计信息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-清除状态"><span class="toc-text">4.清除状态</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-准确率统计实战"><span class="toc-text">5.准确率统计实战</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7、可视化"><span class="toc-text">7、可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-模型端"><span class="toc-text">1.模型端</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-浏览器端"><span class="toc-text">2.浏览器端</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#过拟合"><span class="toc-text">过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、模型的容量"><span class="toc-text">1、模型的容量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、过拟合与欠拟合"><span class="toc-text">2、过拟合与欠拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-欠拟合"><span class="toc-text">1.欠拟合</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-过拟合"><span class="toc-text">2.过拟合</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、数据集划分"><span class="toc-text">3、数据集划分</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-验证集与超参数"><span class="toc-text">1.验证集与超参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-提前停止"><span class="toc-text">2.提前停止</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4、模型设计"><span class="toc-text">4、模型设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5、正则化"><span class="toc-text">5、正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6、Dropout"><span class="toc-text">6、Dropout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7、数据增强"><span class="toc-text">7、数据增强</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-旋转"><span class="toc-text">1.旋转</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-翻转"><span class="toc-text">2.翻转</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-裁剪"><span class="toc-text">3.裁剪</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-生成数据"><span class="toc-text">4.生成数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-其他方式"><span class="toc-text">5.其他方式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8、过拟合问题"><span class="toc-text">8、过拟合问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-数据集构建"><span class="toc-text">1.数据集构建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-网络层数的影响"><span class="toc-text">2.网络层数的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Dropout的影响"><span class="toc-text">3.Dropout的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-正则化的影响"><span class="toc-text">4.正则化的影响</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积神经网络"><span class="toc-text">卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、全连接层的问题"><span class="toc-text">1、全连接层的问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-局部相关性"><span class="toc-text">1.局部相关性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-劝值共享"><span class="toc-text">2.劝值共享</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-卷积运算"><span class="toc-text">3.卷积运算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、卷积神经网络"><span class="toc-text">2、卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-单通道输入和单卷积核"><span class="toc-text">1.单通道输入和单卷积核</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-多通道输入和单卷积核"><span class="toc-text">2.多通道输入和单卷积核</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-多通道输入、多卷积核"><span class="toc-text">3.多通道输入、多卷积核</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-步长"><span class="toc-text">4.步长</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-填充"><span class="toc-text">5.填充</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、卷积层实现"><span class="toc-text">3、卷积层实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-自定义权值"><span class="toc-text">1.自定义权值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-卷积层类"><span class="toc-text">2.卷积层类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4、LeNet-5实战"><span class="toc-text">4、LeNet-5实战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5、表示学习"><span class="toc-text">5、表示学习</span></a></li></ol></li></ol></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-6 i,
    .toc-level-6 ol {
        display: none !important;
    }
</style>
</div>
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="深度学习与TensorFlow2"><a href="#深度学习与TensorFlow2" class="headerlink" title="深度学习与TensorFlow2"></a>深度学习与TensorFlow2</h1><p>环境（版本需要匹配）：</p>
<ul>
<li>TensorFlow：TensorFlow2.0 GPU版本</li>
<li>Anaconda：4.8.1（conda -V，conda -list，Python 3.7 version），链接：<a href="https://www.anaconda.com/distribution/#download-section" target="_blank" rel="noopener">https://www.anaconda.com/distribution/#download-section</a></li>
<li>CUDA：V10.0.130（nvcc -V），链接：<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a><ul>
<li>cuDNN：，链接：<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-archive</a></li>
</ul>
</li>
</ul>
<h2 id="TensorFlow2基础操作"><a href="#TensorFlow2基础操作" class="headerlink" title="TensorFlow2基础操作"></a>TensorFlow2基础操作</h2><p>TensorFlow中的数据载体叫做张量（Tensor）对象，即<code>tf.Tensor</code>，对应不同的类型，能够存储大量的连续的数据。同时所有的运算操作(Operation，简称 OP)也都是基于张量对象进行的</p>
<p>什么是Tensor：</p>
<ul>
<li>Tensor是一个比较广泛的数据</li>
<li>标量（scalar）：1.1、2.2等准确的数据类型（维度dim=0）</li>
<li>向量（vector）：[1.1]、[1.1,2.2,…]（dim=1）</li>
<li>矩阵（matrix）：[[1.1,2.2],[2.2,2.2],[3.3,2.2]]</li>
<li>数学上tensor：一般指维度&gt;2时的数据，但是在TensorFlow中维度&gt;=1时的数据全称为tensor，甚至标量也可以看作是tensor，所以工程上讲tensor一般指所有的数据</li>
</ul>
<h3 id="1、数据类型"><a href="#1、数据类型" class="headerlink" title="1、数据类型"></a>1、数据类型</h3><h4 id="1-数值类型"><a href="#1-数值类型" class="headerlink" title="1.数值类型"></a>1.数值类型</h4><p>数值类型的张量是 TensorFlow 的主要数据载体， 根据维度数来区分，可分为：</p>
<ul>
<li><p>标量(Scalar)：单个的实数，如 1.2, 3.4 等，维度(Dimension)数为 0， shape 为[]</p>
</li>
<li><p>向量(Vector)：单个实数的有序集合，通过中括号包裹，如[1.2]， [1.2, 3.4]等，维度数为 1，长度不定， shape 为[n]</p>
</li>
<li><p>矩阵(Matrix)：n行m列实数的有序集合，如[[1,2], [3,4]]，也可以写成$\begin{bmatrix}<br>  1 &amp; 2\<br>  3 &amp; 4<br>  \end{bmatrix}$，维度数为 2，每个维度上的长度不定， shape 为[n,m]</p>
</li>
<li><p>张量(Tensor)：所有维度数dim &gt; 2的数组统称为张量。 张量的每个维度也作轴(Axis)，一般来说，维度代表了具体的物理含义，张量的维度数以及每个维度所代表的具体物理含义需要由用户自行定义。</p>
<blockquote>
<p>比如 Shape 为[2,32,32,3]的张量共有 4 维，如果表示图片数据的话，每个维度/轴代表的含义分别是图片数量、 图片高度、 图片宽度、 图片通道数，其中 2 代表了 2 张图片， 32 代表了高、 宽均为 32， 3 代表了 RGB 共 3 个通道</p>
</blockquote>
</li>
</ul>
<p>在 TensorFlow 中，为了表达方便，一般把标量、向量、矩阵也统称为张量，不作区分，需要根据张量的维度数或形状自行判断</p>
<p>张量的创建：</p>
<pre><code class="python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; aa = tf.constant(1.2) # TF方式创建标量
&gt;&gt;&gt; x = tf.constant([1,2.,3.3]) # TF方式创建向量
&gt;&gt;&gt; x
&lt;tf.Tensor: id=0, shape=(3,), dtype=float32, numpy=array([1. , 2. , 3.3],dtype=float32)&gt;
&gt;&gt;&gt; a = tf.constant([[1,2],[3,4]]) # TF方式创建2行2列的矩阵
&gt;&gt;&gt; a
&lt;tf.Tensor: id=1, shape=(2, 2), dtype=int32, numpy=
array([[1, 2],
       [3, 4]])&gt;
&gt;&gt;&gt; a = tf.constant([[[1,2],[3,4]],[[5,6],[7,8]]])# TF方式创建3维张量
&gt;&gt;&gt; a
&lt;tf.Tensor: id=2, shape=(2, 2, 2), dtype=int32, numpy=
array([[[1, 2],
        [3, 4]],

       [[5, 6],
        [7, 8]]])&gt;</code></pre>
<ul>
<li>id：TensorFlow 中内部索引对象的编号</li>
<li>shape：张量的形状</li>
<li>dtype：张量的数值精度</li>
</ul>
<blockquote>
<p>张量可以通过 numpy()方法可以返回 Numpy.array 类型的数据，方便导出数据到系统的其他模块</p>
<pre><code class="python">&gt;&gt;&gt; x.numpy() # 将 TF 张量的数据导出为 numpy 数组格式
array([1. , 2. , 3.3], dtype=float32)</code></pre>
</blockquote>
<h4 id="2-字符串类型"><a href="#2-字符串类型" class="headerlink" title="2.字符串类型"></a>2.字符串类型</h4><p>使用频率较低。通过传入字符串对象即可创建字符串类型的张量：</p>
<pre><code class="python">&gt;&gt;&gt; a = tf.constant(&#39;Hello, Deep Learning.&#39;) # 创建字符串
&gt;&gt;&gt; a
&lt;tf.Tensor: id=3, shape=(), dtype=string, numpy=b&#39;Hello, Deep Learning.&#39;&gt;</code></pre>
<p>在 <code>tf.strings</code> 模块中，提供了常见的字符串类型的工具函数，如小写化 lower()、 拼接join()、 长度 length()、 切分 split()等</p>
<blockquote>
<p>将字符串全部小写化：</p>
<pre><code class="python">&gt;&gt;&gt; tf.strings.lower(a) # 小写化字符串
&lt;tf.Tensor: id=19, shape=(), dtype=string, numpy=b&#39;hello, deep learning.&#39;&gt;</code></pre>
</blockquote>
<h4 id="3-布尔类型"><a href="#3-布尔类型" class="headerlink" title="3.布尔类型"></a>3.布尔类型</h4><p>布尔类型的张量需要传入 Python 语言的布尔类型数据，转换成 TensorFlow 内部布尔型即可：</p>
<pre><code class="python">&gt;&gt;&gt; a = tf.constant(True) # 创建布尔类型标量
&lt;tf.Tensor: id=22, shape=(), dtype=bool, numpy=True&gt;
&gt;&gt;&gt; a = tf.constant([True, False]) # 创建布尔类型向量
&lt;tf.Tensor: id=25, shape=(2,), dtype=bool, numpy=array([ True, False])&gt;</code></pre>
<ul>
<li>TensorFlow 的布尔类型和 Python 语言的布尔类型并不等价，不能通用</li>
</ul>
<h3 id="2、数值精度"><a href="#2、数值精度" class="headerlink" title="2、数值精度"></a>2、数值精度</h3><p>保存的数据位越长，精度越高，同时占用的内存空间也就越大。常用的精度类型有<code>tf.int16</code>、<code>tf.int32</code>、<code>tf.int64</code>、<code>tf.float16</code>、<code>tf.float32</code>、<code>tf.float64</code>等，其中<code>tf.float64</code>即为<code>tf.double</code></p>
<p>在创建张量时，可以指定张量的保存精度：</p>
<pre><code class="python">&gt;&gt;&gt; tf.constant(123456789, dtype=tf.int16)
&gt;&gt;&gt; tf.constant(123456789, dtype=tf.int32)</code></pre>
<p>对于大部分深度学习算法，一般使用 tf.int32 和 tf.float32 可满足大部分场合的运算精<br>度要求，部分对精度要求较高的算法，如强化学习某些算法，可以选择使用 tf.int64 和<br>tf.float64 精度保存张量</p>
<h4 id="1-读取精度"><a href="#1-读取精度" class="headerlink" title="1.读取精度"></a>1.读取精度</h4><p>通过访问张量的<code>dtype</code>成员属性可以判断张量的保存精度</p>
<pre><code class="python">print(&#39;before:&#39;,a.dtype) # 读取原有张量的数值精度
if a.dtype != tf.float32: # 如果精度不符合要求，则进行转换
    a = tf.cast(a,tf.float32) # tf.cast 函数可以完成精度转换
print(&#39;after :&#39;,a.dtype) # 打印转换后的精度</code></pre>
<h4 id="2-类型转换"><a href="#2-类型转换" class="headerlink" title="2.类型转换"></a>2.类型转换</h4><p>通常通过<code>tf.cast</code>函数进行转换：</p>
<pre><code class="python">&gt;&gt;&gt; a = tf.constant(np.pi, dtype=tf.float16) # 创建 tf.float16 低精度张量
&gt;&gt;&gt; tf.cast(a, tf.double) # 转换为高精度张量
&lt;tf.Tensor: id=44, shape=(), dtype=float64, numpy=3.140625&gt;</code></pre>
<p>布尔类型与整型之间相互转换也是合法的：</p>
<pre><code class="python">&gt;&gt;&gt; a = tf.constant([True, False])
&gt;&gt;&gt; tf.cast(a, tf.int32) # 布尔类型转整型
&lt;tf.Tensor: id=48, shape=(2,), dtype=int32, numpy=array([1, 0])&gt;</code></pre>
<ul>
<li>一般默认 0 表示 False， 1 表示 True，在 TensorFlow 中，将<strong>非 0 数字都视为 True</strong></li>
</ul>
<h3 id="3、待优化张量"><a href="#3、待优化张量" class="headerlink" title="3、待优化张量"></a>3、待优化张量</h3><p>为了区分需要计算梯度信息的张量与不需要计算梯度信息的张量， TensorFlow 增加了<br>一种专门的数据类型来<u>支持梯度信息的记录</u>：<code>tf.Variable</code></p>
<p><code>tf.Variable</code>类型在普通的张量类型基础上添加了 <code>name</code>， <code>trainable</code> 等属性来支持计算图的构建。由于梯度运算会消耗大量的计算资源，而且会自动更新相关参数，对于不需要的优化的张量，如神经网络的输入<strong>X</strong>，不需要通过 tf.Variable 封装；相反，对于<u>需要计算梯度并优化</u>的张量， 如神经网络层的<strong>W</strong>和<strong>b</strong>，需要通过 tf.Variable 包裹以便TensorFlow 跟踪相关梯度信息。</p>
<p>通过<code>tf.Variable()</code>函数可以将普通张量转换为待优化张量：</p>
<pre><code class="python">&gt;&gt;&gt; a = tf.constant([-1, 0, 1, 2]) # 创建 TF 张量
&gt;&gt;&gt; aa = tf.Variable(a) # 转换为 Variable 类型
&gt;&gt;&gt; aa.name, aa.trainable # Variable 类型张量的属性
(&#39;Variable:0&#39;, True)</code></pre>
<ul>
<li>name 属性：命名计算图中的变量，这套命名体系是 TensorFlow 内部维护的， 一般不需要用户关注 name 属性</li>
<li>trainable属性：当前张量是否需要被优化（创建 Variable 对象时默认启用优化标志，可以设置trainable=False来设置张量不需要优化）</li>
</ul>
<p>直接创建待优化张量：</p>
<pre><code class="python">&gt;&gt;&gt; a = tf.Variable([[1,2],[3,4]]) # 直接创建 Variable 张量
&lt;tf.Variable &#39;Variable:0&#39; shape=(2, 2) dtype=int32, numpy=
array([[1, 2],
       [3, 4]])&gt;</code></pre>
<p>待优化张量可视为普通张量的特殊类型， 普通张量其实也可以通过<code>GradientTape.watch()</code>方法临时加入跟踪梯度信息的列表，从而支持自动求导功能</p>
<h3 id="4、创建张量"><a href="#4、创建张量" class="headerlink" title="4、创建张量"></a>4、创建张量</h3><h4 id="1-从数组、列表中创建"><a href="#1-从数组、列表中创建" class="headerlink" title="1.从数组、列表中创建"></a>1.从数组、列表中创建</h4><p>通过<code>tf.convert_to_tensor</code>函数可以创建新 Tensor，并将保存在 Python List 对象或者Numpy Array 对象中的数据导入到新 Tensor 中：</p>
<pre><code class="python">&gt;&gt;&gt; tf.convert_to_tensor([1,2.]) # 从列表创建张量
&lt;tf.Tensor: id=86, shape=(2,), dtype=float32, numpy=array([1., 2.],
dtype=float32)&gt;
&gt;&gt;&gt; tf.convert_to_tensor(np.array([[1,2.],[3,4]])) # 从数组中创建张量
&lt;tf.Tensor: id=88, shape=(2, 2), dtype=float64, numpy=
array([[1., 2.],
       [3., 4.]])&gt;</code></pre>
<ul>
<li>Numpy 浮点数数组默认使用 64 位精度保存数据，可以在需要的时候将其转换为 tf.float32 类型</li>
<li>实际上， tf.constant()和 tf.convert_to_tensor()都能够自动的把 Numpy 数组或者 Python列表数据类型转化为 Tensor 类型，这两个 API 命名来自 TensorFlow 1.x 的命名习惯，在TensorFlow 2 中函数的名字并不是很贴切，使用其一即可</li>
</ul>
<h4 id="2-创建全0全1张量"><a href="#2-创建全0全1张量" class="headerlink" title="2.创建全0全1张量"></a>2.创建全0全1张量</h4><p>通过<code>tf.zeros()</code>和<code>tf.ones()</code>可创建任意形状，且内容全 0 或全 1 的张量：</p>
<pre><code class="python">&gt;&gt;&gt; tf.zeros([]),tf.ones([]) # 创建全 0，全 1 的标量
(&lt;tf.Tensor: id=90, shape=(), dtype=float32, numpy=0.0&gt;,
&lt;tf.Tensor: id=91, shape=(), dtype=float32, numpy=1.0&gt;)

&gt;&gt;&gt; tf.zeros([1]),tf.ones([1]) # 创建全 0，全 1 的向量
(&lt;tf.Tensor: id=96, shape=(1,), dtype=float32, numpy=array([0.],
dtype=float32)&gt;,
&lt;tf.Tensor: id=99, shape=(1,), dtype=float32, numpy=array([1.],
dtype=float32)&gt;)

&gt;&gt;&gt; tf.ones([3,2]) # 创建全 1 矩阵，指定 shape 为 3 行 2 列
&lt;tf.Tensor: id=108, shape=(3, 2), dtype=float32, numpy=
array([[1., 1.],
       [1., 1.],
       [1., 1.]], dtype=float32)&gt;</code></pre>
<p>通过<code>tf.zeros_like</code>、<code>tf.ones_like</code>可以方便地新建与某个张量 shape 一致， 且内容为全 0 或全 1 的张量：</p>
<pre><code class="python">&gt;&gt;&gt; a = tf.ones([2,3]) # 创建一个矩阵
&gt;&gt;&gt; tf.zeros_like(a) # 创建一个与 a 形状相同，但是全 0 的新矩阵
&lt;tf.Tensor: id=113, shape=(2, 3), dtype=float32, numpy=
array([[0., 0., 0.],
       [0., 0., 0.]], dtype=float32)&gt;</code></pre>
<ul>
<li><code>tf.*_like</code>是一系列的便捷函数，可以通过<code>tf.zeros(a.shape)</code>等方式实现</li>
</ul>
<h4 id="3-创建自定义数值张量"><a href="#3-创建自定义数值张量" class="headerlink" title="3.创建自定义数值张量"></a>3.创建自定义数值张量</h4><p>通过<code>tf.fill(shape, value)</code>可以创建全为自定义数值 value 的张量，形状由 shape 参数指定：</p>
<pre><code class="python">&gt;&gt;&gt; tf.fill([2,2], 99) # 创建 2 行 2 列，元素全为 99 的矩阵
&lt;tf.Tensor: id=136, shape=(2, 2), dtype=int32, numpy=
array([[99, 99],
       [99, 99]])&gt;</code></pre>
<h4 id="4-创建已知分布的张量"><a href="#4-创建已知分布的张量" class="headerlink" title="4.创建已知分布的张量"></a>4.创建已知分布的张量</h4><p>通过<code>tf.random.normal(shape, mean=0.0, stddev=1.0)</code>可以创建形状为 shape，均值为mean，标准差为 stddev 的<strong>正态分布</strong>：</p>
<pre><code class="python">&gt;&gt;&gt; tf.random.normal([2,2]) # 创建标准正态分布的张量
&lt;tf.Tensor: id=143, shape=(2, 2), dtype=float32, numpy=
array([[-0.4307344 , 0.44147003],
       [-0.6563149 , -0.30100572]], dtype=float32)&gt;</code></pre>
<p>通过<code>tf.random.uniform(shape, minval=0, maxval=None, dtype=tf.float32)</code>可以创建采样自[minval, maxval)区间的<strong>均匀分布</strong>的张量：</p>
<pre><code class="python">&gt;&gt;&gt; tf.random.uniform([2,2]) # 创建采样自[0,1)均匀分布的矩阵
&lt;tf.Tensor: id=158, shape=(2, 2), dtype=float32, numpy=
array([[0.65483284, 0.63064325],
       [0.008816 , 0.81437767]], dtype=float32)&gt;</code></pre>
<h4 id="5-创建序列"><a href="#5-创建序列" class="headerlink" title="5.创建序列"></a>5.创建序列</h4><p><code>tf.range(limit, delta=1)</code>可以创建[0, limit)之间，步长为 delta 的整型序列，不包含 limit 本身：</p>
<pre><code class="python">&gt;&gt;&gt; tf.range(10) # 0~10，不包含 10
&lt;tf.Tensor: id=180, shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;</code></pre>
<p>通过<code>tf.range(start, limit, delta=1)</code>可以创建[start, limit)，步长为 delta 的序列，不包含limit 本身</p>
<pre><code class="python">&gt;&gt;&gt; tf.range(1,10,delta=2) # 1~10
&lt;tf.Tensor: id=190, shape=(5,), dtype=int32, numpy=array([1, 3, 5, 7, 9])&gt;</code></pre>
<h3 id="5、张量的典型用途"><a href="#5、张量的典型用途" class="headerlink" title="5、张量的典型用途"></a>5、张量的典型用途</h3><p>标量：误差值的表示、 各种测量指标的表示，比如准确度(Accuracy，简称 acc)，精度(Precision)和召回率(Recall)等</p>
<p>向量：如在全连接层和卷积神经网络层中，偏置张量b就是使用向量来表示</p>
<p>矩阵：比如全连接层的批量输入张量X的形状为[b,din]，其中b表示输入样本的个数，即Batch Size，din表示输入特征的长度</p>
<p>三维张量：表示序列信号，它的格式是$X=[b,sequence,feature\ len]$，其中b表示序列信号的数量， sequence len 表示序列信号在时间维度上的采样点数或步数，feature len 表示每个点的特征长度，自然语言处理(Natural Language Processing，简称 NLP)中会使用到</p>
<p>四维张量：卷积神经网络中应用广泛，用于保存特征图数据，格式一般为$[b,h,w,c]$，其中b表示输入样本的数量， h/w 分别表示特征图的高/宽，c表示特征图的通道数。部分深度学习框架（如PyTorch）会使用$[b,c,h,w]$格式   </p>
<p>大于四维的张量一般应用的比较少，如在元学习(Meta Learning)中会采用五维的张量表示方法，理解方法与三、四维张量类似</p>
<h3 id="6、索引与切片"><a href="#6、索引与切片" class="headerlink" title="6、索引与切片"></a>6、索引与切片</h3><h4 id="1-索引"><a href="#1-索引" class="headerlink" title="1.索引"></a>1.索引</h4><p>在 TensorFlow 中， 支持基本的$[i][j]…$标准索引方式，也支持通过逗号分隔索引号的索引方式</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.random.normal([4,32,32,3]) # 创建 4D 张量（4张32*32大小的彩色图片）
&gt;&gt;&gt; x[0] # 程序中的第一的索引号应为 0，容易混淆，不过不影响理解
&lt;tf.Tensor: id=379, shape=(32, 32, 3), dtype=float32, numpy=
array([[[ 1.3005302 , 1.5301839 , -0.32005513],
        [-1.3020388 , 1.7837263 , -1.0747638 ], ...
        [-1.1092019 , -1.045254 , -0.4980363 ],
        [-0.9099222 , 0.3947732 , -0.10433522]]], dtype=float32)&gt;
&gt;&gt;&gt; x[2][1][0][1] # 取第 3 张图片，第 2 行，第 1 列的像素， B 通道(第 2 个通道)颜色强度值
&lt;tf.Tensor: id=418, shape=(), dtype=float32, numpy=-0.84922135&gt;</code></pre>
<p>当张量的维度数较高时， 使用$[i][j]…[k]$的方式书写不方便，可以采用$[i,j,…,k]$方式索引，他们是等价的：</p>
<pre><code class="python">&gt;&gt;&gt; x[1,9,2] # 取第 2 张图片，第 10 行，第 3 列的数据，实现如下
&lt;tf.Tensor: id=436, shape=(3,), dtype=float32, numpy=array([ 1.7487534 , -0.41491988, -0.2944692 ], dtype=float32)&gt;</code></pre>
<h4 id="2-切片"><a href="#2-切片" class="headerlink" title="2.切片"></a>2.切片</h4><p>通过<code>start:end:step</code>切片方式可以方便地提取一段数据，其中 start 为开始读取位置的索引， end 为结束读取位置的索引(不包含 end 位)， step 为采样步长</p>
<pre><code class="python">&gt;&gt;&gt; x[1:3] # 读取第 2,3 张图片
&lt;tf.Tensor: id=441, shape=(2, 32, 32, 3), dtype=float32, numpy=
array([[[[ 0.6920027 , 0.18658352, 0.0568333 ],
         [ 0.31422952, 0.75933754, 0.26853144],
         [ 2.7898 , -0.4284912 , -0.26247284],...</code></pre>
<p><code>start:end:step</code>切片方式有很多简写方式</p>
<ul>
<li>全部省略时即为<code>::</code>， 表示从最开始读取到最末尾，步长为 1（不跳过任何元素）。为了更加简洁， <code>::</code>可以简写为单个冒号<code>:</code></li>
<li>从第一个元素读取时<code>start</code>可以省略（即start=0可以省略）</li>
<li>取到最后一个元素时<code>end</code>可以省略</li>
<li>步长为 1 时<code>step</code>可以省略</li>
</ul>
<p>特别地，<code>step</code>可以为负数，当step = -1时，<code>start:end:-1</code>表示从 start 开始， 逆序读取至 end 结束(不包含 end)，索引号$end\leqslant start$</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.range(9) # 创建 0~9 向量
&gt;&gt;&gt; x[8:0:-1] # 从 8 取到 0，逆序，不包含 0
&lt;tf.Tensor: id=466, shape=(8,), dtype=int32, numpy=array([8, 7, 6, 5, 4, 3, 2, 1])&gt;
&gt;&gt;&gt; x = tf.random.normal([4,32,32,3])
&gt;&gt;&gt; x[0,::-2,::-2] # 行、列逆序间隔采样
&lt;tf.Tensor: id=487, shape=(16, 16, 3), dtype=float32, numpy=
array([[[ 0.63320625, 0.0655185 , 0.19056146],
        [-1.0078577 , -0.61400175, 0.61183935],
        [ 0.9230892 , -0.6860094 , -0.01580668],
        ...</code></pre>
<p>为了避免出现像 [: , : , : ,1]这样过多冒号的情况，可以使用<code>...</code>符号表示取多个维度上所有的数据， 其中维度的数量需根据规则自动推断：</p>
<ul>
<li>当切片方式出现<code>...</code>符号时，<code>...</code>符号左边的维度将自动对齐到最左边</li>
<li><code>...</code>符号右边的维度将自动对齐到最右边，此时系统再自动推断<code>...</code>符号代表的维度数量</li>
</ul>
<pre><code class="python">&gt;&gt;&gt; x = tf.random.normal([4,32,32,3])
&gt;&gt;&gt; x[0:2,...,1:] # 高宽维度全部采集
&lt;tf.Tensor: id=497, shape=(2, 32, 32, 2), dtype=float32, numpy=
array([[[[ 0.575703 , 0.8872789 ],
         [ 0.11028383, -0.27128693],
         [-0.9950867 , -1.7737272 ],
         ...</code></pre>
<h3 id="7、维度变换"><a href="#7、维度变换" class="headerlink" title="7、维度变换"></a>7、维度变换</h3><p>算法的每个模块对于数据张量的格式有不同的逻辑要求，当现有的数据格式不满足算<br>法要求时，需要通过维度变换将数据调整为正确的格式。这就是维度变换的功能。</p>
<p>基本的维度变换操作函数包含了改变视图 <code>reshape</code>、 插入新维度 <code>expand_dims</code>，删除维度 <code>squeeze</code>、 交换维度 <code>transpose</code>、 复制数据 <code>tile</code> 等函数</p>
<p>Batch 维度：为了实现维度变换，我们需要将原始数据插入一个新的维度，并把它定义为 Batch 维度，然后在 Batch 维度对数据进行相关操作，得到变换后的新的数据。这一系列的操作就是维度变换操作。</p>
<h4 id="1-改变视图-reshape"><a href="#1-改变视图-reshape" class="headerlink" title="1.改变视图 reshape"></a>1.改变视图 reshape</h4><p>张量的视图（View）：就是我们理解张量的方式，比如 shape 为[2,4,4,3]的张量A，从逻辑上可以理解为 2 张图片，每张图片 4 行 4 列，每个位置有 RGB 3 个通道的数据</p>
<p>张量的存储（Storage）：体现在张量在内存上保存为一段连续的内存区域，对于同样的存储，我们可以有不同的理解方式，比如上述张量A，我们可以在不改变张量的存储下，将张量A理解为 2个样本，每个样本的特征为长度 48 的向量</p>
<p>同一个存储，从不同的角度观察数据，可以产生不同的视图， 这就是存储与视图的关系。 视图的产生是非常灵活的，但需要保证是合理。</p>
<p>通过 <code>tf.range()</code>模拟生成一个向量数据，并通过<code>tf.reshape</code>视图改变函数产生不同的视图：</p>
<pre><code class="python">&gt;&gt;&gt; x=tf.range(96) # 生成向量
&gt;&gt;&gt; x=tf.reshape(x,[2,4,4,3]) # 改变 x 的视图，获得 4D 张量，存储并未改变
# 可以观察到数据仍然是 0~95 的顺序，可见数据并未改变，改变的是数据的结构
&lt;tf.Tensor: id=11, shape=(2, 4, 4, 3), dtype=int32, numpy=
array([[[[ 0, 1, 2],
         [ 3, 4, 5],
         [ 6, 7, 8],
         [ 9, 10, 11]],...</code></pre>
<p>在存储数据时，内存只能以平铺方式按序写入内存，因此视图的层级关系需要人为管理。为了方便表达，一般把张量 shape 列表中相对靠左侧的维度叫作大维度， shape 列表中相对靠右侧的维度叫作小维度（如[2,4,4,3]的张量中，图片数量维度与通道数量相比，图片数量叫作大维度，通道数叫作小维度）</p>
<p>改变视图操作在提供便捷性的同时，也会带来很多逻辑隐患，主要的原因是改变视图操作的默认前提是<u>存储不需要改变</u>，否则改变视图操作就是非法的</p>
<blockquote>
<p>张量A按着初始视图[b,h,w,c]写入的内存布局，改变A的理解方式，它可以有多种合法的理解方式：</p>
<ul>
<li>[b,h∙w,c]张量理解为b张图片，h∙w个像素点，c个通道</li>
<li>[b,h,w∙c]张量理解为b张图片，h行，每行的特征长度为w∙c</li>
<li>[b,h∙w∙c]张量理解为b张图片，每张图片的特征长度为h∙w∙c</li>
</ul>
<p>从语法上来说， 视图变换只需要满足新视图的元素总量与存储区域大小相等即可。正是由于视图的设计的语法约束很少，使得在改变视图时容易出现逻辑隐患。</p>
<p>不合法的视图变换：</p>
<p>例如，如果定义新视图为[b,w,h,c]，[b,c,h*w]或者[b,c,h,w]等时，张量的存储顺序需要改变， 如果不同步更新张量的存储顺序，那么恢复出的数据将与新视图不一致，从而导致数据错乱。</p>
<p>这需要用户理解数据，才能判断操作是否合法。我们会在“交换维度”一节介绍如何改变张量的存储</p>
</blockquote>
<p>在通过 reshape 改变视图时，必须始终记住张量的存储顺序，新视图的维度顺序不能与存储顺序相悖，否则需要通过<strong>交换维度</strong>操作将存储顺序同步过来</p>
<p>在 TensorFlow 中，可以通过张量的 <code>ndim</code> 和 <code>shape</code> 成员属性获得张量的维度数和形<br>状：</p>
<pre><code class="python">&gt;&gt;&gt; x.ndim,x.shape # 获取张量的维度数和形状列表
(4, TensorShape([2, 4, 4, 3]))</code></pre>
<hr>
<p>通过 <code>tf.reshape(x, new_shape)</code>，可以将张量的视图任意地合法改变：</p>
<pre><code class="python">&gt;&gt;&gt; tf.reshape(x,[2,-1])
&lt;tf.Tensor: id=520, shape=(2, 48), dtype=int32, numpy=
array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,…
        80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]])&gt;</code></pre>
<ul>
<li>参数-1：当前轴上长度需要根据张量总元素不变的法则自动推导（该处推导成(2*4*4*3)/2=48）</li>
</ul>
<p>再次改变数据的视图为[2,16,3] ，实现如下：</p>
<pre><code class="python">&gt;&gt;&gt; tf.reshape(x,[2,-1,3])
&lt;tf.Tensor: id=526, shape=(2, 16, 3), dtype=int32, numpy=
array([[[ 0, 1, 2], …
       [45, 46, 47]],
      [[48, 49, 50],…
       [93, 94, 95]]])&gt;</code></pre>
<ul>
<li>上述一系列连续变换视图操作中，张量的存储顺序始终没有改变，数据在内存中仍然是按着初始写入的顺序0,1,2, ⋯ ,95保存</li>
</ul>
<h4 id="2-增删维度"><a href="#2-增删维度" class="headerlink" title="2.增删维度"></a>2.增删维度</h4><h5 id="增加维度"><a href="#增加维度" class="headerlink" title="增加维度"></a>增加维度</h5><p>通过 <code>tf.expand_dims(x, axis)</code>可在指定的 <strong>axis 轴前</strong>可以插入一个新的维度（长度为1）：  </p>
<pre><code class="python">&gt;&gt;&gt; x = tf.random.uniform([28,28],maxval=10,dtype=tf.int32) # 产生矩阵
&lt;tf.Tensor: id=11, shape=(28, 28), dtype=int32, numpy=
array([[6, 2, 0, 0, 6, 7, 3, 3, 6, 2, 6, 2, 9, 3, 0, 3, 2, 8, 1, 3, 6, 2, 3, 9, 3, 6, 1, 7],...
&gt;&gt;&gt; x = tf.expand_dims(x,axis=2) # axis=2 表示宽维度后面的一个维度
&lt;tf.Tensor: id=13, shape=(28, 28, 1), dtype=int32, numpy=
array([[[6],
        [2],
        [0],
        [0],
        [6],
        [7],
        [3],...</code></pre>
<ul>
<li>增加一个长度为 1 的维度相当于给原有的数据添加一个新维度的概念，数据并不发生改变，仅仅是改变数据的理解方式，因此它其实可以理解为改变视图的一种特殊方式</li>
</ul>
<p>需要注意的是， <code>tf.expand_dims</code> 的 axis 为<strong>正</strong>时，表示在当前维度<strong>之前</strong>插入一个新维度； 为<strong>负</strong>时，表示当前维度<strong>之后</strong>插入一个新的维度。以[b,h,w,c]张量为例，不同 axis 参数的实际插入位置如下所示：</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200503172458500.png" alt="增加维度 axis 参数位置示意图"></p>
<h5 id="删除维度"><a href="#删除维度" class="headerlink" title="删除维度"></a>删除维度</h5><p>与增加维度一样，删除维度只能删除<strong>长度为1</strong>的维度，也不会改变张量的存储。</p>
<p>通过 <code>tf.squeeze(x, axis)</code>函数， axis 参数为待删除的维度的索引号：</p>
<pre><code class="python">#数据同上一个例子
&gt;&gt;&gt; x = tf.squeeze(x, axis=2) # 删除图片通道数维度
&lt;tf.Tensor: id=588, shape=(28, 28), dtype=int32, numpy=
array([[8, 2, 2, 0, 7, 0, 1, 4, 9, 1, 7, 4, 8, 2, 7, 4, 8, 2, 9, 8, 8, 0, 9, 9, 7, 5, 9, 7],
       [3, 4, 9, 9, 0, 6, 5, 7, 1, 9, 9, 1, 2, 7, 2, 7, 5, 3, 3, 7, 2, 4, 5, 2, 7, 3, 8, 0],...</code></pre>
<p>如果不指定维度参数 axis，即 <code>tf.squeeze(x)</code>， 那么它会默认删除所有长度为 1 的维度</p>
<h4 id="3-交换维度"><a href="#3-交换维度" class="headerlink" title="3.交换维度"></a>3.交换维度</h4><p>在保持维度顺序不变的条件下， 仅仅改变张量的理解方式是不够的，有时需要直接调整的存储顺序，即<strong>交换维度</strong>(Transpose)。通过交换维度操作，改变了张量的存储顺序，同时也改变了张量的视图</p>
<p>我们以图片格式[b,h,w,c]转换到图片格式[b,c,h,w]为例，介绍使用 <code>tf.transpose(x, perm)</code>函数完成维度交换操作，其中参数 perm表示新维度的顺序 List。</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.random.normal([2,32,32,3])
&gt;&gt;&gt; tf.transpose(x,perm=[0,3,1,2]) # 交换维度
&lt;tf.Tensor: id=603, shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[-1.93072677e+00, -4.80163872e-01, -8.85614634e-01, ...,
          1.49124235e-01, 1.16427064e+00, -1.47740364e+00],
         [-1.94761145e+00, 7.26879001e-01, -4.41877693e-01, ...</code></pre>
<ul>
<li>图片张量 shape 为[2,32,32,3]，“图片数量、行、列、通道数” 的维度索引分别为 0、 1、 2、 3。交换为[b,c,h,w]格式，新维度的排序为“图片数量、通道数、行、列”，对应的索引号为[0,3,1,2]</li>
<li>通过 <code>tf.transpose</code> 维度交换后，张量的存储顺序已经改变， 视图也随之改变， 后续的所有操作必须基于新的存续顺序和视图进行。 相对于改变视图操作，维度交换操作的<strong>计算代价更高</strong></li>
</ul>
<h4 id="4-复制数据"><a href="#4-复制数据" class="headerlink" title="4.复制数据"></a>4.复制数据</h4><p>可以通过<code>tf.tile(x, multiples)</code>函数完成数据在指定维度上的复制操作， multiples 分别指定了每个维度上面的复制倍数，对应位置为 1 表明不复制，为 2 表明新长度为原来长度的2 倍，即数据复制一份，以此类推</p>
<pre><code class="python">&gt;&gt;&gt; b = tf.constant([1,2]) # 创建向量 b
&gt;&gt;&gt; b = tf.expand_dims(b, axis=0) # 插入新维度，变成矩阵
&lt;tf.Tensor: id=645, shape=(1, 2), dtype=int32, numpy=array([[1, 2]])&gt;
# 在 Batch 维度上复制数据 1 份，实现如下：
&gt;&gt;&gt; b = tf.tile(b, multiples=[2,1]) # 样本维度上复制一份
&lt;tf.Tensor: id=648, shape=(2, 2), dtype=int32, numpy=
array([[1, 2],
       [1, 2]])&gt;</code></pre>
<p><code>tf.tile</code> 会创建一个<strong>新的张量</strong>来保存复制后的张量，会涉及大量数据的读写 IO 运算，计算代价相对较高</p>
<h3 id="8、Broadcasting"><a href="#8、Broadcasting" class="headerlink" title="8、Broadcasting"></a>8、Broadcasting</h3><p>Broadcasting 称为广播机制(或自动扩展机制)，它是一种轻量级的张量复制手段，在逻辑上扩展张量数据的形状， 但是只会在需要时才会执行实际存储复制操作。</p>
<p>对于用户来说， Broadcasting 和 tf.tile 复制的最终效果是一样的，操作对用户透明，但是 Broadcasting 机制节省了大量计算资源，建议在运算过程中尽可能地利用 Broadcasting 机制提高计算效率</p>
<p>考虑上述的$Y=X@W+b$的例子， $X@W$的 shape 为[2,3]，b的 shape 为[3]，可以通过结合 tf.expand_dims 和 tf.tile 手动完成复制数据操作，将b变换为[2,3]，然后与 $X@W$完成相加运算。但实际上，直接将 shape 为[2,3]与[3]的b相加也是合法的，例如：</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.random.normal([2,4])
&gt;&gt;&gt; w = tf.random.normal([4,3])
&gt;&gt;&gt; b = tf.random.normal([3])
&gt;&gt;&gt; y = x@w+b # 不同 shape 的张量直接相加</code></pre>
<ul>
<li><p>会自动调用 Broadcasting函数 <code>tf.broadcast_to(x, new_shape)</code>， 将两者 shape 扩张为相同的[2,3]， 即上式可以等效为</p>
<pre><code class="python">  &gt;&gt;&gt; y = x@w + tf.broadcast_to(b,[2,3]) # 手动扩展，并相加</code></pre>
</li>
<li><p>操作符+在遇到 shape 不一致的 2 个张量时，会自动考虑将 2 个张量自动扩展到一致的 shape，然后再调用 tf.add 完成张量相加运算</p>
</li>
</ul>
<p>所有的运算都需要在正确逻辑（满足Broadcasting 设计的核心思想）下进行， Broadcasting 机制并不会扰乱正常的计算逻辑， 它只会针对于最常见的场景自动完成增加维度并复制数据的功能， 提高开发效率和运行效率。</p>
<blockquote>
<p>Broadcasting 机制的核心思想是普适性，即同一份数据能普遍适合于其他位置。 在验证普适性之前，需要先将张量 shape 靠右对齐， 然后进行普适性判断： </p>
<ul>
<li>对于<strong>长度为1</strong>的维度，默认这个数据普遍适合于当前维度的其他位置</li>
<li>对于<strong>不存在</strong>的维度， 则在增加新维度后默认当前数据也是普适于新维度的， 从而可以扩展为更多维度数、 任意长度的张量形状</li>
</ul>
</blockquote>
<p>在进行张量运算时，有些运算在处理不同 shape 的张量时，会隐式地自动调用Broadcasting 机制，如+， -， *， /等运算，将参与运算的张量 Broadcasting 成一个公共shape，再进行相应的计算。  </p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200503181730592.png" alt="加法运算时自动Broadcasting示意图"></p>
<pre><code class="python">&gt;&gt;&gt; a = tf.random.normal([2,32,32,1])
&gt;&gt;&gt; b = tf.random.normal([32,32])
&gt;&gt;&gt; a+b,a-b,a*b,a/b # 测试加减乘除运算的 Broadcasting 机制</code></pre>
<ul>
<li>这些运算都能 Broadcasting 成[2,32,32,32]的公共 shape，再进行运算</li>
</ul>
<h3 id="9、数学运算"><a href="#9、数学运算" class="headerlink" title="9、数学运算"></a>9、数学运算</h3><h4 id="1-加减乘除"><a href="#1-加减乘除" class="headerlink" title="1.加减乘除"></a>1.加减乘除</h4><p>加、 减、 乘、 除是最基本的数学运算，分别通过 <code>tf.add</code>, <code>tf.subtract</code>, <code>tf.multiply</code>, <code>tf.divide</code>函数实现， TensorFlow 已经重载了+、 - 、 ∗ 、 /运算符，推荐直接使用运算符来完成加、 减、 乘、 除运算</p>
<p>整除和余除也是常见的运算之一，分别通过<code>//</code>和<code>%</code>运算符实现</p>
<pre><code class="python">&gt;&gt;&gt; a = tf.range(5)
&gt;&gt;&gt; b = tf.constant(2)
&gt;&gt;&gt; a//b # 整除运算
&lt;tf.Tensor: id=115, shape=(5,), dtype=int32, numpy=array([0, 0, 1, 1, 2])&gt;
&gt;&gt;&gt; a%b # 余除运算
&lt;tf.Tensor: id=117, shape=(5,), dtype=int32, numpy=array([0, 1, 0, 1, 0])&gt;</code></pre>
<h4 id="2-乘方"><a href="#2-乘方" class="headerlink" title="2.乘方"></a>2.乘方</h4><p>通过 <code>tf.pow(x, a)</code>可以方便地完成$y=x^a$的乘方运算，也可以通过运算符<code>**</code>实现<code>x∗∗a</code>运算：</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.range(4)
&gt;&gt;&gt; tf.pow(x,3) # 乘方运算
&lt;tf.Tensor: id=124, shape=(4,), dtype=int32, numpy=array([ 0, 1, 8, 27])&gt;

&gt;&gt;&gt; x**2 # 乘方运算符
&lt;tf.Tensor: id=127, shape=(4,), dtype=int32, numpy=array([0, 1, 4, 9])&gt;

&gt;&gt;&gt; x=tf.constant([1.,4.,9.])
&gt;&gt;&gt; x**(0.5) # 平方根
&lt;tf.Tensor: id=139, shape=(3,), dtype=float32, numpy=array([1., 2., 3.], dtype=float32)&gt;</code></pre>
<ul>
<li>设置指数为$\frac{1}{a}$形式， 即可实现$\sqrt[a]{x}$根号运算</li>
<li>对于常见的平方和平方根运算，可以使用 <code>tf.square(x)</code>和 <code>tf.sqrt(x)</code>实现</li>
</ul>
<h4 id="3-指数和对数"><a href="#3-指数和对数" class="headerlink" title="3.指数和对数"></a>3.指数和对数</h4><p>通过 <code>tf.pow(a, x)</code>或者<code>**</code>运算符也可以方便地实现指数运算$a^x$</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.constant([1.,2.,3.])
&gt;&gt;&gt; 2**x # 指数运算
&lt;tf.Tensor: id=179, shape=(3,), dtype=float32, numpy=array([2., 4., 8.], dtype=float32)&gt;

&gt;&gt;&gt; tf.exp(1.) # 自然指数运算
&lt;tf.Tensor: id=182, shape=(), dtype=float32, numpy=2.7182817&gt;</code></pre>
<ul>
<li>对于自然指数$e^x$， 可以通过 <code>tf.exp(x)</code>实现</li>
</ul>
<p>自然对数$log_ex$可以通过 <code>tf.math.log(x)</code>实现：</p>
<pre><code class="python">&gt;&gt;&gt; x=tf.exp(3.)
&gt;&gt;&gt; tf.math.log(x) # 对数运算
&lt;tf.Tensor: id=186, shape=(), dtype=float32, numpy=3.0&gt;</code></pre>
<ul>
<li><p>如果希望计算其它底数的对数，可以根据对数的换底公式$log_ax=\frac{log_ex}{log_ea}$间接实现</p>
<pre><code class="python">  &gt;&gt;&gt; x = tf.constant([1.,2.])
  &gt;&gt;&gt; x = 10**x
  &gt;&gt;&gt; tf.math.log(x)/tf.math.log(10.) # 换底公式
  &lt;tf.Tensor: id=6, shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)&gt;</code></pre>
</li>
</ul>
<h4 id="4-矩阵乘法"><a href="#4-矩阵乘法" class="headerlink" title="4.矩阵乘法"></a>4.矩阵乘法</h4><p>通过<code>@</code>运算符可以方便的实现矩阵相乘，还可以通过 <code>tf.matmul(a, b)</code>函数实现</p>
<p>需要注意的是， TensorFlow 中的矩阵相乘可以使用批量方式，也就是张量A和B的维度数可以大于 2。当张量A和B维度数大于 2 时， TensorFlow 会选择A和B的最后两个维度进行矩阵相乘，前面所有的维度都视作Batch 维度</p>
<pre><code class="python">&gt;&gt;&gt; a = tf.random.normal([4,3,28,32])
&gt;&gt;&gt; b = tf.random.normal([4,3,32,2])
&gt;&gt;&gt; a@b # 批量形式的矩阵相乘
&lt;tf.Tensor: id=236, shape=(4, 3, 28, 2), dtype=float32, numpy=
array([[[[-1.66706240e+00, -8.32602978e+00],
         [ 9.83304405e+00, 8.15909767e+00],
         [ 6.31014729e+00, 9.26124632e-01],...</code></pre>
<ul>
<li><p>矩阵相乘函数同样支持自动 Broadcasting 机制</p>
<pre><code class="python">  &gt;&gt;&gt; a = tf.random.normal([4,28,32])
  &gt;&gt;&gt; b = tf.random.normal([32,16])
  &gt;&gt;&gt; tf.matmul(a,b) # 先自动扩展，再矩阵相乘
  &lt;tf.Tensor: id=264, shape=(4, 28, 16), dtype=float32, numpy=
  array([[[-1.11323869e+00, -9.48194981e+00, 6.48123884e+00, ...,
           6.53280640e+00, -3.10894990e+00, 1.53050375e+00],
          [ 4.35898495e+00, -1.03704405e+01, 8.90656471e+00, ...,</code></pre>
</li>
</ul>
<h2 id="TensorFlow2进阶操作"><a href="#TensorFlow2进阶操作" class="headerlink" title="TensorFlow2进阶操作"></a>TensorFlow2进阶操作</h2><h3 id="1、合并与分割"><a href="#1、合并与分割" class="headerlink" title="1、合并与分割"></a>1、合并与分割</h3><h4 id="1-合并"><a href="#1-合并" class="headerlink" title="1.合并"></a>1.合并</h4><p>合并：将多个张量在某个维度上合并为一个张量</p>
<p>张量的合并可以使用拼接(Concatenate)和堆叠(Stack)操作实现：</p>
<ul>
<li>拼接：不会产生新的维度， 仅在现有的维度上合并</li>
<li>堆叠：会创建新维度</li>
</ul>
<p>选择使用拼接还是堆叠操作来合并张量，取决于具体的场景是否需要创建新维度</p>
<h5 id="拼接"><a href="#拼接" class="headerlink" title="拼接"></a>拼接</h5><p>通过<code>tf.concat(tensors, axis)</code>函数拼接张量，其中参数tensors 保存了所有需要合并的张量 List， axis 参数指定需要合并的维度索引</p>
<pre><code class="python">&gt;&gt;&gt; a = tf.random.normal([4,35,8]) # 模拟成绩册 A
&gt;&gt;&gt; b = tf.random.normal([6,35,8]) # 模拟成绩册 B
&gt;&gt;&gt; tf.concat([a,b],axis=0) # 拼接合并成绩册
&lt;tf.Tensor: id=13, shape=(10, 35, 8), dtype=float32, numpy=
array([[[ 1.95299834e-01, 6.87859178e-01, -5.80048323e-01, ...,
          1.29430830e+00, 2.56610274e-01, -1.27798581e+00],
        [ 4.29753691e-01, 9.11329567e-01, -4.47975427e-01, ...,</code></pre>
<ul>
<li>从语法上来说，拼接合并操作可以在任意的维度上进行，唯一的约束是非合并维度的长度<strong>必须一致</strong></li>
</ul>
<h5 id="堆叠"><a href="#堆叠" class="headerlink" title="堆叠"></a>堆叠</h5><p>使用<code>tf.stack(tensors, axis)</code>可以堆叠方式合并多个张量，通过 tensors 列表表示， 参数axis 指定新维度插入的位置（用法与 <code>tf.expand_dims</code> 一致，当axis ≥ 0时，在 axis之前插入； 当axis &lt; 0时，在 axis 之后插入新维度）</p>
<pre><code class="python">&gt;&gt;&gt; a = tf.random.normal([35,8])
&gt;&gt;&gt; b = tf.random.normal([35,8])
&gt;&gt;&gt; tf.stack([a,b],axis=0) # 堆叠合并为 2 个班级，班级维度插入在最前
&lt;tf.Tensor: id=55, shape=(2, 35, 8), dtype=float32, numpy=
array([[[ 3.68728966e-01, -8.54765773e-01, -4.77824420e-01,
         -3.83714020e-01, -1.73216307e+00, 2.03872994e-02,
          2.63810277e+00, -1.12998331e+00],...</code></pre>
<ul>
<li>需要所有待合并的张量 shape <strong>完全一致</strong>才可合并</li>
</ul>
<h4 id="2-分割"><a href="#2-分割" class="headerlink" title="2.分割"></a>2.分割</h4><p>合并操作的逆过程就是分割，将一个张量分拆为多个张量</p>
<p>通过<code>tf.split(x, num_or_size_splits, axis)</code>可以完成张量的分割操作</p>
<ul>
<li>x：待分割张量</li>
<li>num_or_size_splits：切割方案。当 num_or_size_splits 为单个数值时，如 10，表<br>  示等长切割为 10 份；当 num_or_size_splits 为 List 时， List 的每个元素表示每份的长度，如[2,4,2,2]表示切割为 4 份，每份的长度依次是 2、 4、 2、 2</li>
<li>axis：指定分割的维度索引号</li>
</ul>
<pre><code class="python">&gt;&gt;&gt; x = tf.random.normal([10,35,8])
# 等长切割为 10 份
&gt;&gt;&gt; result = tf.split(x, num_or_size_splits=10, axis=0)
&gt;&gt;&gt; len(result) # 返回的列表为 10 个张量的列表
10
&gt;&gt;&gt; result[0] # 查看第一个班级的成绩册张量
&lt;tf.Tensor: id=136, shape=(1, 35, 8), dtype=float32, numpy=
array([[[-1.7786729 , 0.2970506 , 0.02983334, 1.3970423 ,
          1.315918 , -0.79110134, -0.8501629 , -1.5549672 ],
        [ 0.5398711 , 0.21478991, -0.08685189, 0.7730989 ,...</code></pre>
<ul>
<li>仍保留了第一个维度</li>
</ul>
<p>如果希望在某个维度上全部按长度为 1 的方式分割，还可以使用<code>tf.unstack(x, axis)</code>函数。这种方式是<code>tf.split</code>的一种特殊情况，切割长度固定为 1，只需要指定切割维度的索引号即可</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.random.normal([10,35,8])
&gt;&gt;&gt; result = tf.unstack(x,axis=0) # Unstack 为长度为 1 的张量
&gt;&gt;&gt; len(result) # 返回 10 个张量的列表
10
&gt;&gt;&gt; result[0] # 第一个班级
&lt;tf.Tensor: id=166, shape=(35, 8), dtype=float32, numpy=
array([[-0.2034383 , 1.1851563 , 0.25327438, -0.10160723, 2.094969 , -0.8571669 , -0.48985648, 0.55798006],...</code></pre>
<ul>
<li>通过<code>tf.unstack</code>切割后，shape 变为[35,8]，即第一个维度消失了，这是与tf.split区别之处</li>
</ul>
<h3 id="2、数据统计"><a href="#2、数据统计" class="headerlink" title="2、数据统计"></a>2、数据统计</h3><h4 id="1-向量范数"><a href="#1-向量范数" class="headerlink" title="1.向量范数"></a>1.向量范数</h4><p>向量范数(Vector Norm)是表征向量“长度”的一种度量方法， 它可以推广到张量上。在神经网络中，常用来表示张量的权值大小，梯度大小等。常用的向量范数有：</p>
<ul>
<li>L1 范数，定义为向量x的所有元素绝对值之和：$\left | x \right |<em>1=\sum</em>{i}\left | x_i \right |$</li>
<li>L2 范数，定义为向量x的所有元素的平方和，再开根号：$\left | x \right |<em>2=\sqrt{\sum</em>{i}\left | x_i \right |^2}$</li>
<li>$\infty$-范数，定义为向量x的所有元素绝对值的最大值：$\left | x \right |_\infty=max_i(\left | x_i \right |)$</li>
</ul>
<p>对于矩阵和张量，同样可以利用向量范数的计算公式，等价于将矩阵和张量打平成向量后计算</p>
<p>通过<code>tf.norm(x, ord)</code>求解张量的 L1、 L2、$\infty$等范数，其中参数ord 指定为 1、 2时计算 L1、 L2 范数，指定为<code>np.inf</code>时计算$\infty$-范数</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.ones([2,2])
&gt;&gt;&gt; tf.norm(x,ord=1) # 计算 L1 范数
&lt;tf.Tensor: id=183, shape=(), dtype=float32, numpy=4.0&gt;

&gt;&gt;&gt; tf.norm(x,ord=2) # 计算 L2 范数
&lt;tf.Tensor: id=189, shape=(), dtype=float32, numpy=2.0&gt;

&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; tf.norm(x,ord=np.inf) # 计算∞范数
&lt;tf.Tensor: id=194, shape=(), dtype=float32, numpy=1.0&gt;</code></pre>
<h4 id="2-最值、均值、和"><a href="#2-最值、均值、和" class="headerlink" title="2.最值、均值、和"></a>2.最值、均值、和</h4><p>通过 <code>tf.reduce_max</code>、 <code>tf.reduce_min</code>、 <code>tf.reduce_mean</code>、 <code>tf.reduce_sum</code> 函数可以求解张量在某个维度上的最大、最小、 均值、和，也可以求全局最大、最小、均值、和信息</p>
<pre><code class="python"># 假设第一个维度为样本数量，第二个维度为当前样本分别属于 10 个类别的概率
&gt;&gt;&gt; x = tf.random.normal([4,10]) # 模型生成概率

&gt;&gt;&gt; tf.reduce_max(x,axis=1) # 统计概率维度上的最大值
&lt;tf.Tensor: id=203, shape=(4,), dtype=float32, numpy=array([1.2410722 , 0.88495886, 1.4170984 , 0.9550192 ], dtype=float32)&gt;

&gt;&gt;&gt; tf.reduce_min(x,axis=1) # 统计概率维度上的最小值
&lt;tf.Tensor: id=206, shape=(4,), dtype=float32, numpy=array([-0.27862206, -2.4480672 , -1.9983795 , -1.5287997 ], dtype=float32)&gt;

&gt;&gt;&gt; tf.reduce_mean(x,axis=1) # 统计概率维度上的均值
&lt;tf.Tensor: id=209, shape=(4,), dtype=float32, numpy=array([ 0.39526337, -0.17684573, -0.148988 , -0.43544054], dtype=float32)&gt;

# 统计全局的最大、最小、均值、和，返回的张量均为标量
&gt;&gt;&gt; x = tf.random.normal([4,10]) # 模型生成概率
&gt;&gt;&gt; tf.reduce_max(x),tf.reduce_min(x),tf.reduce_mean(x)
(&lt;tf.Tensor: id=218, shape=(), dtype=float32, numpy=1.8653786&gt;, &lt;tf.Tensor: id=220, shape=(), dtype=float32, numpy=-1.9751656&gt;, &lt;tf.Tensor: id=222, shape=(), dtype=float32, numpy=0.014772797&gt;)</code></pre>
<ul>
<li>不指定 axis 参数时， <code>tf.reduce_*</code>函数会求解出全局元素的最大、最小、 均值、和等数据</li>
</ul>
<p>通过 <code>tf.argmax(x, axis)</code>和 <code>tf.argmin(x, axis)</code>可以求解在 axis 轴上， x 的最大值、 最小值所在的索引号</p>
<pre><code class="python"># out：
&lt;tf.Tensor: id=257, shape=(2, 10), dtype=float32, numpy=
array([[0.18773547, 0.1510464 , 0.09431915, 0.13652141, 0.06579739,
        0.02033597, 0.06067333, 0.0666793 , 0.14594753, 0.07094406],
       [0.5092072 , 0.03887136, 0.0390687 , 0.01911005, 0.03850609,
        0.03442522, 0.08060656, 0.10171875, 0.08244187, 0.05604421]], dtype=float32)&gt;

&gt;&gt;&gt; pred = tf.argmax(out, axis=1) # 选取概率最大的位置
&lt;tf.Tensor: id=262, shape=(2,), dtype=int64, numpy=array([0, 0], dtype=int64)&gt;</code></pre>
<h3 id="3、张量比较"><a href="#3、张量比较" class="headerlink" title="3、张量比较"></a>3、张量比较</h3><p>通过<code>tf.equal(a, b)</code>(或<code>tf.math.equal(a, b)</code>，两者等价)函数可以比较2个张量是否相等</p>
<pre><code class="python"># pred：
&lt;tf.Tensor: id=272, shape=(100,), dtype=int64, numpy=
array([0, 6, 4, 3, 6, 8, 6, 3, 7, 9, 5, 7, 3, 7, 1, 5, 6, 1, 2, 9, 0, 6, 5, 4, 9, 5, 6, 4, 6, 0, 8, 4, 7, 3, 4, 7, 4, 1, 2, 4, 9, 4,...
&gt;&gt;&gt; y = tf.random.uniform([100],dtype=tf.int64,maxval=10)
&lt;tf.Tensor: id=281, shape=(100,), dtype=int64, numpy=
array([0, 9, 8, 4, 9, 7, 2, 7, 6, 7, 3, 4, 2, 6, 5, 0, 9, 4, 5, 8, 4, 2, 5, 5, 5, 3, 8, 5, 2, 0, 3, 6, 0, 7, 1, 1, 7, 0, 6, 1, 2, 1, 3,...
&gt;&gt;&gt; out = tf.equal(pred,y) # 预测值与真实值比较，返回布尔类型的张量
&lt;tf.Tensor: id=288, shape=(100,), dtype=bool, numpy=
array([False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, True,...</code></pre>
<p>相关函数汇总表：</p>
<table>
<thead>
<tr>
<th align="center">函数</th>
<th align="center">比较逻辑</th>
</tr>
</thead>
<tbody><tr>
<td align="center">tf.equal(a, b)</td>
<td align="center">$a=b$</td>
</tr>
<tr>
<td align="center">tf.math.greater(a, b)</td>
<td align="center">$a&gt;b$</td>
</tr>
<tr>
<td align="center">tf.math.less(a, b)</td>
<td align="center">$a&lt;b$</td>
</tr>
<tr>
<td align="center">tf.math.greater_equal(a, b)</td>
<td align="center">$a\geqslant b$</td>
</tr>
<tr>
<td align="center">tf.math.less_equal(a, b)</td>
<td align="center">$a\leqslant b$</td>
</tr>
<tr>
<td align="center">tf.math.not_equal(a, b)</td>
<td align="center">$a\neq b$</td>
</tr>
<tr>
<td align="center">tf.math.is_nan(a, b)</td>
<td align="center">$a=nan$</td>
</tr>
</tbody></table>
<h3 id="4、复制和填充"><a href="#4、复制和填充" class="headerlink" title="4、复制和填充"></a>4、复制和填充</h3><h4 id="1-填充"><a href="#1-填充" class="headerlink" title="1.填充"></a>1.填充</h4><p>之前我们介绍了通过复制的方式可以增加数据的长度，但是重复复制数据会破坏原有的数据结构，并不适合于此处。通常的做法是，在需要补充长度的数据开始或结束处填充足够数量的特定数值， 这些特定数值一般代表了<strong>无效意义</strong>，例如 0，使得填充后的长度满足系统要求。那么这种操作就叫作填充(Padding)</p>
<p>填充操作可以通过<code>tf.pad(x, paddings)</code>函数实现， 参数 paddings 是包含了多个[Left Padding, Right Padding]的嵌套方案 List，如[[0,0], [2,1], [1,2]]表示第一个维度不填<br>充， 第二个维度左边(起始处)填充两个单元， 右边(结束处)填充一个单元， 第三个维度左边填充一个单元， 右边填充两个单元</p>
<pre><code class="python">&gt;&gt;&gt; a = tf.constant([1,2,3,4,5,6]) # 第一个句子
&gt;&gt;&gt; b = tf.constant([7,8,1,6]) # 第二个句子
&gt;&gt;&gt; b = tf.pad(b, [[0,2]]) # 句子末尾填充 2 个 0
&lt;tf.Tensor: id=3, shape=(6,), dtype=int32, numpy=array([7, 8, 1, 6, 0, 0])&gt;</code></pre>
<h4 id="2-复制"><a href="#2-复制" class="headerlink" title="2.复制"></a>2.复制</h4><p>即<code>tf.tile()</code>函数。参考<code>基础操作-&gt;维度变换-&gt;复制数据</code>章节</p>
<h3 id="5、数据限幅"><a href="#5、数据限幅" class="headerlink" title="5、数据限幅"></a>5、数据限幅</h3><p>可以通过<code>tf.maximum(x, a)</code>实现数据的下限幅，即$x\in [a,+\infty )$。可以通过<code>tf.minimum(x, a)</code>实现数据的上限幅，即$x\in (-\infty ,a]$</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.range(9)
&gt;&gt;&gt; tf.maximum(x,2) # 下限幅到 2
&lt;tf.Tensor: id=48, shape=(9,), dtype=int32, numpy=array([2, 2, 2, 3, 4, 5, 6, 7, 8])&gt;
&gt;&gt;&gt; tf.minimum(x,7) # 上限幅到 7
&lt;tf.Tensor: id=41, shape=(9,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 7])&gt;</code></pre>
<p>通过组合<code>tf.maximum(x, a)</code>和<code>tf.minimum(x, b)</code>可以实现同时对数据的上下边界限幅，即$x\in [a,b]$</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.range(9)
&gt;&gt;&gt; tf.minimum(tf.maximum(x,2),7) # 限幅为 2~7
&lt;tf.Tensor: id=57, shape=(9,), dtype=int32, numpy=array([2, 2, 2, 3, 4, 5, 6, 7, 7])&gt;</code></pre>
<p>更方便地，我们可以使用<code>tf.clip_by_value</code>函数实现上下限幅：</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.range(9)
&gt;&gt;&gt; tf.clip_by_value(x,2,7) # 限幅为 2~7
&lt;tf.Tensor: id=66, shape=(9,), dtype=int32, numpy=array([2, 2, 2, 3, 4, 5, 6, 7, 7])&gt;</code></pre>
<h3 id="6、高级操作"><a href="#6、高级操作" class="headerlink" title="6、高级操作"></a>6、高级操作</h3><h4 id="1-tf-gather"><a href="#1-tf-gather" class="headerlink" title="1.tf.gather"></a>1.tf.gather</h4><p><code>tf.gather</code>可以实现根据索引号收集数据的目的（与切片类似，但是对于<strong>不规则</strong>的索引方式，切片实现起来非常麻烦， 而<code>tf.gather</code>则更加方便）</p>
<ul>
<li>参数1：带收集的张量</li>
<li>参数2：指定需要收集的索引号</li>
<li>参数3：指定收集的维度</li>
</ul>
<pre><code class="python">&gt;&gt;&gt; x = tf.random.uniform([4,35,8],maxval=100,dtype=tf.int32) # 成绩册张量（4个班级 35个学生 8门科目）
# 收集第 1,4,9,12,13,27 号同学成绩
&gt;&gt;&gt; tf.gather(x,[0,3,8,11,12,26],axis=1)
&lt;tf.Tensor: id=87, shape=(4, 6, 8), dtype=int32, numpy=
array([[[43, 10, 93, 85, 75, 87, 28, 19],
        [74, 11, 25, 64, 84, 89, 79, 85],...</code></pre>
<p>索引号可以<strong>乱序</strong>排列，此时收集的数据也是<strong>对应顺序</strong>：</p>
<pre><code class="python">&gt;&gt;&gt; a=tf.range(8)
&gt;&gt;&gt; a=tf.reshape(a,[4,2]) # 生成张量 a
&lt;tf.Tensor: id=115, shape=(4, 2), dtype=int32, numpy=
array([[0, 1],
       [2, 3],
       [4, 5],
       [6, 7]])&gt;
&gt;&gt;&gt; tf.gather(a,[3,1,0,2],axis=0) # 收集第 4,2,1,3 号元素
&lt;tf.Tensor: id=119, shape=(4, 2), dtype=int32, numpy=
array([[6, 7],
       [2, 3],
       [0, 1],
       [4, 5]])&gt;</code></pre>
<h4 id="2-tf-gather-nd"><a href="#2-tf-gather-nd" class="headerlink" title="2.tf.gather_nd"></a>2.tf.gather_nd</h4><p>通过 tf.gather_nd 函数，可以通过指定每次采样点的多维坐标来实现采样多个点的目的（利用手动一个一个提取然后stack合并的方式也可以达到同样效果，但是效率极低）</p>
<ul>
<li>参数1：带收集的张量</li>
<li>参数2：指定的采样点的索引坐标</li>
</ul>
<pre><code class="python">&gt;&gt;&gt; x = tf.random.uniform([4,35,8],maxval=100,dtype=tf.int32) # 成绩册张量（4个班级 35个学生 8门科目）
# 根据多维坐标收集数据
&gt;&gt;&gt; tf.gather_nd(x,[[1,1],[2,2],[3,3]])#抽查第 2 个班级的第 2 个同学的所有科目，第 3 个班级的第 3 个同学的所有科目，第 4 个班级的第 4 个同学的所有科目
&lt;tf.Tensor: id=256, shape=(3, 8), dtype=int32, numpy=
array([[45, 34, 99, 17, 3, 1, 43, 86],
       [11, 25, 84, 95, 97, 95, 69, 69],
       [ 0, 89, 52, 29, 76, 7, 2, 98]])&gt;</code></pre>
<p>一般地，在使用 <code>tf.gather_nd</code> 采样多个样本时， 例如希望采样i号班级，j个学生，k门科目的成绩，则可以表达为[. . . , [i,j,k], . . . ]， 外层的括号长度为采样样本的个数，内层列表包含了每个采样点的索引坐标</p>
<h4 id="3-tf-boolean-mask"><a href="#3-tf-boolean-mask" class="headerlink" title="3.tf.boolean_mask"></a>3.tf.boolean_mask</h4><p>除了可以通过给定索引号的方式采样，还可以通过给定掩码(Mask)的方式进行采样。通过<code>tf.boolean_mask(x, mask, axis)</code>可以在 axis 轴上根据mask 方案进行采样</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.random.uniform([4,35,8],maxval=100,dtype=tf.int32) # 成绩册张量（4个班级 35个学生 8门科目）
# 根据掩码方式采样班级，给出掩码和维度索引
&gt;&gt;&gt; tf.boolean_mask(x,mask=[True, False,False,True],axis=0)
&lt;tf.Tensor: id=288, shape=(2, 35, 8), dtype=int32, numpy=
array([[[43, 10, 93, 85, 75, 87, 28, 19],...</code></pre>
<ul>
<li>掩码的长度必须与对应维度的<strong>长度一致</strong></li>
<li>掩码可以是List嵌套，此时效果与<code>tf.gather_nd</code>类似</li>
</ul>
<p><code>tf.boolean_mask</code>既可以实现了<code>tf.gather</code>方式的一维掩码采样， 又可以实现<code>tf.gather_nd</code>方式的多维掩码采样</p>
<h4 id="4-tf-where"><a href="#4-tf-where" class="headerlink" title="4.tf.where"></a>4.tf.where</h4><p>通过<code>tf.where(cond, a, b)</code>操作可以根据 cond 条件的真假从参数A或B中读取数据， 条件判定规则如下：$o_i=\left{\begin{matrix}<br>a_i &amp; cond_i为True\<br>b_i &amp; cond_i为False<br>\end{matrix}\right.$。其中i为张量的元素索引， 返回的张量大小与A和B一致， 当对应位置的$cond_i$为 True， $o_i$从$a_i$中复制数据；当对应位置的$cond_i$为 False， $o_i$从$a_i$中复制数据</p>
<pre><code class="python">&gt;&gt;&gt; a = tf.ones([3,3]) # 构造 a 为全 1 矩阵
&gt;&gt;&gt; b = tf.zeros([3,3]) # 构造 b 为全 0 矩阵
# 构造采样条件
&gt;&gt;&gt; cond = tf.constant([[True,False,False],[False,True,False],[True,True,False]])
&gt;&gt;&gt; tf.where(cond,a,b) # 根据条件从 a,b 中采样
&lt;tf.Tensor: id=384, shape=(3, 3), dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [1., 1., 0.]], dtype=float32)&gt;</code></pre>
<p>当参数a=b=None时，tf.where 会返回 cond 张量中所有 True 的元素的索引坐标</p>
<pre><code class="python">&gt;&gt;&gt; cond = tf.constant([[True,False,False],[False,True,False],[True,True,False]])
&gt;&gt;&gt; tf.where(cond) # 获取 cond 中为 True 的元素索引
&lt;tf.Tensor: id=387, shape=(4, 2), dtype=int64, numpy=
array([[0, 0],
       [1, 1],
       [2, 0],
       [2, 1]], dtype=int64)&gt;</code></pre>
<h5 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h5><pre><code class="python">&gt;&gt;&gt; x = tf.random.normal([3,3]) # 构造 a
&lt;tf.Tensor: id=403, shape=(3, 3), dtype=float32, numpy=
array([[-2.2946844 , 0.6708417 , -0.5222212 ],
       [-0.6919401 , -1.9418817 , 0.3559235 ],
       [-0.8005251 , 1.0603906 , -0.68819374]], dtype=float32)&gt;
# 通过比较运算，得到所有正数的掩码：
&gt;&gt;&gt; mask=x&gt;0 # 比较操作，等同于 tf.math.greater()
&lt;tf.Tensor: id=405, shape=(3, 3), dtype=bool, numpy=
array([[False, True, False],
       [False, False, True],
       [False, True, False]])&gt;
# 通过 tf.where 提取此掩码处 True 元素的索引坐标：
&gt;&gt;&gt; indices=tf.where(mask) # 提取所有大于 0 的元素索引
&lt;tf.Tensor: id=407, shape=(3, 2), dtype=int64, numpy=
array([[0, 1],
       [1, 2],
       [2, 1]], dtype=int64)&gt;
# 拿到索引后，通过 tf.gather_nd 即可恢复出所有正数的元素：
&gt;&gt;&gt; tf.gather_nd(x,indices) # 提取正数的元素值
&lt;tf.Tensor: id=410, shape=(3,), dtype=float32, numpy=array([0.6708417, 0.3559235, 1.0603906], dtype=float32)&gt;
# 实际上，也可以直接通过 tf.boolean_mask 获取所有正数的元素向量:
&gt;&gt;&gt; tf.boolean_mask(x,mask) # 通过掩码提取正数的元素值
&lt;tf.Tensor: id=439, shape=(3,), dtype=float32, numpy=array([0.6708417, 0.3559235, 1.0603906], dtype=float32)&gt;</code></pre>
<h4 id="5-scatter-nd"><a href="#5-scatter-nd" class="headerlink" title="5.scatter_nd"></a>5.scatter_nd</h4><p>通过<code>tf.scatter_nd(indices, updates, shape)</code>函数可以高效地刷新张量的部分数据，但是这个函数只能在<strong>全0</strong>的白板张量上面执行刷新操作，因此可能需要结合其它操作来实现现有张量的数据刷新功能</p>
<p>白板的形状通过 shape 参数表示，需要刷新的数据索引号通过 indices 表示，新数据为 updates。 根据 indices 给出的索引位置将 updates 中新的数据依次写入白板中，并返回更新后的结果张量</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200504220326506.png" alt="scatter_nd更新数据示意图"></p>
<pre><code class="python"># 构造需要刷新数据的位置参数，即为 4、 3、 1 和 7 号位置
&gt;&gt;&gt; indices = tf.constant([[4], [3], [1], [7]])
# 构造需要写入的数据， 4 号位写入 4.4,3 号位写入 3.3，以此类推
&gt;&gt;&gt; updates = tf.constant([4.4, 3.3, 1.1, 7.7])
# 在长度为 8 的全 0 向量上根据 indices 写入 updates 数据
&gt;&gt;&gt; tf.scatter_nd(indices, updates, [8])
&lt;tf.Tensor: id=467, shape=(8,), dtype=float32, numpy=array([0. , 1.1, 0. , 3.3, 4.4, 0. , 0. , 7.7], dtype=float32)&gt;</code></pre>
<p>3维张量刷新例子：</p>
<pre><code class="python"># 构造写入位置，即 2 个位置
&gt;&gt;&gt; indices = tf.constant([[1],[3]])
&gt;&gt;&gt; updates = tf.constant([# 构造写入数据，即 2 个矩阵
[[5,5,5,5],[6,6,6,6],[7,7,7,7],[8,8,8,8]],
[[1,1,1,1],[2,2,2,2],[3,3,3,3],[4,4,4,4]]
])
# 在 shape 为[4,4,4]白板上根据 indices 写入 updates
&gt;&gt;&gt; tf.scatter_nd(indices,updates,[4,4,4])
&lt;tf.Tensor: id=477, shape=(4, 4, 4), dtype=int32, numpy=
array([[[0, 0, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 0]],
       [[5, 5, 5, 5], # 写入的新数据 1
        [6, 6, 6, 6],
        [7, 7, 7, 7],
        [8, 8, 8, 8]],
       [[0, 0, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 0]],
       [[1, 1, 1, 1], # 写入的新数据 2
        [2, 2, 2, 2],
        [3, 3, 3, 3],
        [4, 4, 4, 4]]])&gt;</code></pre>
<h4 id="6-meshgrid"><a href="#6-meshgrid" class="headerlink" title="6.meshgrid"></a>6.meshgrid</h4><p>通过<code>tf.meshgrid</code>函数可以方便地生成二维网格的采样点坐标，方便可视化等应用场合</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.linspace(-8.,8,100) # 设置 x 轴的采样点
&gt;&gt;&gt; y = tf.linspace(-8.,8,100) # 设置 y 轴的采样点
&gt;&gt;&gt; x,y = tf.meshgrid(x,y) # 生成网格点，并内部拆分后返回
&gt;&gt;&gt; x.shape,y.shape # 打印拆分后的所有点的 x,y 坐标张量 shape
(TensorShape([100, 100]), TensorShape([100, 100]))
&gt;&gt;&gt; z = tf.sqrt(x**2+y**2)
&gt;&gt;&gt; z = tf.sin(z)/z # sinc 函数实现
&gt;&gt;&gt; import matplotlib
&gt;&gt;&gt; from matplotlib import pyplot as plt
# 导入 3D 坐标轴支持
&gt;&gt;&gt; from mpl_toolkits.mplot3d import Axes3D
&gt;&gt;&gt; fig = plt.figure()
&gt;&gt;&gt; ax = Axes3D(fig) # 设置 3D 坐标轴
# 根据网格点绘制 sinc 函数 3D 曲面
&gt;&gt;&gt; ax.contour3D(x.numpy(), y.numpy(), z.numpy(), 50)
&gt;&gt;&gt; plt.show()</code></pre>
<ul>
<li>通过在 x 轴上进行采样 100 个数据点， y 轴上采样 100 个数据点，然后利用tf.meshgrid(x, y)即可返回这 10000 个数据点的张量数据， 保存在 shape 为[100,100,2]的张量中。为了方便计算， tf.meshgrid 会返回在 axis=2 维度切割后的 2 个张量A和B，其中张量A包含了所有点的 x 坐标， B包含了所有点的 y 坐标， shape 都为[100,100]</li>
</ul>
<h3 id="7、经典数据集加载"><a href="#7、经典数据集加载" class="headerlink" title="7、经典数据集加载"></a>7、经典数据集加载</h3><p><code>keras.datasets</code> 模块提供了常用经典数据集的自动下载、 管理、 加载与转换功能，并且提供了 <code>tf.data.Dataset</code> 数据集对象， 方便实现多线程(Multi-threading)、 预处理(Preprocessing)、 随机打散(Shuffle)和批训练(Training on Batch)等常用数据集的功能</p>
<p>常用的经典数据集：</p>
<ul>
<li>Boston Housing， 波士顿房价趋势数据集，用于回归模型训练与测试</li>
<li>CIFAR10/100， 真实图片数据集，用于图片分类任务</li>
<li>MNIST/Fashion_MNIST， 手写数字图片数据集，用于图片分类任务</li>
<li>IMDB， 情感分类任务数据集，用于文本分类任务</li>
</ul>
<hr>
<p>通过<code>datasets.xxx.load_data()</code>函数即可实现经典数据集的<strong>自动加载</strong>，其中 xxx 代表具体的数据集名称，如“CIFAR10”、“MNIST”。TensorFlow会默认将数据缓存在用户目录下的<code>.keras/datasets</code> 文件夹，用户无需关心数据集是如何保存的。如果当前数据集不在缓存中，则会自动从网络下载、 解压和加载数据集；如果已经在缓存中， 则自动完成加载</p>
<pre><code class="python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; from tensorflow import keras
&gt;&gt;&gt; from tensorflow.keras import datasets # 导入经典数据集加载模块
# 加载 MNIST 数据集
&gt;&gt;&gt; (x, y), (x_test, y_test) = datasets.mnist.load_data()
&gt;&gt;&gt; print(&#39;x:&#39;, x.shape, &#39;y:&#39;, y.shape, &#39;x test:&#39;, x_test.shape, &#39;y test:&#39;, y_test)
# 返回数组的形状
x: (60000, 28, 28) y: (60000,) x test: (10000, 28, 28) y test: [7 2 1 ... 4 5 6]</code></pre>
<ul>
<li>通过 load_data()函数会返回相应格式的数据，对于图片数据集 MNIST、 CIFAR10 等，会返回 2 个 tuple，第一个 tuple 保存了用于训练的数据 x 和 y 训练集对象；第 2 个 tuple 则保存了用于测试的数据 x_test 和 y_test 测试集对象，所有的数据都用 Numpy 数组容器保存</li>
</ul>
<p>数据加载进入内存后，需要转换成 <code>Dataset</code> 对象。通过<code>Dataset.from_tensor_slices</code>可以将数据<strong>转换成Dataset对象</strong></p>
<pre><code class="python">&gt;&gt;&gt; train_db = tf.data.Dataset.from_tensor_slices((x, y)) # 以将训练部分的数据图片x和标签y转换成Dataset对象</code></pre>
<h4 id="1-随机打散"><a href="#1-随机打散" class="headerlink" title="1.随机打散"></a>1.随机打散</h4><p>通过<code>Dataset.shuffle(buffer_size)</code>工具可以设置 Dataset 对象随机打散数据之间的顺序，防止每次训练时数据按固定顺序产生，从而使得模型尝试“记忆”住标签信息</p>
<pre><code class="python"># train_db为Dataset对象
&gt;&gt;&gt; train_db = train_db.shuffle(10000) # 随机打散样本，不会打乱样本与标签映射关系</code></pre>
<p>buffer_size 参数指定缓冲池的大小，一般设置为一个较大的常数即可。 调用 Dataset提供的这些工具函数会<strong>返回新的Dataset对象</strong>，可以通过<code>db = db.step1().step2().step3()</code>方式按序完成所有的数据处理步骤</p>
<h4 id="2-批训练"><a href="#2-批训练" class="headerlink" title="2.批训练"></a>2.批训练</h4><p>为了利用显卡的并行计算能力，一般在网络的计算过程中会同时计算多个样本，把这种训练方式叫做批训练，其中一个批中样本的数量叫做<code>Batch Size</code></p>
<p>为了一次能够从Dataset 中产生 Batch Size 数量的样本，需要设置 Dataset 为批训练方式：</p>
<pre><code class="python"># train_db为Dataset对象
&gt;&gt;&gt; train_db = train_db.batch(128) # 设置批训练， batch size 为 128</code></pre>
<ul>
<li>一次并行计算 128 个样本的数据</li>
</ul>
<p>Batch Size 一般根据用户的 GPU 显存资源来设置，当显存不足时，可以适量减少 Batch Size 来减少显存使用量</p>
<h4 id="3-预处理"><a href="#3-预处理" class="headerlink" title="3.预处理"></a>3.预处理</h4><p>Dataset 对象通过提供<code>map(func)</code>工具函数， 可以非常方便地调用用户自定义的预处理逻辑， 它实现在func函数里</p>
<pre><code class="python">def preprocess(x, y): # 自定义的预处理函数
    # 调用此函数时会自动传入 x,y 对象， shape 为[b, 28, 28], [b]
    x = tf.cast(x, dtype=tf.float32) / 255.# 标准化到 0~1
    x = tf.reshape(x, [-1, 28*28]) # 打平
    y = tf.cast(y, dtype=tf.int32) # 转成整型张量
    y = tf.one_hot(y, depth=10) # one-hot 编码
    # 返回的 x,y 将替换传入的 x,y 参数，从而实现数据的预处理功能
    return x,y
# 预处理函数实现在 preprocess 函数中，传入函数名即可
train_db = train_db.map(preprocess)</code></pre>
<h4 id="4-循环训练"><a href="#4-循环训练" class="headerlink" title="4.循环训练"></a>4.循环训练</h4><p>对于 Dataset 对象， 在使用时可以通过</p>
<pre><code class="python">for step, (x,y) in enumerate(train_db): # 迭代数据集对象，带 step 参数</code></pre>
<p>或</p>
<pre><code class="python">for x,y in train_db: # 迭代数据集对象</code></pre>
<p>方式进行迭代，每次返回的 x 和 y 对象即为批量样本和标签，当对 train_db 的所有样本完成一次迭代后， for 循环终止退出</p>
<p>这样完成一个 Batch 的数据训练（执行一次循环体），叫做一个<strong>Step</strong></p>
<p>通过多个 step 来完成整个训练集的一次迭代（执行一次整个循环），叫做一个<strong>Epoch</strong></p>
<p>在实际训练时，通常需要对数据集迭代多个 Epoch 才能取得较好地训练效果：</p>
<pre><code class="python">for epoch in range(20): # 训练 Epoch 数
    for step, (x,y) in enumerate(train_db): # 迭代 Step 数
        # training...
# 可以通过repeat设置epoch的迭代次数（上面与下面等价）
train_db = train_db.repeat(20) # 数据集迭代 20 遍才终止
for step, (x,y) in enumerate(train_db): # 迭代 Step 数
    # training...</code></pre>
<ul>
<li>通过repeat，使得数据集对象内部遍历多次才会退出</li>
</ul>
<h3 id="8、MNIST实战"><a href="#8、MNIST实战" class="headerlink" title="8、MNIST实战"></a>8、MNIST实战</h3><pre><code class="python"></code></pre>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>神经网络属于机器学习的一个研究分支，它特指利用多个神经元去参数化映射函数$f_\theta$的模型</p>
<h3 id="1、感知机"><a href="#1、感知机" class="headerlink" title="1、感知机"></a>1、感知机</h3><p>感知机是线性模型，并不能处理线性不可分问题。通过在线性模型后添加激活函数后得到<br>活性值(Activation) $a$：$a=\sigma (z)=\sigma (W^Tx+b)$，其中激活函数可以是阶跃函数(Step function)，也可以是符号函数(Sign function)</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200505012316465.png" alt="感知机模型"></p>
<ul>
<li>接受长度为𝑛的一维向量$x=[x_1,x_2,…,x_n]$</li>
<li>每个输入节点通过权值为$w_i,i\in [1,n]$的连接汇集为变量𝑧，即：$z=w_1x_1+w_2x_2+…+w_nx_n+b$<ul>
<li>𝑏称为感知机的偏置(Bias)</li>
<li>一维向量$W=[w_1,w_2,..w_n]$称为感知机的权值(Weight)</li>
<li>$z$称为感知机的净活性值(Net Activation)</li>
</ul>
</li>
</ul>
<p>但是阶跃函数和符号函数在0处是不连续的， 其他位置导数为 0，无法利用梯度下降算法进行参数优化</p>
<p>以感知机为代表的线性模型不能解决异或(XOR)等线性不可分问题</p>
<h3 id="2、全连接层"><a href="#2、全连接层" class="headerlink" title="2、全连接层"></a>2、全连接层</h3><p>现代深度学习的核心结构在感知机的基础上，将不连续的阶跃激活函数换成了其它平滑连续可导的激活函数， 并通过堆叠多个网络层来增强网络的表达能力</p>
<p>如下所示的整个网络层可以通过矩阵关系式表达$O=X@W+b$</p>
<ul>
<li>输入矩阵$X$的 shape 定义为$[b,d_{in}]$，$b$为样本数量，此处只有1个样本参与前向运算，$d_{in}$为输入节点数（即<strong>输入特征长度</strong>）</li>
<li>权值矩阵$W$的 shape 定义为$[d_{in},d_{out}]$，$d_{out}$为输出节点数（即<strong>输出特征长度</strong>）</li>
<li>偏置向量$b$的 shape 定义为$[d_{out}]$</li>
<li>输出矩阵$O$包含了$b$个样本的输出特征， shape 为$[b,d_{out}]$</li>
</ul>
<p>由于每个输出节点与全部的输入节点相连接，这种网络层称为<strong>全连接层</strong>(Fully-connected Layer)，或者稠密连接层(Dense Layer)，$W$矩阵叫做全连接层的权值矩阵，$b$向量叫做全连接层的偏置向量</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200505013252398.png" alt="全连接层"></p>
<h4 id="1-张量方式实现"><a href="#1-张量方式实现" class="headerlink" title="1.张量方式实现"></a>1.张量方式实现</h4><p>要实现全连接层，只需要定义好权值张量𝑾和偏置张量𝒃，并利用批量矩阵相乘函数<code>tf.matmul()</code>即可完成网络层的计算</p>
<p>例如， 创建输入𝑿矩阵为𝑏 = 2个样本，每个样本的输入特征长度为$d_{in}$ = 784，输出节点数为$d_{out}$ = 256，故定义权值矩阵𝑾的 shape 为[784,256]，并采用正态分布初始化𝑾；偏置向量𝒃的 shape 定义为[256]，在计算完𝑿@𝑾后相加即可，最终全连接层的输出𝑶的 shape 为[2,256]，即 2 个样本的特征，每个特征长度为 256，代码实现如下：</p>
<pre><code class="python"># 创建 W,b 张量
&gt;&gt;&gt; x = tf.random.normal([2,784])
&gt;&gt;&gt; w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))
&gt;&gt;&gt; b1 = tf.Variable(tf.zeros([256]))
&gt;&gt;&gt; o1 = tf.matmul(x,w1) + b1 # 线性变换
&gt;&gt;&gt; o1 = tf.nn.relu(o1) # 激活函数
&lt;tf.Tensor: id=31, shape=(2, 256), dtype=float32, numpy=
array([[ 1.51279330e+00, 2.36286330e+00, 8.16453278e-01,
         1.80338228e+00, 4.58602428e+00, 2.54454136e+00,...</code></pre>
<h4 id="2-层方式实现"><a href="#2-层方式实现" class="headerlink" title="2.层方式实现"></a>2.层方式实现</h4><p>作为最常用的网络层之一，TensorFlow中有更高层、使用更方便的层实现方式： <code>layers.Dense(units, activation)</code>。</p>
<p>通过<code>layer.Dense</code>类， 只需要指定输出节点数 Units 和激活函数类型 activation 即可</p>
<p>注意：</p>
<ul>
<li>输入节点数会根据第一次运算时的输入 shape 确定，同时根据输入、输出节点数<br>  自动创建并初始化权值张量𝑾和偏置张量𝒃（因此在新建类 Dense 实例时，并不会立即创建权值张量𝑾和偏置张量𝒃， 而是需要调用 build 函数或者直接进行一次前向计算，才能完成网络参数的创建）</li>
<li>activation参数指定当前层的激活函数，可以为常见的激活函数或自定义激活函数，也可以指定为 None，即无激活函数</li>
</ul>
<pre><code class="python">&gt;&gt;&gt; x = tf.random.normal([4,28*28])
&gt;&gt;&gt; from tensorflow.keras import layers # 导入层模块
# 创建全连接层，指定输出节点数和激活函数
&gt;&gt;&gt; fc = layers.Dense(512, activation=tf.nn.relu)
&gt;&gt;&gt; h1 = fc(x) # 通过 fc 类实例完成一次全连接层的计算，返回输出张量
&lt;tf.Tensor: id=72, shape=(4, 512), dtype=float32, numpy=
array([[0.63339347, 0.21663809, 0. , ..., 1.7361937 , 0.39962345, 2.4346168 ],...</code></pre>
<p>通过类内部的成员名 kernel 和 bias 来获取权值张量W和偏置张量b对象：</p>
<pre><code class="python">&gt;&gt;&gt; fc.kernel # 获取 Dense 类的权值矩阵
&lt;tf.Variable &#39;dense_1/kernel:0&#39; shape=(784, 512) dtype=float32, numpy=
array([[-0.04067389, 0.05240148, 0.03931375, ..., -0.01595572, -0.01075954, -0.06222073],
&gt;&gt;&gt; fc.bias # 获取 Dense 类的偏置向量
&lt;tf.Variable &#39;dense_1/bias:0&#39; shape=(512,) dtype=float32, numpy=
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</code></pre>
<p>在优化参数时，需要获得网络的所有待优化的张量参数列表，可以通过类的<code>trainable_variables</code>来返回<strong>待优化参数列表</strong></p>
<pre><code class="python">&gt;&gt;&gt; fc.trainable_variables # 返回待优化参数列表
[&lt;tf.Variable &#39;dense_1/kernel:0&#39; shape=(784, 512) dtype=float32,...,
&lt;tf.Variable &#39;dense_1/bias:0&#39; shape=(512,) dtype=float32, numpy=...]</code></pre>
<p>网络层除了保存了待优化张量列表 trainable_variables，还有部分层包含了不参与梯度优化的张量，如后续介绍的 Batch Normalization 层， 可以通过<code>non_trainable_variables</code>成员返回所有<strong>不需要优化参数列表</strong>。如果希望获得<strong>所有参数列表</strong>， 可以通过类的<code>variables</code>返回</p>
<pre><code class="python">&gt;&gt;&gt; fc.variables # 返回所有参数列表
[&lt;tf.Variable &#39;dense_1/kernel:0&#39; shape=(784, 512) dtype=float32,...,
&lt;tf.Variable &#39;dense_1/bias:0&#39; shape=(512,) dtype=float32, numpy=...]</code></pre>
<ul>
<li>对于全连接层，内部张量都参与梯度优化</li>
</ul>
<p>利用网络层类对象进行前向计算时，只需要调用类的<code>__call__</code>方法即可，即写成<code>fc(x)</code>方式便可（会自动调用类的<code>__call__</code>方法，在<code>__call__</code>方法中会自动调用call方法，由 TensorFlow 框架自动完成）</p>
<h3 id="3、神经网络"><a href="#3、神经网络" class="headerlink" title="3、神经网络"></a>3、神经网络</h3><p>通过层层堆叠全连接层，保证前一层的输出节点数与当前层的输入节点数匹配，即可堆叠出任意层数的网络。把这种由神经元相互连接而成的网络叫做神经网络</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200505021027758.png" alt="4层神经网络结构"></p>
<p>通过堆叠 4 个全连接层，可以获得层数为 4 的神经网络，由于每层均为全连接层， 称为<strong>全连接网络</strong>。其中第 1~3 个全连接层在网络中间，称之为<strong>隐藏层</strong>1、 2、3，最后一个全连接层的输出作为网络的输出，称为<strong>输出层</strong></p>
<p>在设计全连接网络时，网络的结构配置等超参数可以按着经验法则自由设置，只需要<br>遵循少量的约束即可。例如：</p>
<ul>
<li>隐藏层 1 的输入节点数需和数据的实际特征长度匹配</li>
<li>每层的输入层节点数与上一层输出节点数匹配</li>
<li>输出层的激活函数和节点数需要根据任务的具体设定进行设计。</li>
</ul>
<p>总的来说，神经网络模型的结构设计自由度较大，至于与哪一组超参数是最优的，这需要很多的领域经验知识和大量的实验尝试</p>
<h4 id="1-张量方式实现-1"><a href="#1-张量方式实现-1" class="headerlink" title="1.张量方式实现"></a>1.张量方式实现</h4><p>对于多层神经网络，以上图4层网络结构为例， 需要分别定义各层的权值矩阵𝑾和偏置向量𝒃，且每层的参数只能用于对应的层，不能混淆使用。在计算时，只需要按照网络层的顺序，将上一层的输出作为当前层的输入即可。最后一层是否需要添加激活函数通常视具体的任务而定</p>
<pre><code class="python"># 隐藏层 1 张量
w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))
b1 = tf.Variable(tf.zeros([256]))
# 隐藏层 2 张量
w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))
b2 = tf.Variable(tf.zeros([128]))
# 隐藏层 3 张量
w3 = tf.Variable(tf.random.truncated_normal([128, 64], stddev=0.1))
b3 = tf.Variable(tf.zeros([64]))
# 输出层张量
w4 = tf.Variable(tf.random.truncated_normal([64, 10], stddev=0.1))
b4 = tf.Variable(tf.zeros([10]))
with tf.GradientTape() as tape: # 梯度记录器
    # x: [b, 28*28]
    # 隐藏层 1 前向计算， [b, 28*28] =&gt; [b, 256]
    h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])
    h1 = tf.nn.relu(h1)
    # 隐藏层 2 前向计算， [b, 256] =&gt; [b, 128]
    h2 = h1@w2 + b2
    h2 = tf.nn.relu(h2)
    # 隐藏层 3 前向计算， [b, 128] =&gt; [b, 64]
    h3 = h2@w3 + b3
    h3 = tf.nn.relu(h3)
    # 输出层前向计算， [b, 64] =&gt; [b, 10]
    h4 = h3@w4 + b4</code></pre>
<ul>
<li>在使用 TensorFlow 自动求导功能计算梯度时，需要将前向计算过程放置在tf.GradientTape()环境中，从而利用 GradientTape 对象的 gradient()方法自动求解参数的梯度，并利用 optimizers 对象更新参数</li>
</ul>
<h4 id="2-层方式实现-1"><a href="#2-层方式实现-1" class="headerlink" title="2.层方式实现"></a>2.层方式实现</h4><pre><code class="python"># 导入常用网络层 layers
from tensorflow.keras import layers,Sequential
fc1 = layers.Dense(256, activation=tf.nn.relu) # 隐藏层 1
fc2 = layers.Dense(128, activation=tf.nn.relu) # 隐藏层 2
fc3 = layers.Dense(64, activation=tf.nn.relu) # 隐藏层 3
fc4 = layers.Dense(10, activation=None) # 输出层
# 前向计算
x = tf.random.normal([4,28*28])
h1 = fc1(x) # 通过隐藏层 1 得到输出
h2 = fc2(h1) # 通过隐藏层 2 得到输出
h3 = fc3(h2) # 通过隐藏层 3 得到输出
h4 = fc4(h3) # 通过输出层得到网络输出</code></pre>
<p>对于这种数据<strong>依次向前传播</strong>的网络， 也可以通过<code>Sequential</code>容器封装成一个网络大类对象，调用大类的前向计算函数一次即可完成所有层的前向计算</p>
<pre><code class="python"># 导入 Sequential 容器
from tensorflow.keras import layers,Sequential
# 通过 Sequential 容器封装为一个网络类
model = Sequential([
    layers.Dense(256, activation=tf.nn.relu) , # 创建隐藏层 1
    layers.Dense(128, activation=tf.nn.relu) , # 创建隐藏层 2
    layers.Dense(64, activation=tf.nn.relu) , # 创建隐藏层 3
    layers.Dense(10, activation=None) , # 创建输出层
])
# 前向计算得到输出
out = model(x)</code></pre>
<h4 id="3-优化目标"><a href="#3-优化目标" class="headerlink" title="3.优化目标"></a>3.优化目标</h4><p>我们把神经网络从输入到输出的计算过程叫做<strong>前向传播</strong>(Forward Propagation)或<strong>前向计</strong><br><strong>算</strong>。神经网络的前向传播过程，也是数据张量(Tensor)从第一层流动(Flow)至输出层的过程，即从输入数据开始，途径每个隐藏层，直至得到输出并计算误差，这也是 TensorFlow<br>框架名字由来</p>
<p>前向传播的最后一步就是完成误差的计算$ℒ=g(𝑓_\theta(x),y)$</p>
<ul>
<li>其中$𝑓_\theta(\cdot)$代表了利用𝜃参数化的神经网络模型</li>
<li>$g(\cdot)$称之为误差函数，用来描述当前网络的预测值$𝑓_\theta(x)$与真实标签𝒚之间的差距度量， 比如常用的均方差误差函数</li>
<li>ℒ称为网络的误差(Error，或损失 Loss)，一般为标量</li>
</ul>
<p>我们希望通过在训练集$\mathbb{D}^{train}$上面学习到一组参数𝜃使得训练的误差ℒ最小：</p>
<p>$\theta^*=\underbrace{arg\ min}_{\theta}g(𝑓_\theta(x),y),x\in \mathbb{D}^{train}$</p>
<p>上述的最小化优化问题一般采用误差反向传播(Backward Propagation，简称 BP)算法来求解网络参数𝜃的梯度信息，并利用梯度下降(Gradient Descent，简称 GD)算法迭代更新参数：$\theta’=\theta-\eta\cdot\bigtriangledown_\thetaℒ$，其中𝜂为学习率</p>
<p>网络的参数量是衡量网络规模的重要指标。计算全连接层的参数量方法：</p>
<p>考虑权值矩阵𝑾，偏置向量𝒃，输入特征长度为$d_{in}$， 输出特征长度为$d_{out}$的网络层， 𝑾的参数量为$d_{in}\cdot d_{out}$，再加上偏置𝒃的参数， 总参数量为$d_{in}\cdot d_{out}+d_{out}$。对于多层的全连接神经网络，总参数量的计算应累加所有全连接层的总参数量</p>
<h3 id="4、激活函数"><a href="#4、激活函数" class="headerlink" title="4、激活函数"></a>4、激活函数</h3><p>激活函数都是平滑可导的，适合于梯度下降算法</p>
<h4 id="1-Sigmoid"><a href="#1-Sigmoid" class="headerlink" title="1.Sigmoid"></a>1.Sigmoid</h4><p>Sigmoid 函数也叫 Logistic 函数，定义为$Sigmoid(x)\doteq\frac{1}{1+e^{-x}}$</p>
<p>它的一个优良特性就是能够把𝑥∈𝑅的输入“压缩” 到𝑥∈(0,1)区间，这个区间的数值在机器学习常用来表示以下意义：</p>
<ul>
<li>概率分布 (0,1)区间的输出和概率的分布范围[0,1]契合，可以通过 Sigmoid 函数将输出转译为<strong>概率输出</strong></li>
<li>信号强度：一般可以将 0~1 理解为某种信号的强度（如像素的颜色强度， 1 代表当前通道颜色最强， 0 代表当前通道无颜色；抑或代表门控值(Gate)的强度， 1 代表当前门控全部开放， 0 代表门控关闭）</li>
</ul>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200505030303847.png" alt="Sigmoid函数曲线"></p>
<p>可以通过<code>tf.nn.sigmoid</code>实现 Sigmoid 函数：</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.linspace(-6.,6.,10) # 构造-6~6 的输入向量
&gt;&gt;&gt; tf.nn.sigmoid(x) # 通过 Sigmoid 函数
&lt;tf.Tensor: id=7, shape=(10,), dtype=float32, numpy=
array([0.00247264, 0.00931597, 0.03444517, 0.11920291, 0.33924365, 0.6607564 , 0.8807971 , 0.96555483, 0.99068403, 0.9975274 ], dtype=float32)&gt;</code></pre>
<h4 id="2-ReLU"><a href="#2-ReLU" class="headerlink" title="2.ReLU"></a>2.ReLU</h4><p>在 ReLU(REctified Linear Unit， 修正线性单元)激活函数提出之前， Sigmoid 函数通常<br>是神经网络的激活函数首选。</p>
<p>Sigmoid 函数在输入值较大或较小时容易出现梯度值接近于 0 的现象，称为<strong>梯度弥散</strong>现象。出现梯度弥散现象时， 网络参数长时间得不到更新，导致训练不收敛或停滞不动的现象发生， 较深层次的网络模型中更容易出现梯度弥散现象</p>
<p>ReLU定义为$ReLU(x)\doteq max(0,x)$，函数曲线如下：</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200505030816893.png" alt="ReLU函数曲线"></p>
<p>可以通过<code>tf.nn.relu</code>实现 ReLU 函数：</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.linspace(-6.,6.,10) # 构造-6~6 的输入向量
&gt;&gt;&gt; tf.nn.relu(x) # 通过 ReLU 激活函数
&lt;tf.Tensor: id=11, shape=(10,), dtype=float32, numpy=
array([0. , 0. , 0. , 0. , 0. , 0.666667, 2. , 3.333334, 4.666667, 6. ], dtype=float32)&gt;</code></pre>
<p>除了可以使用函数式接口 tf.nn.relu 实现 ReLU 函数外，还可以像 Dense 层一样将ReLU 函数作为一个网络层添加到网络中，对应的类为 <code>layers.ReLU()</code>类。一般来说，激活<br>函数类并不是主要的网络运算层，不计入网络的层数</p>
<p>ReLU 函数有着优良的梯度特性，在大量的深度学习应用中被验证非常有效，是应用最广泛的激活函数之一</p>
<h4 id="3-LeakyReLU"><a href="#3-LeakyReLU" class="headerlink" title="3.LeakyReLU"></a>3.LeakyReLU</h4><p>ReLU 函数在𝑥 &lt; 0时导数值恒为 0，也可能会造成梯度弥散现象，为了克服这个问<br>题， LeakyReLU 函数被提出：$LeakyReLU\doteq \left{\begin{matrix}<br>x &amp; x\geqslant 0\<br>px &amp; x&lt;0<br>\end{matrix}\right.$</p>
<p>其中𝑝为用户自行设置的某较小数值的超参数，如 0.02 等。当𝑝 = 0时， LeayReLU 函数退化为 ReLU 函数；当𝑝 ≠ 0时， 𝑥 &lt; 0处能够获得较小的导数值𝑝，从而避免出现梯度弥散现象</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200505031651370.png" alt="LeakyReLU函数曲线"></p>
<p>可以通过<code>tf.nn.leaky_relu</code>实现 LeakyReLU 函数：</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.linspace(-6.,6.,10) # 构造-6~6 的输入向量
&gt;&gt;&gt; tf.nn.leaky_relu(x, alpha=0.1) # 通过 LeakyReLU 激活函数
&lt;tf.Tensor: id=13, shape=(10,), dtype=float32, numpy=
array([-0.6 , -0.46666667, -0.33333334, -0.2 , -0.06666666, 0.666667 , 2. , 3.333334 , 4.666667 , 6. ], dtype=float32)&gt;</code></pre>
<ul>
<li>alpha 参数代表𝑝</li>
</ul>
<p>tf.nn.leaky_relu 对应的类为 <code>layers.LeakyReLU</code>，可以通过LeakyReLU(alpha)创建LeakyReLU 网络层，并设置𝑝参数，像 Dense 层一样将 LeakyReLU层放置在网络的合适位置</p>
<h4 id="4-Tanh"><a href="#4-Tanh" class="headerlink" title="4.Tanh"></a>4.Tanh</h4><p>Tanh 函数能够将𝑥 ∈ 𝑅的输入“压缩” 到(-1,1)区间，定义为：$tanh(x)\doteq\frac{e^x-e^{-x}}{e^x+e^{-x}}=2sigmoid(2x)-1$（tanh 激活函数可通过 Sigmoid 函数缩放平移后实现）</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200505032201454.png" alt="tanh函数曲线"></p>
<p>可以通过 tf.nn.tanh 实现 tanh 函数：</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.linspace(-6.,6.,10) # 构造-6~6 的输入向量
&gt;&gt;&gt; tf.nn.tanh(x) # 通过 tanh 激活函数
&lt;tf.Tensor: id=15, shape=(10,), dtype=float32, numpy=
array([-0.9999877 , -0.99982315, -0.997458 , -0.9640276 , -0.58278286, 0.5827831 , 0.9640276 , 0.997458 , 0.99982315, 0.9999877 ], dtype=float32)&gt;</code></pre>
<h3 id="5、输出层设计"><a href="#5、输出层设计" class="headerlink" title="5、输出层设计"></a>5、输出层设计</h3><p>常见的几种输出类型包括：</p>
<ul>
<li>𝑜𝑖 ∈ $𝑅^𝑑$ 输出属于整个实数空间，或者某段普通的实数空间，比如函数值趋势的预<br>  测，年龄的预测问题等</li>
<li>𝑜𝑖 ∈ [0,1] 输出值特别地落在[0, 1]的区间， 如图片生成，图片像素值一般用[0, 1]区间<br>  的值表示；或者二分类问题的概率，如硬币正反面的概率预测问题</li>
<li>𝑜𝑖 ∈ [0,  1],   𝑖 𝑜𝑖 = 1 输出值落在[0, 1]的区间， 并且所有输出值之和为 1， 常见的如<br>  多分类问题，如 MNIST 手写数字图片识别，图片属于 10 个类别的概率之和应为 1</li>
<li>𝑜𝑖 ∈ [-1,  1] 输出值在[-1, 1]之间</li>
</ul>
<h4 id="1-普通实数空间"><a href="#1-普通实数空间" class="headerlink" title="1.普通实数空间"></a>1.普通实数空间</h4><p>该类问题较普遍，输出层可以不加激活函数。</p>
<p>误差的计算直接基于最后一层的输出𝒐和真实值𝒚进行计算， 如采用均方差误差函数度量输出值𝒐与真实值𝒚之间的距离：ℒ = 𝑔(𝒐, 𝒚)，其中𝑔代表了某个具体的误差计算函数，例如 MSE 等</p>
<h4 id="2-0-1-区间"><a href="#2-0-1-区间" class="headerlink" title="2.[0,1]区间"></a>2.[0,1]区间</h4><p>为了让像素的值范围映射到[0,1]的有效实数空间，需要在输出层后添加某个合适的激活函数𝜎，其中 Sigmoid 函数刚好具有此功能</p>
<p>对于二分类问题，如硬币的正反面的预测， 输出层可以<strong>只设置一个节点</strong>，表示某个事件 A 发生的概率𝑃(A|𝒙)， 𝒙为网络输入。假设网络的输出标量𝑜表示正面事件出现的概率，则反面事件出现的概率即为1 - 𝑜，网络结构如下所示</p>
<ul>
<li>𝑃(正面|𝒙) = 𝑜</li>
<li>𝑃(反面|𝒙) = 1 - 𝑜</li>
</ul>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200505180332392.png" alt="单输出节点的二分类网络"></p>
<ul>
<li>输出层的净活性值𝑧后添加 Sigmoid 函数即可将输出转译为概率值</li>
</ul>
<h4 id="3-0-1-区间，和为1"><a href="#3-0-1-区间，和为1" class="headerlink" title="3.[0,1]区间，和为1"></a>3.[0,1]区间，和为1</h4><p>输出值𝑜𝑖 ∈ [0,1]， 且所有输出值之和为 1，这种设定以多分类问题最为常见</p>
<p>可以通过在输出层添加 Softmax 函数实现：$Softmax(z_i)=\frac{e^{Z_i}}{\sum^{d_{out}}_{j=1}e^{Z_j}}$</p>
<ul>
<li>Softmax 函数不仅可以将输出值映射到[0,1]区间，还满足所有的输出值之和为 1 的特性</li>
<li>通过 Softmax函数可以将输出层的输出转译为类别概率，在分类问题中使用的非常频繁</li>
</ul>
<p>可以通过 tf.nn.softmax 实现 Softmax 函数：</p>
<pre><code class="python">&gt;&gt;&gt; z = tf.constant([2.,1.,0.1])
&gt;&gt;&gt; tf.nn.softmax(z) # 通过 Softmax 函数
&lt;tf.Tensor: id=19, shape=(3,), dtype=float32, numpy=array([0.6590012, 0.242433 , 0.0985659], dtype=float32)&gt;</code></pre>
<p>与 Dense 层类似， Softmax 函数也可以作为网络层类使用， 通过类<code>layers.Softmax(axis=-1)</code>可以方便添加 Softmax 层，其中 axis 参数指定需要进行计算的维度</p>
<p>在 Softmax 函数的数值计算过程中，容易因输入值偏大发生<strong>数值溢出</strong>现象；在计算交叉熵时，也会出现数值溢出的问题。为了数值计算的稳定性， TensorFlow 中提供了一个统一的接口，将 Softmax 与交叉熵损失函数同时实现，同时也处理了数值不稳定的异常，一般推荐使用这些接口函数，避免分开使用 Softmax 函数与交叉熵损失函数。函数式接口为<code>tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False)</code></p>
<ul>
<li>y_true：One-hot 编码后的真实标签</li>
<li>y_pred：网络的预测值<ul>
<li>当 from_logits 设置为 True 时，y_pred 表示须为未经过 Softmax 函数的变量 z</li>
<li>当 from_logits 设置为 False 时， y_pred 表示为经过 Softmax 函数的输出</li>
</ul>
</li>
<li>为了数值计算稳定性，一般设置 from_logits 为 True（此时<code>tf.keras.losses.categorical_crossentropy</code>将在内部进行 Softmax 函数计算，所以不需要在模型中显式调用 Softmax 函数）</li>
</ul>
<pre><code class="python">&gt;&gt;&gt; z = tf.random.normal([2,10]) # 构造输出层的输出
&gt;&gt;&gt; y_onehot = tf.constant([1,3]) # 构造真实值
&gt;&gt;&gt; y_onehot = tf.one_hot(y_onehot, depth=10) # one-hot 编码
# 输出层未使用 Softmax 函数，故 from_logits 设置为 True
# 这样 categorical_crossentropy 函数在计算损失函数前，会先内部调用 Softmax 函数
&gt;&gt;&gt; loss = keras.losses.categorical_crossentropy(y_onehot,z,from_logits=True)
&gt;&gt;&gt; loss = tf.reduce_mean(loss) # 计算平均交叉熵损失
&lt;tf.Tensor: id=210, shape=(), dtype=float32, numpy= 2.4201946&gt;</code></pre>
<p>除了函数式接口， 也可以利用<code>losses.CategoricalCrossentropy(from_logits)</code>类方式同时实现 Softmax 与交叉熵损失函数的计算， from_logits 参数的设置方式相同</p>
<pre><code class="python"># 创建 Softmax 与交叉熵计算类，输出层的输出 z 未使用 softmax
&gt;&gt;&gt; criteon = keras.losses.CategoricalCrossentropy(from_logits=True)
&gt;&gt;&gt; loss = criteon(y_onehot,z) # 计算损失
&lt;tf.Tensor: id=258, shape=(), dtype=float32, numpy= 2.4201946&gt;</code></pre>
<h4 id="4-1-1-区间"><a href="#4-1-1-区间" class="headerlink" title="4.[-1,1]区间"></a>4.[-1,1]区间</h4><p>使用 tanh 激活函数即可：</p>
<pre><code class="python">&gt;&gt;&gt; x = tf.linspace(-6.,6.,10)
&gt;&gt;&gt; tf.tanh(x) # tanh 激活函数
&lt;tf.Tensor: id=264, shape=(10,), dtype=float32, numpy=
array([-0.9999877 , -0.99982315, -0.997458 , -0.9640276 , -0.58278286, 0.5827831 , 0.9640276 , 0.997458 , 0.99982315, 0.9999877 ], dtype=float32)&gt;</code></pre>
<h3 id="6、误差计算"><a href="#6、误差计算" class="headerlink" title="6、误差计算"></a>6、误差计算</h3><p>常见的误差函数有均方差、 交叉熵、 KL 散度、 Hinge Loss 函数等，其中均方差函数和交叉熵函数在深度学习中比较常见，均方差函数主要用于回归问题，交叉熵函数主要用于分类问题</p>
<h4 id="1-均方差误差函数"><a href="#1-均方差误差函数" class="headerlink" title="1.均方差误差函数"></a>1.均方差误差函数</h4><p>均方差(Mean Squared Error，简称 MSE)误差函数：把输出向量和真实向量映射到笛卡尔坐标系的两个点上，通过计算这两个点之间的欧式距离(准确地说是欧式距离的平方)来衡量两个向量之间的差距：$MSE(y,o)=\frac{1}{d_{out}}\sum^{d_{out}}_{i=1}(y_i-o_i)^2$</p>
<ul>
<li>MSE 误差函数的值总是大于等于 0</li>
<li>当 MSE 函数达到最小值 0 时， 输出等于真实标签，此时神经网络的参数达到最优状态</li>
</ul>
<p>可以通过函数方式或层方式实现 MSE 误差计算。通过函数式调用：</p>
<pre><code class="python">&gt;&gt;&gt; o = tf.random.normal([2,10]) # 构造网络输出
&gt;&gt;&gt; y_onehot = tf.constant([1,3]) # 构造真实值
&gt;&gt;&gt; y_onehot = tf.one_hot(y_onehot, depth=10)
&gt;&gt;&gt; loss = keras.losses.MSE(y_onehot, o) # 计算均方差
&lt;tf.Tensor: id=27, shape=(2,), dtype=float32, numpy=array([0.779179 ,
1.6585705], dtype=float32)&gt;</code></pre>
<ul>
<li><p>MSE 函数返回的是每个样本的均方差</p>
</li>
<li><p>可以在样本维度上再次平均来获得平均样本的均方差</p>
<pre><code class="python">  &gt;&gt;&gt; loss = tf.reduce_mean(loss) # 计算 batch 均方差
  &lt;tf.Tensor: id=30, shape=(), dtype=float32, numpy=1.2188747&gt;</code></pre>
</li>
</ul>
<p>通过层方式实现，对应的类为<code>keras.losses.MeanSquaredError()</code>，和其他层的类一样，调用<code>__call__</code>函数即可完成前向计算：</p>
<pre><code class="python"># 创建 MSE 类
&gt;&gt;&gt; criteon = keras.losses.MeanSquaredError()
&gt;&gt;&gt; loss = criteon(y_onehot,o) # 计算 batch 均方差
&lt;tf.Tensor: id=54, shape=(), dtype=float32, numpy=1.2188747&gt;</code></pre>
<h4 id="2-交叉熵误差函数"><a href="#2-交叉熵误差函数" class="headerlink" title="2.交叉熵误差函数"></a>2.交叉熵误差函数</h4><p>熵，在信息论中，用来衡量信息的不确定度。 熵在信息学科中也叫信息熵，或者香农熵。熵越大，代表不确定性越大，信息量也就越大。 某个分布𝑃(𝑖)的熵定义为：$H(P)=-\sum_iP(i)log_2P(i)$</p>
<ul>
<li>对于确定的分布，熵取得最小值0，不确定性为0</li>
<li>由于𝑃(𝑖) ∈ [0,1], $log_2P(i)$ ≤ 0，因此熵𝐻(𝑃)总是大于等于 0</li>
<li>在 TensorFlow 中，可以用<code>tf.math.log</code>来计算熵</li>
</ul>
<p>交叉熵(Cross Entropy)的定义：$H(p||q)=H(p)+D_{KL}(p||q)$</p>
<ul>
<li>其中$D_{KL}(p||q)$为𝑝与𝑞的 KL 散度(Kullback-Leibler Divergence)：$D_{KL}(p||q)=\sum_ip(i)log(\frac{p(i)}{q(i)})$<ul>
<li>KL 散度是用于衡量 2 个分布之间距离的指标：𝑝 = 𝑞时，$D_{KL}(p||q)$取得最小值 0， 𝑝与𝑞之间的差距越大，$D_{KL}(p||q)$也越大</li>
<li>交叉熵和 KL 散度都不是对称的：<ul>
<li>$H(p||q)\neq H(q||p)$</li>
<li>$D_{KL}(p||q)\neq D_{KL}(q||p)$</li>
</ul>
</li>
</ul>
</li>
<li>当分类问题中 y 的编码分布𝑝采用 One-hot 编码𝒚时： 𝐻(𝑝) = 0</li>
</ul>
<p>分类问题中交叉熵的计算表达式：$H(p||q)=D_{KL}(p||q)=-logo_i$</p>
<ul>
<li>𝑖为 One-hot 编码中为 1 的索引号，也是当前输入的真实类别</li>
<li>ℒ只与真实类别𝑖上的概率𝑜𝑖有关， 对应概率𝑜𝑖越大， 𝐻(𝑝||𝑞)越小</li>
<li>当对应类别上的概率为 1 时， 交叉熵𝐻(𝑝||𝑞)取得最小值 0，此时网络输出𝒐与真实标签𝒚完全一致，神经网络取得最优状态</li>
<li>最小化交叉熵损失函数的过程也是最大化正确类别的预测概率的过程</li>
</ul>
<h3 id="7、神经网络类型"><a href="#7、神经网络类型" class="headerlink" title="7、神经网络类型"></a>7、神经网络类型</h3><p>全连接层是神经网络最基本的网络类型，优点是全连接层前向计算流程相对简单，梯度求导也较简单，缺点是在处理较大特征长度的数据时， 全连接层的参数量往往较大</p>
<h4 id="1-卷积神经网络"><a href="#1-卷积神经网络" class="headerlink" title="1.卷积神经网络"></a>1.卷积神经网络</h4><p>全连接层在处理高维度的图片、 视频数据时往往出现网络参数量巨大，训练非常困难。通过利用局部相关性和权值共享的思想，Yann Lecun在1986年提出了卷积神经网络(Convolutional Neural Network， 简称 CNN)</p>
<p>其中比较流行的模型：</p>
<ul>
<li>用于图片分类的 AlexNet、 VGG、 GoogLeNet、 ResNet、 DenseNet 等</li>
<li>用于目标识别的 RCNN、 Fast RCNN、 Faster RCNN、 Mask RCNN、 YOLO、 SSD 等</li>
</ul>
<h4 id="2-循环神经网络"><a href="#2-循环神经网络" class="headerlink" title="2.循环神经网络"></a>2.循环神经网络</h4><p>除了具有空间结构的图片、 视频等数据外，序列信号也是非常常见的一种数据类型，其中一个最具代表性的序列信号就是文本数据。卷积神经网络由于缺乏 Memory 机制和处理不定长序列信号的能力，并不擅长序列信号的任务。循环神经网络(Recurrent Neural Network，被证明非常擅长处理序列信号</p>
<p>1997年提出的LSTM网络，作为 RNN 的变种，较好地克服了 RNN 缺乏长期记忆、 不擅长处理长序列的问题，在自然语言处理中得到了广泛的应用。基于LSTM 模型， Google 提出了用于机器翻译的 Seq2Seq 模型，并成功商用于谷歌神经机器翻译系统(GNMT)</p>
<p>其他的 RNN 变种还有 GRU、 双向 RNN 等</p>
<h4 id="3-注意力（机制）网络"><a href="#3-注意力（机制）网络" class="headerlink" title="3.注意力（机制）网络"></a>3.注意力（机制）网络</h4><p>RNN 并不是自然语言处理的最终解决方案，近年来随着注意力机制(Attention Mechanism)的提出，克服了 RNN 训练不稳定、 难以并行化等缺陷，在自然语言处理和图片生成等领域中逐渐崭露头角</p>
<p>2017 年， Google 提出了第一个利用纯注意力机制实现的网络模型Transformer，随后基于 Transformer 模型相继提出了一系列的用于机器翻译的注意力网络模型，如 GPT、 BERT、 GPT-2 等</p>
<p>在其它领域，基于注意力机制，尤其是自注意力(SelfAttention)机制构建的网络也取得了不错的效果，比如基于自注意力机制的 BigGAN 模型等</p>
<h4 id="4-图卷积神经网络"><a href="#4-图卷积神经网络" class="headerlink" title="4.图卷积神经网络"></a>4.图卷积神经网络</h4><p>图片、 文本等数据具有规则的空间、时间结构，称为 Euclidean Data(欧几里德数据)。卷积神经网络和循环神经网络被证明非常擅长处理这种类型的数据。而像类似于社交网络、 通信网络、 蛋白质分子结构等一系列的不规则空间拓扑结构的数据， 它们显得力不从心。 2016 年，基于前人在一阶近似的谱卷积算法上提出了图卷积网络(Graph Convolution Network， GCN)模型。 GCN 算法实现简单，从空间一阶邻居信息聚合的角度也能直观地理解，在半监督任务上取得了不错效果。随后，一系列的网络模型相继被提出，如 GAT， EdgeConv， DeepGCN 等</p>
<h2 id="Keras高层接口"><a href="#Keras高层接口" class="headerlink" title="Keras高层接口"></a>Keras高层接口</h2><p>Keras 与 tf.keras 的区别与联系：</p>
<ul>
<li>Keras 可以理解为一套搭建与训练神经网络的高层 API 协议， Keras 本身已经实现了此协议， 安装标准的 Keras 库就可以方便地调用TensorFlow、 CNTK 等后端完成加速计算</li>
<li>在 TensorFlow 中，也实现了一套 Keras 协议，即 tf.keras，它与 TensorFlow 深度融合，且只能基于 TensorFlow 后端运算， 并对TensorFlow 的支持更完美。 对于使用 TensorFlow 的开发者来说， tf.keras 可以理解为一个普通的子模块，与其他子模块，如 tf.math， tf.data 等并没有什么差别。 下文如无特别说明，Keras <strong>均指代 tf.keras</strong>，而不是标准的 Keras 库</li>
</ul>
<h3 id="1、常见功能模块"><a href="#1、常见功能模块" class="headerlink" title="1、常见功能模块"></a>1、常见功能模块</h3><p>Keras 提供了一系列高层的神经网络相关类和函数，如经典数据集加载函数（进阶操作–&gt;经典数据集加载 章节中讲到过）、 网络层类、 模型容器、 损失函数类、 优化器类、 经典模型类等</p>
<h4 id="1-常见网络层类"><a href="#1-常见网络层类" class="headerlink" title="1.常见网络层类"></a>1.常见网络层类</h4><p>对于常见的神经网络层，可以使用张量方式的底层接口函数来实现，这些接口函数一般在<code>tf.nn</code>模块中。</p>
<p>对于常见的<strong>网络层</strong>，我们一般直接使用层方式来完成模型的搭建，在<code>tf.keras.layers</code>命名空间(下文使用 <code>layers</code> 指代 <code>tf.keras.layers</code>)中提供了大量常见网络层的类，如全连接层、 激活函数层、 池化层、 卷积层、 循环神经网络层等。对于这些网络层类，只需要在创建时指定网络层的相关参数， 并调用<code>__call__</code>方法即可完成前向计算。在调用<code>__call__</code>方法时， Keras 会自动调用每个层的前向传播逻辑，这些逻辑一般实现在类的call 函数中。</p>
<p>以 Softmax 层为例， 它既可以使用<code>tf.nn.softmax</code>函数在前向传播逻辑中完成Softmax运算， 也可以通过<code>layers.Softmax(axis)</code>类搭建Softmax网络层，其中<code>axis</code>参数指定进行softmax 运算的维度：</p>
<pre><code class="python">&gt;&gt;&gt; import tensorflow as tf
# 导入 keras 模型，不能使用 import keras，它导入的是标准的 Keras 库
&gt;&gt;&gt; from tensorflow import keras
&gt;&gt;&gt; from tensorflow.keras import layers # 导入常见网络层类
# 创建 Softmax 层，并调用__call__方法完成前向计算
&gt;&gt;&gt; x = tf.constant([2.,1.,0.1]) # 创建输入张量
&gt;&gt;&gt; layer = layers.Softmax(axis=-1) # 创建 Softmax 层
&gt;&gt;&gt; out = layer(x) # 调用 softmax 前向计算，输出为 out
# 经过 Softmax 网络层后， 得到概率分布 out 为：
&lt;tf.Tensor: id=2, shape=(3,), dtype=float32, numpy=array([0.6590012, 0.242433 , 0.0985659], dtype=float32)&gt;
# 当然，也可以直接通过 tf.nn.softmax()函数完成计算，代码如下：
&gt;&gt;&gt; out = tf.nn.softmax(x) # 调用 softmax 函数完成前向计算</code></pre>
<h4 id="2-网络容器"><a href="#2-网络容器" class="headerlink" title="2.网络容器"></a>2.网络容器</h4><p>当网络层数变得较深时，手动调用每一层的类实例完成前向传播运算这部分代码显得非常臃肿。可以通过 Keras 提供的网络容器 <code>Sequential</code> 将多个网络层封装成一个大网络模型，只需要调用网络模型的实例一次即可完成数据从第一层到最末层的顺序传播运算  </p>
<pre><code class="python"># 导入 Sequential 容器
from tensorflow.keras import layers, Sequential
network = Sequential([ # 封装为一个网络
    layers.Dense(3, activation=None), # 全连接层，此处不使用激活函数
    layers.ReLU(),#激活函数层
    layers.Dense(2, activation=None), # 全连接层，此处不使用激活函数
    layers.ReLU() #激活函数层
])
x = tf.random.normal([4,3])
out = network(x) # 输入从第一层开始， 逐层传播至输出层，并返回输出层的输出</code></pre>
<p>Sequential 容器也可以通过<code>add()</code>方法继续追加新的网络层， 实现动态创建网络的功能：</p>
<pre><code class="python">layers_num = 2 # 堆叠 2 次
network = Sequential([]) # 先创建空的网络容器
for _ in range(layers_num):
    network.add(layers.Dense(3)) # 添加全连接层
    network.add(layers.ReLU())# 添加激活函数层
network.build(input_shape=(4, 4)) # 创建网络参数
network.summary()</code></pre>
<ul>
<li><p>在完成网络创建时， 网络层类并没有创建内部权值张量等成员变量，此时通过调用类的<code>build</code>方法并指定输入大小，即可自动创建所有层的内部张量</p>
</li>
<li><p>通过Sequential容量封装多个网络层时，每层的参数列表将会自动并入Sequential容器的参数列表中，不需要人为合并网络参数列表</p>
</li>
<li><p>Sequential 对象的<code>trainable_variables</code>和<code>variables</code>包含了所有层的待优化张量列表和全部张量列表</p>
<pre><code class="python">  # 打印网络的待优化参数名与 shape
  &gt;&gt;&gt; for p in network.trainable_variables:
  ...     print(p.name, p.shape) # 参数名和形状
  ...
  dense/kernel:0 (4, 3)
  dense/bias:0 (3,)
  dense_1/kernel:0 (3, 3)
  dense_1/bias:0 (3,)</code></pre>
</li>
<li><p>通过<code>summary()</code>函数可以方便打印出网络结构和参数量，输出：</p>
<pre><code class="python">  Model: &quot;sequential&quot;
  _________________________________________________________________
  Layer (type)                 Output Shape              Param #
  =================================================================
  dense (Dense)                multiple                  15
  _________________________________________________________________
  re_lu (ReLU)                 multiple                  0
  _________________________________________________________________
  dense_1 (Dense)              multiple                  12
  _________________________________________________________________
  re_lu_1 (ReLU)               multiple                  0
  =================================================================
  Total params: 27
  Trainable params: 27
  Non-trainable params: 0
  _________________________________________________________________</code></pre>
<ul>
<li><code>Layer</code>：每层的名字，由 TensorFlow 内部维护，与 Python 的对象名并不一样</li>
<li><code>Param#</code>：层的参数个数</li>
<li><code>Total params</code>：统计出了总的参数量</li>
<li><code>Trainable params</code>：总的待优化参数量</li>
<li><code>Non-trainable params</code>：总的不需要优化的参数量</li>
</ul>
</li>
</ul>
<h3 id="2、模型装配、训练与测试"><a href="#2、模型装配、训练与测试" class="headerlink" title="2、模型装配、训练与测试"></a>2、模型装配、训练与测试</h3><p>在训练网络时，一般的流程是通过前向计算获得网络的输出值， 再通过损失函数计算网络误差，然后通过自动求导工具计算梯度并更新，同时间隔性地测试网络的性能。对于这种常用的训练逻辑，可以直接通过 Keras 提供的模型装配与训练等高层接口实现</p>
<h4 id="1-模型装配"><a href="#1-模型装配" class="headerlink" title="1.模型装配"></a>1.模型装配</h4><p>在 Keras 中，有 2 个比较特殊的类：</p>
<ul>
<li>keras.Model类：<strong>网络的母类</strong>，除了具有Layer类的功能，还添加了保存模型、加载模型、 训练与测试模型等便捷功能。<u>Sequential也是Model的子类</u>（具有Model类的所有功能）</li>
<li>keras.layers.Layer类：<strong>网络层的母类</strong>，定义了网络层的一些常见功能，如添加权值、 管理权值列表等</li>
</ul>
<p>下面介绍 Model 及其子类的模型装配与训练功能</p>
<p>创建网络：</p>
<pre><code class="python"># 创建 5 层的全连接网络
network = Sequential([layers.Dense(256, activation=&#39;relu&#39;),
                      layers.Dense(128, activation=&#39;relu&#39;),
                      layers.Dense(64, activation=&#39;relu&#39;),
                      layers.Dense(32, activation=&#39;relu&#39;),
                      layers.Dense(10)])
network.build(input_shape=(4, 28*28))
network.summary()</code></pre>
<p>通过<code>compile</code>函数指定网络使用的优化器对象、 损失函数类型， 评价指标等设定，这一步称为<strong>装配</strong></p>
<pre><code class="python"># 导入优化器，损失函数模块
from tensorflow.keras import optimizers,losses
# 模型装配
# 采用 Adam 优化器，学习率为 0.01;采用交叉熵损失函数，包含 Softmax
network.compile(optimizer=optimizers.Adam(lr=0.01),
                loss=losses.CategoricalCrossentropy(from_logits=True),
                metrics=[&#39;accuracy&#39;] # 设置测量指标为准确率
               )</code></pre>
<h4 id="2-模型训练"><a href="#2-模型训练" class="headerlink" title="2.模型训练"></a>2.模型训练</h4><p>模型装配完成后，可通过<code>fit()</code>函数送入待训练的数据集和验证用的数据集，实现网络的训练与验证，这一步称为<strong>模型训练</strong></p>
<pre><code class="python"># 指定训练集为 train_db，验证集为 val_db,训练 5 个 epochs，每 2 个 epoch 验证一次
# 返回训练轨迹信息保存在 history 对象中
history = network.fit(train_db, epochs=5, validation_data=val_db,
                      validation_freq=2)</code></pre>
<ul>
<li><p>train_db：tf.data.Dataset对象，也可以传入Numpy Array类型的数据</p>
</li>
<li><p>epochs：指定训练迭代的Epoch数量</p>
</li>
<li><p>validation_data：指定用于验证(测试)的数据集</p>
</li>
<li><p>validation_freq：验证的频率</p>
</li>
<li><p>history：训练过程的数据记录，其中<code>history.history</code>为字典对象，包含了训练过程中的loss、测量指标等记录项</p>
<pre><code class="python">  &gt;&gt;&gt; history.history # 打印训练记录
  # 历史训练准确率
  {&#39;accuracy&#39;: [0.00011666667, 0.0, 0.0, 0.010666667, 0.02495],
   &#39;loss&#39;: [2465719710540.5845, # 历史训练误差
            78167808898516.03,
            404488834518159.6,
            1049151145155144.4,
            1969370184858451.0],
   &#39;val_accuracy&#39;: [0.0, 0.0], # 历史验证准确率
   # 历史验证误差
   &#39;val_loss&#39;: [197178788071657.3, 1506234836955706.2]}</code></pre>
</li>
</ul>
<p><code>fit()</code>函数的运行代表了网络的训练过程，会消耗相当的训练时间，并在训练结束后才返回</p>
<h4 id="3-模型测试"><a href="#3-模型测试" class="headerlink" title="3.模型测试"></a>3.模型测试</h4><p>关于验证和测试的区别，会在过拟合一章详细阐述，此处可以将验证和测试理解为模型评估的一种方式</p>
<p>通过<code>Model.predict(x)</code>方法即可完成模型的<strong>预测</strong>：</p>
<pre><code class="python"># 加载一个 batch 的测试数据
x,y = next(iter(db_test))
print(&#39;predict x:&#39;, x.shape) # 打印当前 batch 的形状
out = network.predict(x) # 模型预测，预测结果保存在 out 中
print(out)</code></pre>
<p>如果只是简单的测试模型的<strong>性能</strong>，可以通过<code>Model.evaluate(db)</code>循环测试完db数据集上所有样本，并打印出性能指标：</p>
<pre><code class="python">network.evaluate(db_test) # 模型测试，测试在 db_test 上的性能表现</code></pre>
<h3 id="3、模型保存与加载"><a href="#3、模型保存与加载" class="headerlink" title="3、模型保存与加载"></a>3、模型保存与加载</h3><h4 id="1-张量方式"><a href="#1-张量方式" class="headerlink" title="1.张量方式"></a>1.张量方式</h4><p>网络的状态主要体现在网络的结构以及网络层内部张量数据上，因此在<u>拥有网络结构源文件</u>的条件下，直接保存网络张量参数到文件系统上是最轻量级的一种方式</p>
<p>通过调用<code>Model.save_weights(path)</code>方法，可将当前的网络参数保存到path文件上</p>
<pre><code class="python">network.save_weights(&#39;weights.ckpt&#39;) # 保存模型的所有张量数据</code></pre>
<p>在需要的时候，先创建好网络对象，然后调用网络对象的<code>load_weights(path)</code>方法即可将指定的模型文件中保存的张量数值写入到当前网络参数中</p>
<pre><code class="python"># 重新创建相同的网络结构
new_network = Sequential([layers.Dense(256, activation=&#39;relu&#39;),
                          layers.Dense(128, activation=&#39;relu&#39;),
                          layers.Dense(64, activation=&#39;relu&#39;),
                          layers.Dense(32, activation=&#39;relu&#39;),
                          layers.Dense(10)])
new_network.compile(optimizer=optimizers.Adam(lr=0.01),
                    loss=tf.losses.CategoricalCrossentropy(from_logits=True),
                    metrics=[&#39;accuracy&#39;]
                   )
# 从参数文件中读取数据并写入当前网络
new_network.load_weights(&#39;weights.ckpt&#39;)
print(&#39;loaded weights!&#39;)</code></pre>
<h4 id="2-网络方式"><a href="#2-网络方式" class="headerlink" title="2.网络方式"></a>2.网络方式</h4><p>通过<code>Model.save(path)</code>函数可以将模型的<strong>结构</strong>以及模型的<strong>参数</strong>保存到path文件上，在<u>不需要网络源文件</u>的条件下，通过<code>keras.models.load_model(path)</code>即可恢复网络结构和网络参数</p>
<pre><code class="python"># 保存模型结构与模型参数到文件
network.save(&#39;model.h5&#39;)
print(&#39;saved total model.&#39;)
del network # 删除网络对象
# 从文件恢复网络结构与网络参数
network = keras.models.load_model(&#39;model.h5&#39;)</code></pre>
<h4 id="3-SavedModel方式"><a href="#3-SavedModel方式" class="headerlink" title="3.SavedModel方式"></a>3.SavedModel方式</h4><p>当需要将模型部署到其他平台时，采用SavedModel方式更具有平台无关性。</p>
<p>通过<code>tf.saved_model.save(network, path)</code>即可将模型以SavedModel方式保存到path<strong>目录</strong>中</p>
<pre><code class="python"># 保存模型结构与模型参数到文件
tf.saved_model.save(network, &#39;model-savedmodel&#39;)
print(&#39;saving savedmodel.&#39;)
del network # 删除网络对象</code></pre>
<p>通过<code>tf.saved_model.load</code>函数即可恢复出模型对象，我们在恢复出模型实例后，完成测试准确率的计算</p>
<pre><code class="python"># 从文件恢复网络结构与网络参数
network = tf.saved_model.load(&#39;model-savedmodel&#39;)
# 准确率计量器
acc_meter = metrics.CategoricalAccuracy()
for x,y in ds_val: # 遍历测试集
    pred = network(x) # 前向计算
    acc_meter.update_state(y_true=y, y_pred=pred) # 更新准确率统计
# 打印准确率
print(&quot;Test Accuracy:%f&quot; % acc_meter.result())</code></pre>
<h3 id="4、自定义网络"><a href="#4、自定义网络" class="headerlink" title="4、自定义网络"></a>4、自定义网络</h3><p>对于需要创建自定义逻辑的网络层，可以通过<strong>自定义类</strong>来实现</p>
<ul>
<li>在创建自定义<strong>网络层类</strong>时，需要继承自 layers.Layer 基类</li>
<li>创建自定义的<strong>网络类</strong>时，需要继承自 keras.Model 基类</li>
</ul>
<h4 id="1-自定义网络层"><a href="#1-自定义网络层" class="headerlink" title="1.自定义网络层"></a>1.自定义网络层</h4><p>对于自定义的网络层， 需要实现初始化<code>__init__</code>方法和前向传播逻辑<code>call</code>方法</p>
<p>以某个具体的自定义网络层为例，假设需要一个没有偏置向量的全连接层，同时固定激活函数为 ReLU 函数：</p>
<ol>
<li><p>首先创建类，并继承自 Layer 基类。创建初始化方法，并调用母类的初始化函数。由于是全连接层， 因此需要设置两个参数：输入特征的长度inp_dim和输出特征的长度outp_dim，并通过<code>self.add_variable(name, shape)</code>创建 shape 大小，名字为 name 的张量𝑾，并设置为需要优化</p>
<pre><code class="python"> class MyDense(layers.Layer):
     # 自定义网络层
     def __init__(self, inp_dim, outp_dim):
         super(MyDense, self).__init__() # 调用母类的初始化函数
         # 创建权值张量并添加到类管理列表中，设置为需要优化
         self.kernel = self.add_variable(&#39;w&#39;, [inp_dim, outp_dim],
                                         trainable=True)
         # 此外，通过 tf.Variable 创建的类成员也会自动加入类参数列表
         # self.kernel = tf.Variable(tf.random.normal([inp_dim, outp_dim]),
         #                           trainable=False)</code></pre>
<ul>
<li>self.add_variable会返回张量𝑾的 Python 引用</li>
<li>变量名 name 由TensorFlow 内部维护， 使用的比较少</li>
<li>trainable：创建的张量是否需要优化</li>
</ul>
</li>
<li><p>设计自定义类的前向运算逻辑。对于本例，只需要完成𝑶 = 𝑿@𝑾矩阵运算，并通过固定的ReLU激活函数即可</p>
<pre><code class="python"> def call(self, inputs, training=None):
     # 实现自定义类的前向计算逻辑
     # X@W
     out = inputs @ self.kernel
     # 执行激活函数运算
     out = tf.nn.relu(out)
     return out</code></pre>
<ul>
<li>inputs：输入， 由用户在调用时传入</li>
<li>training：用于指定模型的状态：<ul>
<li>True：执行训练模式</li>
<li>False：执行测试模式，默认参数为 None，即测试模式</li>
</ul>
</li>
<li>由于全连接层的训练模式和测试模式逻辑一致，此处不需要额外处理。对于部份测试模式和训练模式不一致的网络层，需要根据 training 参数来设计需要执行的逻辑</li>
</ul>
</li>
</ol>
<p>此时可以实例化 MyDense 类，并查看其参数列表：</p>
<pre><code class="python">net = MyDense(4,3) # 创建输入为 4，输出为 3 节点的自定义层
net.variables,net.trainable_variables # 查看自定义层的参数列表</code></pre>
<h4 id="2-自定义网络"><a href="#2-自定义网络" class="headerlink" title="2.自定义网络"></a>2.自定义网络</h4><p>自定义网络类可以和其他标准类一样，通过 Sequential 容器方便地封装成一个网络模型：</p>
<pre><code class="python">network = Sequential([MyDense(784, 256), # 使用自定义的层
                      MyDense(256, 128),
                      MyDense(128, 64),
                      MyDense(64, 32),
                      MyDense(32, 10)])
network.build(input_shape=(None, 28*28))
network.summary()</code></pre>
<p>通过堆叠自定义网络层类，可以实现 5 层的全连接层网络，每层全连接层无偏置张量，同时激活函数固定地使用 ReLU 函数</p>
<hr>
<p>Sequential 容器适合于数据按序从第一层传播到第二层，再从第二层传播到第三层，以此规律传播的网络模型。对于复杂的网络结构，例如第三层的输入不仅是第二层的输出，还有第一层的输出，此时使用自定义网络更加灵活：</p>
<ol>
<li><p>创建自定义网络类，首先创建类， 并继承自 Model 基类，分别创建对应的网络层对象：</p>
<pre><code class="python"> class MyModel(keras.Model):
     # 自定义网络类，继承自 Model 基类
     def __init__(self):
         super(MyModel, self).__init__() # 调用母类的初始化函数
         # 完成网络内需要的网络层的创建工作
         self.fc1 = MyDense(28*28, 256)
         self.fc2 = MyDense(256, 128)
         self.fc3 = MyDense(128, 64)
         self.fc4 = MyDense(64, 32)
         self.fc5 = MyDense(32, 10)</code></pre>
</li>
<li><p>实现自定义网络的前向运算逻辑：</p>
<pre><code class="python"> def call(self, inputs, training=None):
     # 自定义前向运算逻辑
     x = self.fc1(inputs)
     x = self.fc2(x)
     x = self.fc3(x)
     x = self.fc4(x)
     x = self.fc5(x)
     return x</code></pre>
</li>
</ol>
<h3 id="5、模型乐园"><a href="#5、模型乐园" class="headerlink" title="5、模型乐园"></a>5、模型乐园</h3><p>对于常用的网络模型，如 ResNet、 VGG 等，不需要手动创建网络，可以直接从<code>keras.applications</code>子模块中通过一行代码即可创建并使用这些经典模型，同时还可以通过设置 weights 参数加载预训练的网络参数</p>
<h4 id="1-加载模型"><a href="#1-加载模型" class="headerlink" title="1.加载模型"></a>1.加载模型</h4><p>暂无，等待施工</p>
<h3 id="6、测量工具"><a href="#6、测量工具" class="headerlink" title="6、测量工具"></a>6、测量工具</h3><p>Keras 提供了一些常用的测量工具，位于<code>keras.metrics</code>模块中，专门用于统计训练过程中常用的指标数据。Keras 的测量工具的使用方法一般有 4 个主要步骤：</p>
<ul>
<li>新建测量器</li>
<li>写入数据</li>
<li>读取统计数据</li>
<li>清零测量器</li>
</ul>
<h4 id="1-新建测量器"><a href="#1-新建测量器" class="headerlink" title="1.新建测量器"></a>1.新建测量器</h4><p>在<code>keras.metrics</code>模块中，提供了较多的常用测量器类， 如统计平均值的 <code>Mean</code> 类，统计准确率的 <code>Accuracy</code> 类，统计余弦相似度的 <code>CosineSimilarity</code> 类等</p>
<p>例子：统计误差值</p>
<p>在前向运算时，会得到每一个 Batch 的平均误差，但是希望统计每个Step的平均误差，因此选择使用Mean测量器：</p>
<pre><code class="python"># 新建平均测量器，适合 Loss 数据
loss_meter = metrics.Mean()</code></pre>
<h4 id="2-写入数据"><a href="#2-写入数据" class="headerlink" title="2.写入数据"></a>2.写入数据</h4><p>通过测量器的<code>update_state</code>函数可以写入新的数据，测量器会根据自身逻辑记录并处理采样数据。例如，在每个 Step 结束时采集一次 loss 值，代码如下：</p>
<pre><code class="python"># 记录采样的数据，通过 float()函数将张量转换为普通数值
loss_meter.update_state(float(loss))</code></pre>
<ul>
<li>放置在每个 Batch 运算结束后即可， 测量器会自动根据采样的数据来统计平均值</li>
</ul>
<h4 id="3-读取统计信息"><a href="#3-读取统计信息" class="headerlink" title="3.读取统计信息"></a>3.读取统计信息</h4><p>在采样多次数据后，可以选择在需要的地方调用测量器的<code>result()</code>函数，来获取统计值</p>
<pre><code class="python"># 打印统计期间的平均 loss
print(step, &#39;loss:&#39;, loss_meter.result())</code></pre>
<h4 id="4-清除状态"><a href="#4-清除状态" class="headerlink" title="4.清除状态"></a>4.清除状态</h4><p>测量器会统计<strong>所有历史记录</strong>的数据，因此在启动新一轮统计时，有必要清除历史状态。通过<code>reset_states()</code>即可实现清除状态功能</p>
<p>例如，在每次读取完平均误差后， 清零统计信息，以便下一轮统计的开始</p>
<pre><code class="python">if step % 100 == 0:
    # 打印统计的平均 loss
    print(step, &#39;loss:&#39;, loss_meter.result())
    loss_meter.reset_states() # 打印完后， 清零测量器</code></pre>
<h4 id="5-准确率统计实战"><a href="#5-准确率统计实战" class="headerlink" title="5.准确率统计实战"></a>5.准确率统计实战</h4><p>新建准确率测量器</p>
<pre><code class="python">acc_meter = metrics.Accuracy() # 创建准确率测量器</code></pre>
<p>Accuracy 类的 update_state函数的参数为预测值和真实值，而不是当前 Batch 的准确率</p>
<pre><code class="python"># [b, 784] =&gt; [b, 10]，网络输出值
out = network(x)
# [b, 10] =&gt; [b]，经过 argmax 后计算预测值
pred = tf.argmax(out, axis=1)
pred = tf.cast(pred, dtype=tf.int32)
# 根据预测值与真实值写入测量器
acc_meter.update_state(y, pred)</code></pre>
<p>在统计完测试集所有 Batch 的预测值后， 打印统计的平均准确率， 并清零测量器</p>
<pre><code class="python"># 读取统计结果
print(step, &#39;Evaluate Acc:&#39;, acc_meter.result().numpy())
acc_meter.reset_states() # 清零测量器</code></pre>
<h3 id="7、可视化"><a href="#7、可视化" class="headerlink" title="7、可视化"></a>7、可视化</h3><p>TensorFlow 提供了一个可视化工具<code>TensorBoard</code>。原理是通过将监控数据写入到文件系统， 并利用Web后端监控对应的文件目录， 从而可以允许用户从远程查看网络的监控数据。</p>
<p>TensorBoard 的使用需要模型代码和浏览器相互配合。在使用 TensorBoard 之前，需要安装 TensorBoard 库：</p>
<pre><code class="python"># 安装 TensorBoard
pip install tensorboard</code></pre>
<h4 id="1-模型端"><a href="#1-模型端" class="headerlink" title="1.模型端"></a>1.模型端</h4><p>创建写入监控数据的<code>Summary</code>类， 并在需要的时候写入监控数据即可。</p>
<p>首先通过<code>tf.summary.create_file_writer</code>创建监控对象类实例，并指定监控数据的写入<u>目录</u></p>
<pre><code class="python"># 创建监控类，监控数据将写入 log_dir 目录
summary_writer = tf.summary.create_file_writer(log_dir)</code></pre>
<hr>
<p>例子：监控误差数据和可视化数据</p>
<p>在前向计算完成后，对于误差这种<strong>标量</strong>数据， 我们通过<code>tf.summary.scalar</code>函数记录监控数据，并指定时间戳 step 参数</p>
<pre><code class="python">with summary_writer.as_default(): # 写入环境
    # 当前时间戳 step 上的数据为 loss，写入到名为 train-loss 数据库中
    tf.summary.scalar(&#39;train-loss&#39;, float(loss), step=step)</code></pre>
<ul>
<li>step：类似于每个数据对应的时间刻度信息（可以理解为数据曲线的x坐标），不宜重复。</li>
<li>每类数据通过字符串名字来区分，同类的数据需要写入相同名字的数据库中</li>
</ul>
<p>对于<strong>图片</strong>类型的数据， 可以通过<code>tf.summary.image</code>函数监控多个图片的张量数据，并通过设置max_outputs参数来选择最多显示的图片数量</p>
<pre><code class="python">with summary_writer.as_default():# 写入环境
    # 写入测试准确率
    tf.summary.scalar(&#39;test-acc&#39;, float(total_correct/total), step=step)
    # 可视化测试用的图片，设置最多可视化 9 张图片
    tf.summary.image(&quot;val-onebyone-images:&quot;, val_images, max_outputs=9, step=step)</code></pre>
<h4 id="2-浏览器端"><a href="#2-浏览器端" class="headerlink" title="2.浏览器端"></a>2.浏览器端</h4><p>打开 Web 后端：通过在 cmd 终端运行<code>tensorboard --logdir path</code>指定 Web 后端监控的文件目录 path， 即可打开 Web 后端监控进程</p>
<p>之后打开浏览器，输入网址<code>http://localhost:6006</code>(也可通过 IP 地址远程访问， 具体端口号可能会变动，可查看命令提示) 即可监控网络训练进度</p>
<p>除了监控标量数据和图片数据外， TensorBoard 还支持通过<code>tf.summary.histogram</code>查看张量数据的直方图分布，以及通过<code>tf.summary.text</code>打印文本信息等功能</p>
<pre><code class="python">with summary_writer.as_default():
    # 当前时间戳 step 上的数据为 loss，写入到 ID 位 train-loss 对象中
    tf.summary.scalar(&#39;train-loss&#39;, float(loss), step=step)
    # 可视化真实标签的直方图分布
    tf.summary.histogram(&#39;y-hist&#39;,y, step=step)
    # 查看文本信息
    tf.summary.text(&#39;loss-text&#39;,str(float(loss)))</code></pre>
<p>实际上，除了 TensorBoard 外，Visdom 工具同样可以方便可视化数据。Visdom 可以直接接受PyTorch 的张量类型的数据，但不能直接接受 TensorFlow 的张量类型数据，需要转换为Numpy 数组</p>
<h2 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h2><p>机器学习的主要目的是从训练集上学习到数据的真实模型， 从而能够在未见过的测试集上也能够表现良好，我们把这种能力叫做<strong>泛化能力</strong></p>
<h3 id="1、模型的容量"><a href="#1、模型的容量" class="headerlink" title="1、模型的容量"></a>1、模型的容量</h3><p>通俗地讲，模型的容量或表达能力，是指模型拟合复杂函数的能力。一种体现模型容量的指标为模型的假设空间(Hypothesis Space)大小，即模型可以表示的函数集的大小。</p>
<p>假设空间越大越完备， 从假设空间中搜索出逼近真实模型的函数也就越有可能； 反之，如果假设空间非常受限，就很难从中找到逼近真实模型的函数</p>
<p>实际上，较大的假设空间并不一定能搜索出更好的函数模型。 由于观测误差的存在，较大的假设空间中可能包含了大量表达能力过强的函数， 能够将训练样本的观测误差也学习进来，从而伤害了模型的泛化能力。挑选合适容量的学习模型是一个很大的难题</p>
<h3 id="2、过拟合与欠拟合"><a href="#2、过拟合与欠拟合" class="headerlink" title="2、过拟合与欠拟合"></a>2、过拟合与欠拟合</h3><ul>
<li>过拟合(Overfitting)：当模型的容量过大时，网络模型除了学习到训练集数据的模态之外，还把额外的观测误差也学习进来，导致学习的模型在训练集上面表现较好，但是在未见的样本上表现不佳，也就是模型泛化能力偏弱</li>
<li>欠拟合(Underfitting)：当模型的容量过小时，模型不能够很好地学习到训练集数据的模态，导致训练集上表现不佳，同时在未见的样本上表现也不佳</li>
</ul>
<p>那么如何去选择模型的容量？</p>
<ul>
<li><p>统计学习理论中的 VC 维度(Vapnik-Chervonenkis 维度)是一个应用比较广泛的度量函数容量的方法。但是该方法却很少应用到深度学习中去，一部分原因是神经网络过于复杂，很难去确定网络结构背后的数学模型的 VC 维度</p>
</li>
<li><p>可以根据奥卡姆剃刀原理(Occam’s razor)来指导神经网络的设计和训练。即“切勿浪费较多东西，去做‘用较少的东西，同样可以做好的事情’。”。也就是说，如果两层的神经网络结构能够很好的表达真实模型，那么三层的神经网络也能够很好的表达，但是我们应该优先选择使用更简单的两层神经网络，因为它的参数量更少，更容易训练，也更容易通过较少的训练样本获得不错的泛化误差</p>
</li>
</ul>
<h4 id="1-欠拟合"><a href="#1-欠拟合" class="headerlink" title="1.欠拟合"></a>1.欠拟合</h4><p>当我们发现当前的模型在训练集上误差一直维持较高的状态，很难优化减少，同时在测试集上也表现不佳时，我们可以考虑是否出现了欠拟合的现象</p>
<p>可以通过增加神经网络的层数、增大中间维度的大小等手段， 比较好的解决欠拟合的问题。 </p>
<p>在实际使用过程中，更多的是出现过拟合现象</p>
<h4 id="2-过拟合"><a href="#2-过拟合" class="headerlink" title="2.过拟合"></a>2.过拟合</h4><p>现代深度神经网络中过拟合现象非常容易出现，主要是因为神经网络的表达能力非常强， 训练集样本数不够</p>
<h3 id="3、数据集划分"><a href="#3、数据集划分" class="headerlink" title="3、数据集划分"></a>3、数据集划分</h3><p>前面我们介绍了数据集需要划分为训练集(Train set)和测试集(Test set)，但是为了挑选模型超参数和检测过拟合现象，一般需要将原来的训练集再次切分为新的训练集和验证集(Validation set)，即数据集需要切分为训练集、验证集和测试集 3 个子集</p>
<h4 id="1-验证集与超参数"><a href="#1-验证集与超参数" class="headerlink" title="1.验证集与超参数"></a>1.验证集与超参数</h4><p>验证集：选择模型的超参数(模型选择， Model selection)，功能包括：</p>
<ul>
<li>根据验证集的性能表现来调整学习率、 权值衰减系数、 训练次数等</li>
<li>根据验证集的性能表现来重新调整网络拓扑结构</li>
<li>根据验证集的性能表现判断是否过拟合和欠拟合</li>
</ul>
<p>训练集、验证集和测试集可以按着自定义的比例来划分，比如常见的 60%-20%-20%的划分</p>
<p>验证集与测试集的区别：</p>
<ul>
<li>算法设计人员可以根据<strong>验证集</strong>的表现来调整模型的各种超参数的设置，提升模型的<strong>泛化能力</strong>（测试泛化性能）</li>
<li>测试集的表现不能用来反馈模型的调整，否则测试集将和验证集的功能重合， 因此在测试集上的性能表现将<strong>无法</strong>代表模型的泛化能力</li>
</ul>
<h4 id="2-提前停止"><a href="#2-提前停止" class="headerlink" title="2.提前停止"></a>2.提前停止</h4><p>一般把对训练集中的一个<strong>Batch</strong>运算更新一次叫做一个<strong>Step</strong>， 对训练集的所有样本循环迭代一次叫做一个<strong>Epoch</strong>。验证集可以在数次 Step 或数次 Epoch 后使用，计算模型的验证性能（一般建议几个 Epoch 后进行一次验证运算）</p>
<ul>
<li>训练时，一般关注的指标有训练误差、 训练准确率等</li>
<li>验证时，也有验证误差和验证准确率等</li>
<li>测试时，也有测试误差和测试准确率等</li>
</ul>
<p>通过观测<u>训练准确率</u>和<u>验证准确率</u>可以大致推断模型是否出现过拟合和欠拟合</p>
<ul>
<li>过拟合：如果模型的训练误差较低，训练准确率较高，但是验证误差较高，验证准确率较低    <ul>
<li>解决方法：可以从新设计网络模型的容量，如降低网络的层数、降低网络的参数量、 添加正则化手段、 添加假设空间的约束等，使得模型的实际容量降低</li>
</ul>
</li>
<li>欠拟合：如果训练集和验证集上面的误差都较高，准确率较低<ul>
<li>解决方法：尝试增大网络的容量，如加深网络的层数、 增加网络的参数量，尝试更复杂的网络结构</li>
</ul>
</li>
</ul>
<p>实际上， 由于网络的实际容量可以随着训练的进行发生改变，因此在相同的网络设定下，随着训练的进行， 可能观测到不同的过拟合、 欠拟合状况</p>
<ul>
<li>在训练的前期，随着训练的进行，模型的训练准确率和测试准确率都呈现增大的趋势，此时并没有出现过拟合现象</li>
<li>在训练后期，即使是相同网络结构下， 由于模型的实际容量发生改变，我们观察到了过拟合的现象，具体表现为<u>训练准确度继续改善</u>，但是泛化能力变弱(<u>测试准确率减低</u>)</li>
</ul>
<p>记录模型的验证准确率，并监控验证准确率的变化， 当发现验证准确率连续𝑛个 Epoch 没有下降时，可以预测可能已经达到了最适合的 Epoch 附近，从而<strong>提前终止</strong>训练</p>
<h3 id="4、模型设计"><a href="#4、模型设计" class="headerlink" title="4、模型设计"></a>4、模型设计</h3><p>对于神经网络来说，网络的层数和参数量是网络容量很重要的参考指标</p>
<ul>
<li>减少网络的层数， 减少每层中网络参数量的规模， 可以有效降低网络的容量</li>
<li>如果发现模型欠拟合，需要增大网络的容量，可以通过增加层数，增大每层的参数量等方式实现</li>
</ul>
<h3 id="5、正则化"><a href="#5、正则化" class="headerlink" title="5、正则化"></a>5、正则化</h3><p>待补充</p>
<h3 id="6、Dropout"><a href="#6、Dropout" class="headerlink" title="6、Dropout"></a>6、Dropout</h3><p>Dropout 通过随机断开神经网络的连接，减少每次训练时实际参与计算的模型的参数量；但是在测试时， Dropout 会恢复所有的连接，保证模型测试时获得最好的性能</p>
<p>在 TensorFlow 中，可以通过<code>tf.nn.dropout(x, rate)</code>函数实现某条连接的 Dropout 功能，其中 rate 参数设置断开的概率值𝑝</p>
<pre><code class="python"># 添加 dropout 操作，断开概率为 0.5
x = tf.nn.dropout(x, rate=0.5)</code></pre>
<p>也可以将 Dropout 作为一个网络层使用， 在网络中间插入一个 Dropout 层：</p>
<pre><code class="python"># 添加 Dropout 层，断开概率为 0.5
model.add(layers.Dropout(rate=0.5))</code></pre>
<p>随着 Dropout 层的增加，网络模型训练时的实际容量减少，泛化能力变强</p>
<h3 id="7、数据增强"><a href="#7、数据增强" class="headerlink" title="7、数据增强"></a>7、数据增强</h3><p>增加数据集规模是解决过拟合最重要的途径。在有限的数据集上，通过数据增强技术可以增加训练的样本数量，获得一定程度上的性能提升</p>
<p>数据增强(Data Augmentation)是指在维持样本标签不变的条件下，根据先验知识改变样本的特征， 使得新产生的样本也符合或者近似符合数据的真实分布</p>
<p>以图片数据为例。数据集中的图片大小往往是不一致的，为了方便神经网络处理，需要将图片缩放到某个固定的大小，如缩放后的固定224 × 224大小的图片。对于图中的人物图片， 根据先验知识，我们知道旋转、缩放、 平移、裁剪、改变视角、 遮挡某局部区域都不会改变图片的主体类别标签，因此针对图片数据，可以有多种数据增强方式</p>
<p>TensorFlow 中提供了常用图片的处理函数， 位于<code>tf.image</code>子模块中。通过<code>tf.image.resize</code>函数可以实现图片的缩放功能</p>
<p>将图片从文件系统读取进来后，即可进行图片数据增强操作。通过预处理：</p>
<pre><code class="python">def preprocess(x,y):
# 预处理函数
# x: 图片的路径， y：图片的数字编码
x = tf.io.read_file(x)
x = tf.image.decode_jpeg(x, channels=3) # RGBA
# 图片缩放到 244x244 大小，这个大小根据网络设定自行调整
x = tf.image.resize(x, [244, 244])</code></pre>
<h4 id="1-旋转"><a href="#1-旋转" class="headerlink" title="1.旋转"></a>1.旋转</h4><p>通过<code>tf.image.rot90(x, k=1)</code>可以实现图片按逆时针方式旋转 k 个 90 度</p>
<pre><code class="python"># 图片逆时针旋转 180 度
x = tf.image.rot90(x,2)  </code></pre>
<h4 id="2-翻转"><a href="#2-翻转" class="headerlink" title="2.翻转"></a>2.翻转</h4><p>图片的翻转分为沿水平轴翻转和竖直轴翻转，可以通过<code>tf.image.random_flip_left_right</code>和<code>tf.image.random_flip_up_down</code>实现图片在水平方向和竖直方向的随机翻转操作</p>
<pre><code class="python"># 随机水平翻转
x = tf.image.random_flip_left_right(x)
# 随机竖直翻转
x = tf.image.random_flip_up_down(x)</code></pre>
<h4 id="3-裁剪"><a href="#3-裁剪" class="headerlink" title="3.裁剪"></a>3.裁剪</h4><p>通过在原图的左右或者上下方向去掉部分边缘像素，可以保持图片主体不变，同时获得新的图片样本。在实际裁剪时，一般先将图片缩放到略大于网络输入尺寸的大小， 再裁剪到合适大小</p>
<p>如网络的输入大小为224 × 224，那么可以先通过 resize 函数将图片缩放到244 × 244大小，再随机裁剪到224 × 224大小：</p>
<pre><code class="python"># 图片先缩放到稍大尺寸
x = tf.image.resize(x, [244, 244])
# 再随机裁剪到合适尺寸
x = tf.image.random_crop(x, [224,224,3])</code></pre>
<h4 id="4-生成数据"><a href="#4-生成数据" class="headerlink" title="4.生成数据"></a>4.生成数据</h4><p>通过生成模型在原有数据上进行训练， 学习到真实数据的分布，从而利用生成模型获得新的样本，这种方式也可以在一定程度上提升网络性能。 如通过条件生成对抗网络(Conditional GAN,简称 CGAN)可以生成带标签的样本数据</p>
<h4 id="5-其他方式"><a href="#5-其他方式" class="headerlink" title="5.其他方式"></a>5.其他方式</h4><p>除了上述介绍的典型图片数据增强方式以外，可以根据先验知识，在不改变图片标签信息的条件下，任意变换图片数据，获得新的图片。如在原图上叠加高斯噪声、通过改变图片的观察视角后、在原图上随机遮挡部分区域等</p>
<h3 id="8、过拟合问题"><a href="#8、过拟合问题" class="headerlink" title="8、过拟合问题"></a>8、过拟合问题</h3><h4 id="1-数据集构建"><a href="#1-数据集构建" class="headerlink" title="1.数据集构建"></a>1.数据集构建</h4><p>我们使用的数据集样本特性向量长度为 2， 标签为 0 或 1，分别代表了两种类别。借助于<code>scikit-learn</code>库中提供的<code>make_moons</code>工具， 我们可以生成任意多数据的训练集。首先安装 scikit-learn 库：</p>
<pre><code class="python"># pip 安装 scikit-learn 库
pip install -U scikit-learn</code></pre>
<p>为了演示过拟合现象，采样1000个样本数据，同时添加标准差为 0.25 的高斯噪声数据：</p>
<pre><code class="python"># 导入数据集生成工具
from sklearn.datasets import make_moons
# 从 moon 分布中随机采样 1000 个点，并切分为训练集-测试集
X, y = make_moons(n_samples = N_SAMPLES, noise=0.25, random_state=100)
X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size = TEST_SIZE, random_state=42)</code></pre>
<p>编写make_plot函数，方便根据样本的坐标 X 和样本的标签 y 绘制出数据的分布图：</p>
<pre><code class="python">def make_plot(X, y, plot_name, file_name, XX=None, YY=None, preds=None):
    plt.figure()
    # sns.set_style(&quot;whitegrid&quot;)
    axes = plt.gca()
    axes.set_xlim([x_min,x_max])
    axes.set_ylim([y_min,y_max])
    axes.set(xlabel=&quot;$x_1$&quot;, ylabel=&quot;$x_2$&quot;)
    # 根据网络输出绘制预测曲面
    if(XX is not None and YY is not None and preds is not None):
        plt.contourf(XX, YY, preds.reshape(XX.shape), 25, alpha = 0.08,
                     cmap=cm.Spectral)
        plt.contour(XX, YY, preds.reshape(XX.shape), levels=[.5],
                    cmap=&quot;Greys&quot;,
                    vmin=0, vmax=.6)
    # 绘制正负样本
    markers = [&#39;o&#39; if i == 1 else &#39;s&#39; for i in y.ravel()]
    mscatter(X[:, 0], X[:, 1], c=y.ravel(), s=20,
             cmap=plt.cm.Spectral, edgecolors=&#39;none&#39;, m=markers)
    # 保存矢量图
    plt.savefig(OUTPUT_DIR+&#39;/&#39;+file_name)</code></pre>
<p>绘制出采样的 1000 个样本分布：</p>
<pre><code class="python"># 绘制数据集分布
make_plot(X, y, None, &quot;dataset.svg&quot;)  </code></pre>
<h4 id="2-网络层数的影响"><a href="#2-网络层数的影响" class="headerlink" title="2.网络层数的影响"></a>2.网络层数的影响</h4><p>为了探讨不同的网络深度下的过拟合程度，我们共进行了 5 次训练实验。在𝑛 ∈ [0,4]时，构建网络层数为𝑛 + 2层的全连接层网络，并通过 Adam 优化器训练 500 个 Epoch，获得网络在训练集上的分隔曲线</p>
<pre><code class="python">for n in range(5): # 构建 5 种不同层数的网络
    model = Sequential()# 创建容器
    # 创建第一层
    model.add(Dense(8, input_dim=2,activation=&#39;relu&#39;))
    for _ in range(n): # 添加 n 层，共 n+2 层
        model.add(Dense(32, activation=&#39;relu&#39;))
    model.add(Dense(1, activation=&#39;sigmoid&#39;)) # 创建最末层
    model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;,
                  metrics=[&#39;accuracy&#39;]) # 模型装配与训练
    history = model.fit(X_train, y_train, epochs=N_EPOCHS, verbose=1)
    # 绘制不同层数的网络决策边界曲线
    preds = model.predict_classes(np.c_[XX.ravel(), YY.ravel()])
    title = &quot;网络层数({})&quot;.format(n)
    file = &quot;网络容量%f.png&quot;%(2+n*1)
    make_plot(X_train, y_train, title, file, XX, YY, preds)</code></pre>
<h4 id="3-Dropout的影响"><a href="#3-Dropout的影响" class="headerlink" title="3.Dropout的影响"></a>3.Dropout的影响</h4><p>待添加</p>
<h4 id="4-正则化的影响"><a href="#4-正则化的影响" class="headerlink" title="4.正则化的影响"></a>4.正则化的影响</h4><p>待添加</p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><h3 id="1、全连接层的问题"><a href="#1、全连接层的问题" class="headerlink" title="1、全连接层的问题"></a>1、全连接层的问题</h3><p>全连接层较高的内存占用量严重限制了神经网络朝着更大规模、更深层数方向的发展</p>
<h4 id="1-局部相关性"><a href="#1-局部相关性" class="headerlink" title="1.局部相关性"></a>1.局部相关性</h4><p>网络的每个输出节点都与所有的输入节点相连接， 用于提取所有输入节点的特征信息，这种稠密的连接方式是全连接层参数量大、 计算代价高的根本原因。 全连接层也称为稠密连接层(Dense Layer)</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200511214046062.png" alt="全连接示意图"></p>
<p>基于距离的重要性分布假设特性称为<strong>局部相关性</strong>，只关注和自己距离较近的部分节点，而忽略距离较远的节点。 在这种重要性分布假设下，全连接层的连接模式变成了下图所示的状态，输出节点𝑗只与以𝑗为中心的局部区域(感受野)相连接，与其它像素无连接</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200511214621069.png" alt="局部链接的网络层示意图"></p>
<p>其中和自己距离较近的部分节点形成的窗口称为<strong>感受野</strong>(Receptive Field)，表征了每个像素对于中心像素的重要性分布情况，网格内的像素才会被考虑，网格外的像素对于中心像素会被忽略</p>
<h4 id="2-劝值共享"><a href="#2-劝值共享" class="headerlink" title="2.劝值共享"></a>2.劝值共享</h4><p>如下图所示，在计算左上角位置的输出像素时，使用权值矩阵：$W=\begin{bmatrix}<br>w_{11} &amp; w_{12} &amp; w_{13}\<br>w_{21} &amp; w_{22} &amp; w_{23}\<br>w_{31} &amp; w_{32} &amp; w_{33}<br>\end{bmatrix}$</p>
<p>与对应感受野内部的像素相乘累加， 作为左上角像素的输出值；在计算右下方感受野区域时，共享权值参数𝑾，即使用相同的权值参数𝑾相乘累加，得到右下角像素的输出值，此时网络层的参数量9个，且与输入、输出节点数无关</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200511215857765.png" alt="劝值共享矩阵示意图"></p>
<p>通过运用局部相关性和权值共享的思想，成功把网络的参数量减少到𝑘 × 𝑘(准确地说，是在单输入通道、 单卷积核的条件下)。这种共享权值的“局部连接层”网络其实就是卷积神经网络</p>
<h4 id="3-卷积运算"><a href="#3-卷积运算" class="headerlink" title="3.卷积运算"></a>3.卷积运算</h4><p>略</p>
<h3 id="2、卷积神经网络"><a href="#2、卷积神经网络" class="headerlink" title="2、卷积神经网络"></a>2、卷积神经网络</h3><p>卷积神经网络通过充分利用局部相关性和权值共享的思想，大大地减少了网络的参数量， 从而提高训练效率，更容易实现超大规模的深层网络</p>
<p>以图片数据为例，卷积层接受高、 宽分别为ℎ、 𝑤，通道数为𝑐𝑖𝑛的输入特征图𝑿，在𝑐𝑜𝑢𝑡个高、 宽都为𝑘，通道数为𝑐𝑖𝑛的卷积核作用下，生成高、 宽分别为ℎ′、 𝑤′，通道数为𝑐𝑜𝑢𝑡的特征图输出。需要注意的是，卷积核的高宽可以不等，为了简化讨论，这里仅讨论高宽都为𝑘的情况</p>
<h4 id="1-单通道输入和单卷积核"><a href="#1-单通道输入和单卷积核" class="headerlink" title="1.单通道输入和单卷积核"></a>1.单通道输入和单卷积核</h4><p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200512112621597.png" alt="示意图"></p>
<p>完成第一个感受野区域的特征提取后，感受野窗口向右移动一个步长单位(Strides， 记为𝑠， 默认为 1)</p>
<h4 id="2-多通道输入和单卷积核"><a href="#2-多通道输入和单卷积核" class="headerlink" title="2.多通道输入和单卷积核"></a>2.多通道输入和单卷积核</h4><p>在多通道输入的情况下， 卷积核的通道数需要和输入𝑿的通道数量相匹配， 卷积核的第𝑖个通道和𝑿的第𝑖个通道运算，得到第𝑖个中间矩阵，此时可以视为单通道输入与单卷积核的情况， 所有通道的中间矩阵对应元素再次相加， 作为最终输出</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200512112150245.png" alt="多通道输入和单卷积核"></p>
<p>整个的计算示意图如下所示， 输入的每个通道处的感受野均与卷积核的对应通道相乘累加，得到与通道数量相等的中间变量，这些中间变量全部相加即得到当前位置的输出值。 输入通道的通道数量决定了卷积核的通道数。 一个卷积核只能得到一个输出矩阵，无论输入𝑿的通道数量</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200512112452481.png" alt="多通道输入和单卷积核计算示意图"></p>
<h4 id="3-多通道输入、多卷积核"><a href="#3-多通道输入、多卷积核" class="headerlink" title="3.多通道输入、多卷积核"></a>3.多通道输入、多卷积核</h4><p>一般来说，一个卷积核只能完成某种逻辑的特征提取，当需要同时提取多种逻辑特征时， 可以通过增加多个卷积核来得到多种特征，提高神经网络的表达能力，这就是多通道输入、 多卷积核的情况</p>
<p>当出现多卷积核时， 第𝑖 ($i \in [1,n]$， 𝑛为卷积核个数)个卷积核与输入𝑿运算得到第𝑖个输出矩阵(也称为输出张量𝑶的通道𝑖）， 最后全部的输出矩阵在通道维度上进行拼接(Stack 操作，创建输出通道数的新维度)，产生输出张量𝑶， 𝑶包含了𝑛个通道数</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200512113018813.png" alt="多卷积核示意图"></p>
<ul>
<li>每个卷积核的大小𝑘、步长𝑠、填充设定等都是统一设置（保证输出的每个通道大小一致）</li>
</ul>
<h4 id="4-步长"><a href="#4-步长" class="headerlink" title="4.步长"></a>4.步长</h4><p>感受野密度的控制手段一般是通过移动步长(Strides)实现的</p>
<p>步长是指感受野窗口每次移动的长度单位，对于2D输入来说，分为沿𝑥(向右)方向和𝑦(向下)方向的移动长度</p>
<ul>
<li>当步长设计的较小时，感受野以较小幅度移动窗口，有利于提取到更多的特征信息，输出张量的尺寸也更大</li>
<li>当步长设计的较大时， 感受野以较大幅度移动窗口，有利于减少计算代价， 过滤冗余信息，输出张量的尺寸也更小</li>
</ul>
<h4 id="5-填充"><a href="#5-填充" class="headerlink" title="5.填充"></a>5.填充</h4><p>在网络模型设计时，有时希望输出𝑶的高宽能够与输入𝑿的高宽相同， 从而方便网络参数的设计、 残差连接等</p>
<p>方法：通过在原输入𝑿的高和宽维度上面进行填充(Padding)若干无效元素操作，得到增大的输入𝑿′。 通过精心设计填充单元的数量， 在𝑿′上面进行卷积运算得到输出𝑶的高宽可以和原输入𝑿相等，甚至更大</p>
<p>卷积神经层的输出尺寸$[b,h’,w’,c_{out}]$由卷积核的数量$c_{out}$，卷积核的大小𝑘，步长𝑠，填充数𝑝(只考虑上下填充数量$p_h$相同，左右填充数量$p_w$相同的情况)以及输入𝑿的高宽ℎ/𝑤共同决定， 它们之间的数学关系可以表达为：</p>
<p>$h’=\left \lfloor \frac{h+2\cdot p_h-k}{s} \right \rfloor+1$</p>
<p>$w’=\left \lfloor \frac{w+2\cdot p_w-k}{s} \right \rfloor+1$</p>
<ul>
<li>其中$p_h$、$p_w$分别表示高、宽方向的填充数量</li>
<li>$\left \lfloor \right \rfloor$表示向下取整</li>
</ul>
<p>在 TensorFlow 中， 在𝑠 = 1 时， 如果希望输出𝑶和输入𝑿高、 宽相等， 只需要简单地设置参数<code>padding=”SAME”</code>即可使 TensorFlow 自动计算 padding 数量</p>
<h3 id="3、卷积层实现"><a href="#3、卷积层实现" class="headerlink" title="3、卷积层实现"></a>3、卷积层实现</h3><p>在 TensorFlow 中，既可以通过自定义权值的底层实现方式搭建神经网络，也可以直接调用现成的卷积层类的高层方式快速搭建复杂网络</p>
<h4 id="1-自定义权值"><a href="#1-自定义权值" class="headerlink" title="1.自定义权值"></a>1.自定义权值</h4><p>通过<code>tf.nn.conv2d</code>函数可以方便地实现 2D 卷积运算。<code>tf.nn.conv2d</code>基于输入$X:[b,h,w,c_{in}]$和卷积核$W:[k,k,c_{in},c_{out}]$进行卷积运算， 得到输出$O:[b,h’,w’,c_{out}]$</p>
<ul>
<li>$c_{in}$：输入通道数</li>
<li>$c_{out}$：卷积核的数量，即输出特征图的通道数</li>
<li>$k$：卷积核宽高</li>
<li>$b$：图片数量</li>
</ul>
<pre><code class="python">&gt;&gt;&gt; x = tf.random.normal([2,5,5,3]) # 模拟输入， 3 通道，高宽为 5
# 需要根据[k,k,cin,cout]格式创建 W 张量， 4 个 3x3 大小卷积核
&gt;&gt;&gt; w = tf.random.normal([3,3,3,4])
# 步长为 1, padding 为 0,
&gt;&gt;&gt; out = tf.nn.conv2d(x,w,strides=1,padding=[[0,0],[0,0],[0,0],[0,0]])
# 输出张量的 shape
TensorShape([2, 3, 3, 4])</code></pre>
<ul>
<li><p>padding参数格式：<code>padding=[[0,0],[上,下],[左,右],[0,0]]</code></p>
<pre><code class="python">  # 上下左右各填充一个单位：
  padding=[[0,0],[1,1],[1,1],[0,0]]</code></pre>
</li>
<li><p>特别地， 通过设置参数<code>padding=&#39;SAME&#39;</code>、<code>strides=1</code>可以直接得到输入、 输出同大小的卷积层</p>
</li>
<li><p>当<code>strides&gt;1</code>时， 设置<code>padding=&#39;SAME&#39;</code>将使得输出高、宽将成$\frac{1}{strides}$倍地减少</p>
<pre><code class="python">  &gt;&gt;&gt; x = tf.random.normal([2,5,5,3])
  &gt;&gt;&gt; w = tf.random.normal([3,3,3,4])
  # 高宽先 padding 成可以整除 3 的最小整数 6，然后 6 按 3 倍减少，得到 2x2
  &gt;&gt;&gt; out = tf.nn.conv2d(x,w,strides=3,padding=&#39;SAME&#39;)
  TensorShape([2, 2, 2, 4])</code></pre>
</li>
</ul>
<p>卷积神经网络层与全连接层一样，可以设置网络带偏置向量。<code>tf.nn.conv2d</code>函数是没有实现偏置向量计算的， 添加偏置需要手动累加偏置张量：</p>
<pre><code class="python"># 根据[cout]格式创建偏置向量
&gt;&gt;&gt; b = tf.zeros([4])
# 在卷积输出上叠加偏置向量，它会自动 broadcasting 为[b,h&#39;,w&#39;,cout]
&gt;&gt;&gt; out = out + b</code></pre>
<h4 id="2-卷积层类"><a href="#2-卷积层类" class="headerlink" title="2.卷积层类"></a>2.卷积层类</h4><p>通过卷积层类<code>layers.Conv2D</code>可以直接调用类实例完成卷积层的前向计算（TensorFlow中，API的首字母大写的对象一般表示类，全部小写的一般表示函数）。使用类方式会自动创建（在创建类时或build时）需要的权值张量和偏置向量等， 用户不需要记忆卷积核张量的定义格式</p>
<p>在新建卷积层类时，只需要指定卷积核数量参数<code>filters</code>，卷积核大小<code>kernel_size</code>， 步长<code>strides</code>，填充 <code>padding</code> 等即可</p>
<pre><code class="python"># 创建了 4 个3 × 3大小的卷积核的卷积层，步长为 1，padding 方案为&#39;SAME&#39;
layer = layers.Conv2D(4,kernel_size=3,strides=1,padding=&#39;SAME&#39;)</code></pre>
<p>如果卷积核高宽不等，步长行列方向不等，此时需要将kernel_size参数设计为元组格式$(k_h,k_w)$，strides参数设计为$(s_h,s_w)$</p>
<pre><code class="python"># 创建 4 个3 × 4大小的卷积核，竖直方向移动步长𝑠ℎ = 2，水平方向移动步长𝑠𝑤 = 1：
layer = layers.Conv2D(4,kernel_size=(3,4),strides=(2,1),padding=&#39;SAME&#39;)</code></pre>
<p>创建完成后，通过调用实例(的<code>__call__</code>方法)即可完成前向计算：</p>
<pre><code class="python"># 创建卷积层类
&gt;&gt;&gt; layer = layers.Conv2D(4,kernel_size=3,strides=1,padding=&#39;SAME&#39;)
&gt;&gt;&gt; out = layer(x) # 前向计算
&gt;&gt;&gt; out.shape # 输出张量的 shape
TensorShape([2, 5, 5, 4])</code></pre>
<p>在类<code>Conv2D</code>中，可以通过类成员<code>trainable_variables</code>直接返回𝑾和𝒃的列表：</p>
<pre><code class="python"># 返回所有待优化张量列表
&gt;&gt;&gt; layer.trainable_variables
[&lt;tf.Variable &#39;conv2d/kernel:0&#39; shape=(3, 3, 3, 4) dtype=float32, numpy=
 array([[[[ 0.13485974, -0.22861657, 0.01000655, 0.11988598],
          [ 0.12811887, 0.20501086, -0.29820845, -0.19579397],
          [ 0.00858489, -0.24469738, -0.08591779, -0.27885547]], ...
 &lt;tf.Variable &#39;conv2d/bias:0&#39; shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)&gt;]</code></pre>
<ul>
<li>可以直接调用类实例<code>layer.kernel</code>、<code>layer.bias</code>名访问𝑾和𝒃张量</li>
</ul>
<h3 id="4、LeNet-5实战"><a href="#4、LeNet-5实战" class="headerlink" title="4、LeNet-5实战"></a>4、LeNet-5实战</h3><p>1990 年代， Yann LeCun 等人提出了用于手写数字和机器打印字符图片识别的神经网络，被命名为 LeNet-5 [4]。 LeNet-5 的提出，使得卷积神经网络在当时能够成功被商用，广泛应用在邮政编码、支票号码识别等任务中</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200513020542762.png" alt="LeNet-5网络结构"></p>
<ul>
<li>接受32*32大小的数字、字符图片，经过第一个卷积层得到[b,28,28,6]形状的张量，经过一个向下采样层，张量尺寸缩小到[b,14,14,6]</li>
<li>经过第二个卷积层，得到[b,10,10,16]形状的张量，同样经过下采样层，张量尺寸缩小到[b,5,5,16]</li>
<li>在进入全连接层之前，先将张量打成[b,400]的张量</li>
<li>送入输出节点数分别为120、84的2个全连接层，得到[b,84]的张量</li>
<li>最后通过Gaussian connections层</li>
</ul>
<p>在上述基础上进行少许调整，使得它更容易在现代深度学习框架上实现：</p>
<p><img src="//NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/image-20200513021054068.png" alt="调整后的网络结构"></p>
<ul>
<li>将输入𝑿形状由32 × 32调整为28 × 28</li>
<li>将 2 个下采样层实现为最大池化层(降低特征图的高、宽，后续会介绍)</li>
<li>利用全连接层替换掉Gaussian connections层</li>
</ul>
<p>加载MNIST数据集：进阶操作 -&gt; 7、经典数据集加载</p>
<p>创建网络：</p>
<pre><code class="python">from tensorflow.keras import Sequential
network = Sequential([ # 网络容器
    layers.Conv2D(6,kernel_size=3,strides=1), # 第一个卷积层, 6 个 3x3 卷积核
    layers.MaxPooling2D(pool_size=2,strides=2), # 高宽各减半的池化层
    layers.ReLU(), # 激活函数
    layers.Conv2D(16,kernel_size=3,strides=1), # 第二个卷积层, 16 个 3x3 卷积核
    layers.MaxPooling2D(pool_size=2,strides=2), # 高宽各减半的池化层
    layers.ReLU(), # 激活函数
    layers.Flatten(), # 打平层，方便全连接层处理
    layers.Dense(120, activation=&#39;relu&#39;), # 全连接层， 120 个节点
    layers.Dense(84, activation=&#39;relu&#39;), # 全连接层， 84 节点
    layers.Dense(10) # 全连接层， 10 个节点
])
# build 一次网络模型，给输入 X 的形状，其中 4 为随意给的 batchsz
network.build(input_shape=(4, 28, 28, 1))
# 统计网络信息
network.summary()</code></pre>
<p>网络信息如下：</p>
<pre><code class="python">Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              multiple                  60        
_________________________________________________________________
max_pooling2d (MaxPooling2D) multiple                  0         
_________________________________________________________________
re_lu (ReLU)                 multiple                  0         
_________________________________________________________________
conv2d_1 (Conv2D)            multiple                  880       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 multiple                  0         
_________________________________________________________________
re_lu_1 (ReLU)               multiple                  0         
_________________________________________________________________
flatten (Flatten)            multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  48120     
_________________________________________________________________
dense_1 (Dense)              multiple                  10164     
_________________________________________________________________
dense_2 (Dense)              multiple                  850       
=================================================================
Total params: 60,074
Trainable params: 60,074
Non-trainable params: 0
_________________________________________________________________</code></pre>
<ul>
<li>显著降低网络参数量，同时增加网络深度</li>
</ul>
<h3 id="5、表示学习"><a href="#5、表示学习" class="headerlink" title="5、表示学习"></a>5、表示学习</h3>
      
       <hr><span style="font-style: italic;color: gray;"> 欢迎指出任何有错误或不够清晰的表达。邮件：1125934312@qq.com </span>
    </div>
</article>



<div class="article_copyright">
    <p><span class="copy-title">文章标题:</span>深度学习与TensorFlow2</p>
    <p><span class="copy-title">文章字数:</span><span class="post-count">32.9k</span></p>
    <p><span class="copy-title">本文作者:</span><a href="javascript:void(0)" title="NU-LL">NU-LL</a></p>
    <p><span class="copy-title">发布时间:</span>2020-01-03, 21:04:26</p>
    <p><span class="copy-title">最后更新:</span>2020-05-13, 16:30:26</p>
    <span class="copy-title">原始链接:</span><a class="post-url" href="/2020/01/03/深度学习与TensorFlow2/" title="深度学习与TensorFlow2">http://NU-LL.github.io/2020/01/03/深度学习与TensorFlow2/</a>
    <p>
        <span class="copy-title">版权声明:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
    </p>
</div>





    




    </div>
    <div class="copyright">
        <p class="footer-entry">©2016-2019 NU-LL</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full"><span class="min "></span></button>
<button class="post-toc-menu"><span class="post-toc-menu-icons"></span></button>
<div class="post-toc"><span class="post-toc-title">目录</span>
    <div class="post-toc-content">

    </div>
</div>
<a class="" id="rocket" href="javascript:void(0)"></a>
    </div>
</div>
<div class="acParent"></div>

</body>
<script src="//cdn.bootcss.com/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>

<script src="/js/script.js?v=1" ></script>
<script>
    var img_resize = 'default';
    /*作者、标签的自动补全*/
    $(function () {
        $('.search').AutoComplete({
            'data': ['#GO','#ESP32','#Docker','#DM9000C','#网卡移植','#ESP8266','#Latex','#Ctex','#IIC驱动','#Linux服务','#Linux内核','#文件描述符','#LiCheePi Zero','#Kconfig语法','#Mininet','#NanoPi Neo Core','#SVM','#TensorFlow2','#Markdown','#Xmanager','#远程链接','#字符驱动','#数据分析','#等待队列','#wait_queue_head_t','#wait_queue_t','#network namespace','#markdownlint','#git','#Kubernetes','#python','#爬虫','#python实战','#存储器','#二维数组','#指针','#u-boot','#IAP','#BootLoader','#dual bank','#按键驱动','#poll机制','#异步通知机制','#根文件系统','#快速排序','#设备树','#网卡驱动框架','#虚拟网卡','#输入子系统','#Go语言入门经典','#驱动API','#深度学习','#廖雪峰python教程',],
            'itemHeight': 20,
            'width': 418
        }).AutoComplete('show');
    })
    function initArticle() {
        /*渲染对应的表格样式*/
        

        /*渲染打赏样式*/
        

        /*高亮代码块行号*/
        
        $('pre code').each(function(){
            var lines = $(this).text().split('\n').length - 1, widther='';
            if (lines>99) {
                widther = 'widther'
            }
            var $numbering = $('<ul/>').addClass('pre-numbering ' + widther).attr("unselectable","on");
            $(this).addClass('has-numbering ' + widther)
                    .parent()
                    .append($numbering);
            for(var i=1;i<=lines;i++){
                $numbering.append($('<li/>').text(i));
            }
        });
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
    }

    /*打赏页面隐藏与展示*/
    

</script>

<!--加入行号的高亮代码块样式-->

<style>
    pre{
        position: relative;
        margin-bottom: 24px;
        border-radius: 10px;
        border: 1px solid #e2dede;
        background: #FFF;
        overflow: hidden;
    }
    code.has-numbering{
        margin-left: 30px;
    }
    code.has-numbering.widther{
        margin-left: 35px;
    }
    .pre-numbering{
        margin: 0px;
        position: absolute;
        top: 0;
        left: 0;
        width: 20px;
        padding: 0.5em 3px 0.7em 5px;
        border-right: 1px solid #C3CCD0;
        text-align: right;
        color: #AAA;
        background-color: ;
    }
    .pre-numbering.widther {
        width: 35px;
    }
</style>

<!--自定义样式设置-->
<style>
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    
    .post .pjax article .article-entry>ol, .post .pjax article .article-entry>ul, .post .pjax article>ol, .post .pjax article>ul{
        border: #e2dede solid 1px;
        border-radius: 10px;
        padding: 10px 32px 10px 56px;
    }
    .post .pjax article .article-entry li>ol, .post .pjax article .article-entry li>ul,.post .pjax article li>ol, .post .pjax article li>ul{
        padding-top: 5px;
        padding-bottom: 5px;
    }
    .post .pjax article .article-entry>ol>li, .post .pjax article .article-entry>ul>li,.post .pjax article>ol>li, .post .pjax article>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    .post .pjax article .article-entry li>ol>li, .post .pjax article .article-entry li>ul>li,.post .pjax article li>ol>li, .post .pjax article li>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    
    .post .pjax article blockquote {
        padding: 10px 20px;
        background-color: white;
        border: none;
        border-left: 4px solid #42b983;
        border-right: 4px solid #42b983;
        border-radius: 10px;
    }
    

    /*文章列表背景图*/
    

    
</style>







</html>
