<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>深度学习与TensorFlow2 | spaceman</title><meta name="keywords" content="TensorFlow2,深度学习"><meta name="author" content="spaceman"><meta name="copyright" content="spaceman"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="深度学习与TensorFlow2 环境（版本需要匹配）：  TensorFlow：TensorFlow2.0 GPU版本 Anaconda：4.8.1（conda -V，conda -list，Python 3.7 version），链接：https:&#x2F;&#x2F;www.anaconda.com&#x2F;distribution&#x2F;#download-section CUDA：V10.0.130（nvcc -V），">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习与TensorFlow2">
<meta property="og:url" content="http://nu-ll.github.io/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/index.html">
<meta property="og:site_name" content="spaceman">
<meta property="og:description" content="深度学习与TensorFlow2 环境（版本需要匹配）：  TensorFlow：TensorFlow2.0 GPU版本 Anaconda：4.8.1（conda -V，conda -list，Python 3.7 version），链接：https:&#x2F;&#x2F;www.anaconda.com&#x2F;distribution&#x2F;#download-section CUDA：V10.0.130（nvcc -V），">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2020-01-03T13:04:26.000Z">
<meta property="article:modified_time" content="2021-04-08T06:34:38.563Z">
<meta property="article:author" content="spaceman">
<meta property="article:tag" content="TensorFlow2">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="/img/favicon.jpg"><link rel="canonical" href="http://nu-ll.github.io/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习与TensorFlow2',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-04-08 14:34:38'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/spaceman.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">79</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">85</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/todo/"><i class="fa-fw fas fa-list"></i><span> 清单</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">spaceman</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/todo/"><i class="fa-fw fas fa-list"></i><span> 清单</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">深度学习与TensorFlow2</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-01-03T13:04:26.000Z" title="发表于 2020-01-03 21:04:26">2020-01-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-04-08T06:34:38.563Z" title="更新于 2021-04-08 14:34:38">2021-04-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">47.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>182分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="深度学习与TensorFlow2"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>深度学习与TensorFlow2</h1>
<p>环境（版本需要匹配）：</p>
<ul>
<li>TensorFlow：TensorFlow2.0 GPU版本</li>
<li>Anaconda：4.8.1（conda -V，conda -list，Python 3.7 version），链接：<a target="_blank" rel="noopener" href="https://www.anaconda.com/distribution/#download-section">https://www.anaconda.com/distribution/#download-section</a></li>
<li>CUDA：V10.0.130（nvcc -V），链接：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a>
<ul>
<li>cuDNN：，链接：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/rdp/cudnn-archive">https://developer.nvidia.com/rdp/cudnn-archive</a></li>
</ul>
</li>
</ul>
<h2 id="一、TensorFlow2基础操作">一、TensorFlow2基础操作</h2>
<p>TensorFlow中的数据载体叫做张量（Tensor）对象，即<code>tf.Tensor</code>，对应不同的类型，能够存储大量的连续的数据。同时所有的运算操作(Operation，简称 OP)也都是基于张量对象进行的</p>
<p>什么是Tensor：</p>
<ul>
<li>Tensor是一个比较广泛的数据</li>
<li>标量（scalar）：1.1、2.2等准确的数据类型（维度dim=0）</li>
<li>向量（vector）：[1.1]、[1.1,2.2,…]（dim=1）</li>
<li>矩阵（matrix）：[[1.1,2.2],[2.2,2.2],[3.3,2.2]]</li>
<li>数学上tensor：一般指维度&gt;2时的数据，但是在TensorFlow中维度&gt;=1时的数据全称为tensor，甚至标量也可以看作是tensor，所以工程上讲tensor一般指所有的数据</li>
</ul>
<h3 id="1、数据类型">1、数据类型</h3>
<h4 id="1-数值类型">1.数值类型</h4>
<p>数值类型的张量是 TensorFlow 的主要数据载体， 根据维度数来区分，可分为：</p>
<ul>
<li>
<p>标量(Scalar)：单个的实数，如 1.2, 3.4 等，维度(Dimension)数为 0， shape 为[]</p>
</li>
<li>
<p>向量(Vector)：单个实数的有序集合，通过中括号包裹，如[1.2]， [1.2, 3.4]等，维度数为 1，长度不定， shape 为[n]</p>
</li>
<li>
<p>矩阵(Matrix)：n行m列实数的有序集合，如[[1,2], [3,4]]，也可以写成<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>3</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>4</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix}
  1 &amp; 2\\ 
  3 &amp; 4
  \end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span></span></span></span>，维度数为 2，每个维度上的长度不定， shape 为[n,m]</p>
</li>
<li>
<p>张量(Tensor)：所有维度数dim &gt; 2的数组统称为张量。 张量的每个维度也作轴(Axis)，一般来说，维度代表了具体的物理含义，张量的维度数以及每个维度所代表的具体物理含义需要由用户自行定义。</p>
<blockquote>
<p>比如 Shape 为[2,32,32,3]的张量共有 4 维，如果表示图片数据的话，每个维度/轴代表的含义分别是图片数量、 图片高度、 图片宽度、 图片通道数，其中 2 代表了 2 张图片， 32 代表了高、 宽均为 32， 3 代表了 RGB 共 3 个通道</p>
</blockquote>
</li>
</ul>
<p>在 TensorFlow 中，为了表达方便，一般把标量、向量、矩阵也统称为张量，不作区分，需要根据张量的维度数或形状自行判断</p>
<p>张量的创建：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>aa = tf.constant(<span class="number">1.2</span>) <span class="comment"># TF方式创建标量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.constant([<span class="number">1</span>,<span class="number">2.</span>,<span class="number">3.3</span>]) <span class="comment"># TF方式创建向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">0</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">1.</span> , <span class="number">2.</span> , <span class="number">3.3</span>],dtype=float32)&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]) <span class="comment"># TF方式创建2行2列的矩阵</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">1</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>]])&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]]])<span class="comment"># TF方式创建3维张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">2</span>, shape=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]],</span><br><span class="line"></span><br><span class="line">       [[<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]]])&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>id：TensorFlow 中内部索引对象的编号</li>
<li>shape：张量的形状</li>
<li>dtype：张量的数值精度</li>
</ul>
<blockquote>
<p>张量可以通过 numpy()方法可以返回 Numpy.array 类型的数据，方便导出数据到系统的其他模块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.numpy() <span class="comment"># 将 TF 张量的数据导出为 numpy 数组格式</span></span><br><span class="line">array([<span class="number">1.</span> , <span class="number">2.</span> , <span class="number">3.3</span>], dtype=float32)</span><br></pre></td></tr></table></figure>
</blockquote>
<h4 id="2-字符串类型">2.字符串类型</h4>
<p>使用频率较低。通过传入字符串对象即可创建字符串类型的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant(<span class="string">&#x27;Hello, Deep Learning.&#x27;</span>) <span class="comment"># 创建字符串</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">3</span>, shape=(), dtype=string, numpy=<span class="string">b&#x27;Hello, Deep Learning.&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>
<p>在 <code>tf.strings</code> 模块中，提供了常见的字符串类型的工具函数，如小写化 lower()、 拼接join()、 长度 length()、 切分 split()等</p>
<blockquote>
<p>将字符串全部小写化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.strings.lower(a) <span class="comment"># 小写化字符串</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">19</span>, shape=(), dtype=string, numpy=<span class="string">b&#x27;hello, deep learning.&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<h4 id="3-布尔类型">3.布尔类型</h4>
<p>布尔类型的张量需要传入 Python 语言的布尔类型数据，转换成 TensorFlow 内部布尔型即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant(<span class="literal">True</span>) <span class="comment"># 创建布尔类型标量</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">22</span>, shape=(), dtype=<span class="built_in">bool</span>, numpy=<span class="literal">True</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([<span class="literal">True</span>, <span class="literal">False</span>]) <span class="comment"># 创建布尔类型向量</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">25</span>, shape=(<span class="number">2</span>,), dtype=<span class="built_in">bool</span>, numpy=array([ <span class="literal">True</span>, <span class="literal">False</span>])&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>TensorFlow 的布尔类型和 Python 语言的布尔类型并不等价，不能通用</li>
</ul>
<h3 id="2、数值精度">2、数值精度</h3>
<p>保存的数据位越长，精度越高，同时占用的内存空间也就越大。常用的精度类型有<code>tf.int16</code>、<code>tf.int32</code>、<code>tf.int64</code>、<code>tf.float16</code>、<code>tf.float32</code>、<code>tf.float64</code>等，其中<code>tf.float64</code>即为<code>tf.double</code></p>
<p>在创建张量时，可以指定张量的保存精度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.constant(<span class="number">123456789</span>, dtype=tf.int16)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.constant(<span class="number">123456789</span>, dtype=tf.int32)</span><br></pre></td></tr></table></figure>
<p>对于大部分深度学习算法，一般使用 tf.int32 和 tf.float32 可满足大部分场合的运算精度要求，部分对精度要求较高的算法，如强化学习某些算法，可以选择使用 tf.int64 和 tf.float64 精度保存张量</p>
<h4 id="1-读取精度">1.读取精度</h4>
<p>通过访问张量的<code>dtype</code>成员属性可以判断张量的保存精度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">&#x27;before:&#x27;</span>,a.dtype) <span class="comment"># 读取原有张量的数值精度</span></span><br><span class="line"><span class="keyword">if</span> a.dtype != tf.float32: <span class="comment"># 如果精度不符合要求，则进行转换</span></span><br><span class="line">    a = tf.cast(a,tf.float32) <span class="comment"># tf.cast 函数可以完成精度转换</span></span><br><span class="line">print(<span class="string">&#x27;after :&#x27;</span>,a.dtype) <span class="comment"># 打印转换后的精度</span></span><br></pre></td></tr></table></figure>
<h4 id="2-类型转换">2.类型转换</h4>
<p>通常通过<code>tf.cast</code>函数进行转换：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant(np.pi, dtype=tf.float16) <span class="comment"># 创建 tf.float16 低精度张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.cast(a, tf.double) <span class="comment"># 转换为高精度张量</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">44</span>, shape=(), dtype=float64, numpy=<span class="number">3.140625</span>&gt;</span><br></pre></td></tr></table></figure>
<p>布尔类型与整型之间相互转换也是合法的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([<span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.cast(a, tf.int32) <span class="comment"># 布尔类型转整型</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">48</span>, shape=(<span class="number">2</span>,), dtype=int32, numpy=array([<span class="number">1</span>, <span class="number">0</span>])&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>一般默认 0 表示 False， 1 表示 True，在 TensorFlow 中，将<strong>非 0 数字都视为 True</strong></li>
</ul>
<h3 id="3、待优化张量">3、待优化张量</h3>
<p>为了区分需要计算梯度信息的张量与不需要计算梯度信息的张量， TensorFlow 增加了<br>
一种专门的数据类型来<u>支持梯度信息的记录</u>：<code>tf.Variable</code></p>
<p><code>tf.Variable</code>类型在普通的张量类型基础上添加了 <code>name</code>， <code>trainable</code> 等属性来支持计算图的构建。由于梯度运算会消耗大量的计算资源，而且会自动更新相关参数，对于不需要的优化的张量，如神经网络的输入<strong>X</strong>，不需要通过 tf.Variable 封装；相反，对于<u>需要计算梯度并优化</u>的张量， 如神经网络层的<strong>W</strong>和<strong>b</strong>，需要通过 tf.Variable 包裹以便TensorFlow 跟踪相关梯度信息。</p>
<p>通过<code>tf.Variable()</code>函数可以将普通张量转换为待优化张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([-<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]) <span class="comment"># 创建 TF 张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>aa = tf.Variable(a) <span class="comment"># 转换为 Variable 类型</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>aa.name, aa.trainable <span class="comment"># Variable 类型张量的属性</span></span><br><span class="line">(<span class="string">&#x27;Variable:0&#x27;</span>, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>name 属性：命名计算图中的变量，这套命名体系是 TensorFlow 内部维护的， 一般不需要用户关注 name 属性</li>
<li>trainable属性：当前张量是否需要被优化（创建 Variable 对象时默认启用优化标志，可以设置trainable=False来设置张量不需要优化）</li>
</ul>
<p>直接创建待优化张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]) <span class="comment"># 直接创建 Variable 张量</span></span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=(<span class="number">2</span>, <span class="number">2</span>) dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>]])&gt;</span><br></pre></td></tr></table></figure>
<p>待优化张量可视为普通张量的特殊类型， 普通张量其实也可以通过<code>GradientTape.watch()</code>方法临时加入跟踪梯度信息的列表，从而支持自动求导功能</p>
<h3 id="4、创建张量">4、创建张量</h3>
<h4 id="1-从数组、列表中创建">1.从数组、列表中创建</h4>
<p>通过<code>tf.convert_to_tensor</code>函数可以创建新 Tensor，并将保存在 Python List 对象或者Numpy Array 对象中的数据导入到新 Tensor 中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.convert_to_tensor([<span class="number">1</span>,<span class="number">2.</span>]) <span class="comment"># 从列表创建张量</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">86</span>, shape=(<span class="number">2</span>,), dtype=float32, numpy=array([<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">dtype=float32)&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.convert_to_tensor(np.array([[<span class="number">1</span>,<span class="number">2.</span>],[<span class="number">3</span>,<span class="number">4</span>]])) <span class="comment"># 从数组中创建张量</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">88</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float64, numpy=</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">       [<span class="number">3.</span>, <span class="number">4.</span>]])&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>Numpy 浮点数数组默认使用 64 位精度保存数据，可以在需要的时候将其转换为 tf.float32 类型</li>
<li>实际上， tf.constant()和 tf.convert_to_tensor()都能够自动的把 Numpy 数组或者 Python列表数据类型转化为 Tensor 类型，这两个 API 命名来自 TensorFlow 1.x 的命名习惯，在TensorFlow 2 中函数的名字并不是很贴切，使用其一即可</li>
</ul>
<h4 id="2-创建全0全1张量">2.创建全0全1张量</h4>
<p>通过<code>tf.zeros()</code>和<code>tf.ones()</code>可创建任意形状，且内容全 0 或全 1 的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.zeros([]),tf.ones([]) <span class="comment"># 创建全 0，全 1 的标量</span></span><br><span class="line">(&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">90</span>, shape=(), dtype=float32, numpy=<span class="number">0.0</span>&gt;,</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">91</span>, shape=(), dtype=float32, numpy=<span class="number">1.0</span>&gt;)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.zeros([<span class="number">1</span>]),tf.ones([<span class="number">1</span>]) <span class="comment"># 创建全 0，全 1 的向量</span></span><br><span class="line">(&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">96</span>, shape=(<span class="number">1</span>,), dtype=float32, numpy=array([<span class="number">0.</span>],</span><br><span class="line">dtype=float32)&gt;,</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">99</span>, shape=(<span class="number">1</span>,), dtype=float32, numpy=array([<span class="number">1.</span>],</span><br><span class="line">dtype=float32)&gt;)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.ones([<span class="number">3</span>,<span class="number">2</span>]) <span class="comment"># 创建全 1 矩阵，指定 shape 为 3 行 2 列</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">108</span>, shape=(<span class="number">3</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<p>通过<code>tf.zeros_like</code>、<code>tf.ones_like</code>可以方便地新建与某个张量 shape 一致， 且内容为全 0 或全 1 的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.ones([<span class="number">2</span>,<span class="number">3</span>]) <span class="comment"># 创建一个矩阵</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.zeros_like(a) <span class="comment"># 创建一个与 a 形状相同，但是全 0 的新矩阵</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">113</span>, shape=(<span class="number">2</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>tf.*_like</code>是一系列的便捷函数，可以通过<code>tf.zeros(a.shape)</code>等方式实现</li>
</ul>
<h4 id="3-创建自定义数值张量">3.创建自定义数值张量</h4>
<p>通过<code>tf.fill(shape, value)</code>可以创建全为自定义数值 value 的张量，形状由 shape 参数指定：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.fill([<span class="number">2</span>,<span class="number">2</span>], <span class="number">99</span>) <span class="comment"># 创建 2 行 2 列，元素全为 99 的矩阵</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">136</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">99</span>, <span class="number">99</span>],</span><br><span class="line">       [<span class="number">99</span>, <span class="number">99</span>]])&gt;</span><br></pre></td></tr></table></figure>
<h4 id="4-创建已知分布的张量">4.创建已知分布的张量</h4>
<p>通过<code>tf.random.normal(shape, mean=0.0, stddev=1.0)</code>可以创建形状为 shape，均值为mean，标准差为 stddev 的<strong>正态分布</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.random.normal([<span class="number">2</span>,<span class="number">2</span>]) <span class="comment"># 创建标准正态分布的张量</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">143</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[-<span class="number">0.4307344</span> , <span class="number">0.44147003</span>],</span><br><span class="line">       [-<span class="number">0.6563149</span> , -<span class="number">0.30100572</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<p>通过<code>tf.random.uniform(shape, minval=0, maxval=None, dtype=tf.float32)</code>可以创建采样自[minval, maxval)区间的<strong>均匀分布</strong>的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.random.uniform([<span class="number">2</span>,<span class="number">2</span>]) <span class="comment"># 创建采样自[0,1)均匀分布的矩阵</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">158</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">0.65483284</span>, <span class="number">0.63064325</span>],</span><br><span class="line">       [<span class="number">0.008816</span> , <span class="number">0.81437767</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h4 id="5-创建序列">5.创建序列</h4>
<p><code>tf.range(limit, delta=1)</code>可以创建[0, limit)之间，步长为 delta 的整型序列，不包含 limit 本身：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.<span class="built_in">range</span>(<span class="number">10</span>) <span class="comment"># 0~10，不包含 10</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">180</span>, shape=(<span class="number">10</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])&gt;</span><br></pre></td></tr></table></figure>
<p>通过<code>tf.range(start, limit, delta=1)</code>可以创建[start, limit)，步长为 delta 的序列，不包含limit 本身</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>,delta=<span class="number">2</span>) <span class="comment"># 1~10</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">190</span>, shape=(<span class="number">5</span>,), dtype=int32, numpy=array([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>])&gt;</span><br></pre></td></tr></table></figure>
<h3 id="5、张量的典型用途">5、张量的典型用途</h3>
<p>标量：误差值的表示、 各种测量指标的表示，比如准确度(Accuracy，简称 acc)，精度(Precision)和召回率(Recall)等</p>
<p>向量：如在全连接层和卷积神经网络层中，偏置张量b就是使用向量来表示</p>
<p>矩阵：比如全连接层的批量输入张量X的形状为[b,din]，其中b表示输入样本的个数，即Batch Size，din表示输入特征的长度</p>
<p>三维张量：表示序列信号，它的格式是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mo>=</mo><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><mi>s</mi><mi>e</mi><mi>q</mi><mi>u</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mo separator="true">,</mo><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mtext> </mtext><mi>l</mi><mi>e</mi><mi>n</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">X=[b,sequence,feature\ len]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mord mathdefault">u</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">c</span><span class="mord mathdefault">e</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">u</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">e</span><span class="mspace"> </span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mclose">]</span></span></span></span>，其中b表示序列信号的数量， sequence len 表示序列信号在时间维度上的采样点数或步数，feature len 表示每个点的特征长度，自然语言处理(Natural Language Processing，简称 NLP)中会使用到</p>
<p>四维张量：卷积神经网络中应用广泛，用于保存特征图数据，格式一般为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi><mo separator="true">,</mo><mi>c</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[b,h,w,c]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">c</span><span class="mclose">]</span></span></span></span>，其中b表示输入样本的数量， h/w 分别表示特征图的高/宽，c表示特征图的通道数。部分深度学习框架（如PyTorch）会使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><mi>c</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[b,c,h,w]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">]</span></span></span></span>格式</p>
<p>大于四维的张量一般应用的比较少，如在元学习(Meta Learning)中会采用五维的张量表示方法，理解方法与三、四维张量类似</p>
<h3 id="6、索引与切片">6、索引与切片</h3>
<h4 id="1-索引">1.索引</h4>
<p>在 TensorFlow 中， 支持基本的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">[i][j]...</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mclose">]</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span></span></span></span>标准索引方式，也支持通过逗号分隔索引号的索引方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">4</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>]) <span class="comment"># 创建 4D 张量（4张32*32大小的彩色图片）</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x[<span class="number">0</span>] <span class="comment"># 程序中的第一的索引号应为 0，容易混淆，不过不影响理解</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">379</span>, shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[[ <span class="number">1.3005302</span> , <span class="number">1.5301839</span> , -<span class="number">0.32005513</span>],</span><br><span class="line">        [-<span class="number">1.3020388</span> , <span class="number">1.7837263</span> , -<span class="number">1.0747638</span> ], ...</span><br><span class="line">        [-<span class="number">1.1092019</span> , -<span class="number">1.045254</span> , -<span class="number">0.4980363</span> ],</span><br><span class="line">        [-<span class="number">0.9099222</span> , <span class="number">0.3947732</span> , -<span class="number">0.10433522</span>]]], dtype=float32)&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x[<span class="number">2</span>][<span class="number">1</span>][<span class="number">0</span>][<span class="number">1</span>] <span class="comment"># 取第 3 张图片，第 2 行，第 1 列的像素， B 通道(第 2 个通道)颜色强度值</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">418</span>, shape=(), dtype=float32, numpy=-<span class="number">0.84922135</span>&gt;</span><br></pre></td></tr></table></figure>
<p>当张量的维度数较高时， 使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[i][j]...[k]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mclose">]</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mclose">]</span></span></span></span>的方式书写不方便，可以采用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[i,j,...,k]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mclose">]</span></span></span></span>方式索引，他们是等价的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x[<span class="number">1</span>,<span class="number">9</span>,<span class="number">2</span>] <span class="comment"># 取第 2 张图片，第 10 行，第 3 列的数据，实现如下</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">436</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([ <span class="number">1.7487534</span> , -<span class="number">0.41491988</span>, -<span class="number">0.2944692</span> ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h4 id="2-切片">2.切片</h4>
<p>通过<code>start:end:step</code>切片方式可以方便地提取一段数据，其中 start 为开始读取位置的索引， end 为结束读取位置的索引(不包含 end 位)， step 为采样步长</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x[<span class="number">1</span>:<span class="number">3</span>] <span class="comment"># 读取第 2,3 张图片</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">441</span>, shape=(<span class="number">2</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[[[ <span class="number">0.6920027</span> , <span class="number">0.18658352</span>, <span class="number">0.0568333</span> ],</span><br><span class="line">         [ <span class="number">0.31422952</span>, <span class="number">0.75933754</span>, <span class="number">0.26853144</span>],</span><br><span class="line">         [ <span class="number">2.7898</span> , -<span class="number">0.4284912</span> , -<span class="number">0.26247284</span>],...</span><br></pre></td></tr></table></figure>
<p><code>start:end:step</code>切片方式有很多简写方式</p>
<ul>
<li>全部省略时即为<code>::</code>， 表示从最开始读取到最末尾，步长为 1（不跳过任何元素）。为了更加简洁， <code>::</code>可以简写为单个冒号<code>:</code></li>
<li>从第一个元素读取时<code>start</code>可以省略（即start=0可以省略）</li>
<li>取到最后一个元素时<code>end</code>可以省略</li>
<li>步长为 1 时<code>step</code>可以省略</li>
</ul>
<p>特别地，<code>step</code>可以为负数，当step = -1时，<code>start:end:-1</code>表示从 start 开始， 逆序读取至 end 结束(不包含 end)，索引号<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>e</mi><mi>n</mi><mi>d</mi><mo>⩽</mo><mi>s</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">end\leqslant start</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83111em;vertical-align:-0.13667em;"></span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel amsrm">⩽</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">t</span></span></span></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.<span class="built_in">range</span>(<span class="number">9</span>) <span class="comment"># 创建 0~9 向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x[<span class="number">8</span>:<span class="number">0</span>:-<span class="number">1</span>] <span class="comment"># 从 8 取到 0，逆序，不包含 0</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">466</span>, shape=(<span class="number">8</span>,), dtype=int32, numpy=array([<span class="number">8</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>])&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">4</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x[<span class="number">0</span>,::-<span class="number">2</span>,::-<span class="number">2</span>] <span class="comment"># 行、列逆序间隔采样</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">487</span>, shape=(<span class="number">16</span>, <span class="number">16</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[[ <span class="number">0.63320625</span>, <span class="number">0.0655185</span> , <span class="number">0.19056146</span>],</span><br><span class="line">        [-<span class="number">1.0078577</span> , -<span class="number">0.61400175</span>, <span class="number">0.61183935</span>],</span><br><span class="line">        [ <span class="number">0.9230892</span> , -<span class="number">0.6860094</span> , -<span class="number">0.01580668</span>],</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>为了避免出现像 [: , : , : ,1]这样过多冒号的情况，可以使用<code>...</code>符号表示取多个维度上所有的数据， 其中维度的数量需根据规则自动推断：</p>
<ul>
<li>当切片方式出现<code>...</code>符号时，<code>...</code>符号左边的维度将自动对齐到最左边</li>
<li><code>...</code>符号右边的维度将自动对齐到最右边，此时系统再自动推断<code>...</code>符号代表的维度数量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">4</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x[<span class="number">0</span>:<span class="number">2</span>,...,<span class="number">1</span>:] <span class="comment"># 高宽维度全部采集</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">497</span>, shape=(<span class="number">2</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[[[ <span class="number">0.575703</span> , <span class="number">0.8872789</span> ],</span><br><span class="line">         [ <span class="number">0.11028383</span>, -<span class="number">0.27128693</span>],</span><br><span class="line">         [-<span class="number">0.9950867</span> , -<span class="number">1.7737272</span> ],</span><br><span class="line">         ...</span><br></pre></td></tr></table></figure>
<h3 id="7、维度变换">7、维度变换</h3>
<p>算法的每个模块对于数据张量的格式有不同的逻辑要求，当现有的数据格式不满足算法要求时，需要通过维度变换将数据调整为正确的格式。这就是维度变换的功能。</p>
<p>基本的维度变换操作函数包含了改变视图 <code>reshape</code>、 插入新维度 <code>expand_dims</code>，删除维度 <code>squeeze</code>、 交换维度 <code>transpose</code>、 复制数据 <code>tile</code> 等函数</p>
<p>Batch 维度：为了实现维度变换，我们需要将原始数据插入一个新的维度，并把它定义为 Batch 维度，然后在 Batch 维度对数据进行相关操作，得到变换后的新的数据。这一系列的操作就是维度变换操作。</p>
<h4 id="1-改变视图-reshape">1.改变视图 reshape</h4>
<p>张量的视图（View）：就是我们理解张量的方式，比如 shape 为[2,4,4,3]的张量A，从逻辑上可以理解为 2 张图片，每张图片 4 行 4 列，每个位置有 RGB 3 个通道的数据</p>
<p>张量的存储（Storage）：体现在张量在内存上保存为一段连续的内存区域，对于同样的存储，我们可以有不同的理解方式，比如上述张量A，我们可以在不改变张量的存储下，将张量A理解为 2个样本，每个样本的特征为长度 48 的向量</p>
<p>同一个存储，从不同的角度观察数据，可以产生不同的视图， 这就是存储与视图的关系。 视图的产生是非常灵活的，但需要保证是合理。</p>
<p>通过 <code>tf.range()</code>模拟生成一个向量数据，并通过<code>tf.reshape</code>视图改变函数产生不同的视图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x=tf.<span class="built_in">range</span>(<span class="number">96</span>) <span class="comment"># 生成向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x=tf.reshape(x,[<span class="number">2</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">3</span>]) <span class="comment"># 改变 x 的视图，获得 4D 张量，存储并未改变</span></span><br><span class="line"><span class="comment"># 可以观察到数据仍然是 0~95 的顺序，可见数据并未改变，改变的是数据的结构</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">11</span>, shape=(<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>), dtype=int32, numpy=</span><br><span class="line">array([[[[ <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">         [ <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">         [ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]],...</span><br></pre></td></tr></table></figure>
<p>在存储数据时，内存只能以平铺方式按序写入内存，因此视图的层级关系需要人为管理。为了方便表达，一般把张量 shape 列表中相对靠左侧的维度叫作大维度， shape 列表中相对靠右侧的维度叫作小维度（如[2,4,4,3]的张量中，图片数量维度与通道数量相比，图片数量叫作大维度，通道数叫作小维度）</p>
<p>改变视图操作在提供便捷性的同时，也会带来很多逻辑隐患，主要的原因是改变视图操作的默认前提是<u>存储不需要改变</u>，否则改变视图操作就是非法的</p>
<blockquote>
<p>张量A按着初始视图[b,h,w,c]写入的内存布局，改变A的理解方式，它可以有多种合法的理解方式：</p>
<ul>
<li>[b,h∙w,c]张量理解为b张图片，h∙w个像素点，c个通道</li>
<li>[b,h,w∙c]张量理解为b张图片，h行，每行的特征长度为w∙c</li>
<li>[b,h∙w∙c]张量理解为b张图片，每张图片的特征长度为h∙w∙c</li>
</ul>
<p>从语法上来说， 视图变换只需要满足新视图的元素总量与存储区域大小相等即可。正是由于视图的设计的语法约束很少，使得在改变视图时容易出现逻辑隐患。</p>
<p>不合法的视图变换：</p>
<p>例如，如果定义新视图为[b,w,h,c]，[b,c,h*w]或者[b,c,h,w]等时，张量的存储顺序需要改变， 如果不同步更新张量的存储顺序，那么恢复出的数据将与新视图不一致，从而导致数据错乱。</p>
<p>这需要用户理解数据，才能判断操作是否合法。我们会在“交换维度”一节介绍如何改变张量的存储</p>
</blockquote>
<p>在通过 reshape 改变视图时，必须始终记住张量的存储顺序，新视图的维度顺序不能与存储顺序相悖，否则需要通过<strong>交换维度</strong>操作将存储顺序同步过来</p>
<p>在 TensorFlow 中，可以通过张量的 <code>ndim</code> 和 <code>shape</code> 成员属性获得张量的维度数和形状：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.ndim,x.shape <span class="comment"># 获取张量的维度数和形状列表</span></span><br><span class="line">(<span class="number">4</span>, TensorShape([<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>]))</span><br></pre></td></tr></table></figure>
<hr>
<p>通过 <code>tf.reshape(x, new_shape)</code>，可以将张量的视图任意地合法改变：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reshape(x,[<span class="number">2</span>,-<span class="number">1</span>])</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">520</span>, shape=(<span class="number">2</span>, <span class="number">48</span>), dtype=int32, numpy=</span><br><span class="line">array([[ <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>,</span><br><span class="line">        <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>, <span class="number">31</span>,…</span><br><span class="line">        <span class="number">80</span>, <span class="number">81</span>, <span class="number">82</span>, <span class="number">83</span>, <span class="number">84</span>, <span class="number">85</span>, <span class="number">86</span>, <span class="number">87</span>, <span class="number">88</span>, <span class="number">89</span>, <span class="number">90</span>, <span class="number">91</span>, <span class="number">92</span>, <span class="number">93</span>, <span class="number">94</span>, <span class="number">95</span>]])&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>参数-1：当前轴上长度需要根据张量总元素不变的法则自动推导（该处推导成(2*4*4*3)/2=48）</li>
</ul>
<p>再次改变数据的视图为[2,16,3] ，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reshape(x,[<span class="number">2</span>,-<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">526</span>, shape=(<span class="number">2</span>, <span class="number">16</span>, <span class="number">3</span>), dtype=int32, numpy=</span><br><span class="line">array([[[ <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], …</span><br><span class="line">       [<span class="number">45</span>, <span class="number">46</span>, <span class="number">47</span>]],</span><br><span class="line">      [[<span class="number">48</span>, <span class="number">49</span>, <span class="number">50</span>],…</span><br><span class="line">       [<span class="number">93</span>, <span class="number">94</span>, <span class="number">95</span>]]])&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>上述一系列连续变换视图操作中，张量的存储顺序始终没有改变，数据在内存中仍然是按着初始写入的顺序0,1,2, ⋯ ,95保存</li>
</ul>
<h4 id="2-增删维度">2.增删维度</h4>
<h5 id="增加维度">增加维度</h5>
<p>通过 <code>tf.expand_dims(x, axis)</code>可在指定的 <strong>axis 轴前</strong>可以插入一个新的维度（长度为1）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.uniform([<span class="number">28</span>,<span class="number">28</span>],maxval=<span class="number">10</span>,dtype=tf.int32) <span class="comment"># 产生矩阵</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">11</span>, shape=(<span class="number">28</span>, <span class="number">28</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">6</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">7</span>],...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.expand_dims(x,axis=<span class="number">2</span>) <span class="comment"># axis=2 表示宽维度后面的一个维度</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">13</span>, shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>), dtype=int32, numpy=</span><br><span class="line">array([[[<span class="number">6</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>],</span><br><span class="line">        [<span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>],</span><br><span class="line">        [<span class="number">3</span>],...</span><br></pre></td></tr></table></figure>
<ul>
<li>增加一个长度为 1 的维度相当于给原有的数据添加一个新维度的概念，数据并不发生改变，仅仅是改变数据的理解方式，因此它其实可以理解为改变视图的一种特殊方式</li>
</ul>
<p>需要注意的是， <code>tf.expand_dims</code> 的 axis 为<strong>正</strong>时，表示在当前维度<strong>之前</strong>插入一个新维度； 为<strong>负</strong>时，表示当前维度<strong>之后</strong>插入一个新的维度。以[b,h,w,c]张量为例，不同 axis 参数的实际插入位置如下所示：</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200503172458500.png" alt="增加维度 axis 参数位置示意图"></p>
<h5 id="删除维度">删除维度</h5>
<p>与增加维度一样，删除维度只能删除<strong>长度为1</strong>的维度，也不会改变张量的存储。</p>
<p>通过 <code>tf.squeeze(x, axis)</code>函数， axis 参数为待删除的维度的索引号：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据同上一个例子</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.squeeze(x, axis=<span class="number">2</span>) <span class="comment"># 删除图片通道数维度</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">588</span>, shape=(<span class="number">28</span>, <span class="number">28</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">8</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">9</span>, <span class="number">7</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">0</span>],...</span><br></pre></td></tr></table></figure>
<p>如果不指定维度参数 axis，即 <code>tf.squeeze(x)</code>， 那么它会默认删除所有长度为 1 的维度</p>
<h4 id="3-交换维度">3.交换维度</h4>
<p>在保持维度顺序不变的条件下， 仅仅改变张量的理解方式是不够的，有时需要直接调整的存储顺序，即<strong>交换维度</strong>(Transpose)。通过交换维度操作，改变了张量的存储顺序，同时也改变了张量的视图</p>
<p>以图片格式[b,h,w,c]转换到图片格式[b,c,h,w]为例，介绍使用 <code>tf.transpose(x, perm)</code>函数完成维度交换操作，其中参数 perm表示新维度的顺序 List。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">2</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.transpose(x,perm=[<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>]) <span class="comment"># 交换维度</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">603</span>, shape=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>), dtype=float32, numpy=</span><br><span class="line">array([[[[-<span class="number">1.93072677e+00</span>, -<span class="number">4.80163872e-01</span>, -<span class="number">8.85614634e-01</span>, ...,</span><br><span class="line">          <span class="number">1.49124235e-01</span>, <span class="number">1.16427064e+00</span>, -<span class="number">1.47740364e+00</span>],</span><br><span class="line">         [-<span class="number">1.94761145e+00</span>, <span class="number">7.26879001e-01</span>, -<span class="number">4.41877693e-01</span>, ...</span><br></pre></td></tr></table></figure>
<ul>
<li>图片张量 shape 为[2,32,32,3]，“图片数量、行、列、通道数” 的维度索引分别为 0、 1、 2、 3。交换为[b,c,h,w]格式，新维度的排序为“图片数量、通道数、行、列”，对应的索引号为[0,3,1,2]</li>
<li>通过 <code>tf.transpose</code> 维度交换后，张量的存储顺序已经改变， 视图也随之改变， 后续的所有操作必须基于新的存续顺序和视图进行。 相对于改变视图操作，维度交换操作的<strong>计算代价更高</strong></li>
</ul>
<h4 id="4-复制数据">4.复制数据</h4>
<p>可以通过<code>tf.tile(x, multiples)</code>函数完成数据在指定维度上的复制操作， multiples 分别指定了每个维度上面的复制倍数，对应位置为 1 表明不复制，为 2 表明新长度为原来长度的2 倍，即数据复制一份，以此类推</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.constant([<span class="number">1</span>,<span class="number">2</span>]) <span class="comment"># 创建向量 b</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.expand_dims(b, axis=<span class="number">0</span>) <span class="comment"># 插入新维度，变成矩阵</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">645</span>, shape=(<span class="number">1</span>, <span class="number">2</span>), dtype=int32, numpy=array([[<span class="number">1</span>, <span class="number">2</span>]])&gt;</span><br><span class="line"><span class="comment"># 在 Batch 维度上复制数据 1 份，实现如下：</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.tile(b, multiples=[<span class="number">2</span>,<span class="number">1</span>]) <span class="comment"># 样本维度上复制一份</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">648</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>]])&gt;</span><br></pre></td></tr></table></figure>
<p><code>tf.tile</code> 会创建一个<strong>新的张量</strong>来保存复制后的张量，会涉及大量数据的读写 IO 运算，计算代价相对较高</p>
<h3 id="8、Broadcasting">8、Broadcasting</h3>
<p>Broadcasting 称为广播机制(或自动扩展机制)，它是一种轻量级的张量复制手段，在逻辑上扩展张量数据的形状， 但是只会在需要时才会执行实际存储复制操作。</p>
<p>对于用户来说， Broadcasting 和 tf.tile 复制的最终效果是一样的，操作对用户透明，但是 Broadcasting 机制节省了大量计算资源，建议在运算过程中尽可能地利用 Broadcasting 机制提高计算效率</p>
<p>考虑一个例子：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mi mathvariant="normal">@</mi><mi>W</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">Y=X@W+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord">@</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>， 其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mi mathvariant="normal">@</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">X@W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord">@</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>的 shape 为[2,3]，b的 shape 为[3]，可以通过结合 tf.expand_dims 和 tf.tile 手动完成复制数据操作，将b变换为[2,3]，然后与 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mi mathvariant="normal">@</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">X@W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord">@</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>完成相加运算。但实际上，直接将 shape 为[2,3]与[3]的b相加也是合法的，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">2</span>,<span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>w = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.random.normal([<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x@w+b <span class="comment"># 不同 shape 的张量直接相加</span></span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>会自动调用 Broadcasting函数 <code>tf.broadcast_to(x, new_shape)</code>， 将两者 shape 扩张为相同的[2,3]， 即上式可以等效为</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x@w + tf.broadcast_to(b,[<span class="number">2</span>,<span class="number">3</span>]) <span class="comment"># 手动扩展，并相加</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>操作符+在遇到 shape 不一致的 2 个张量时，会自动考虑将 2 个张量自动扩展到一致的 shape，然后再调用 tf.add 完成张量相加运算</p>
</li>
</ul>
<p>所有的运算都需要在正确逻辑（满足Broadcasting 设计的核心思想）下进行， Broadcasting 机制并不会扰乱正常的计算逻辑， 它只会针对于最常见的场景自动完成增加维度并复制数据的功能， 提高开发效率和运行效率。</p>
<blockquote>
<p>Broadcasting 机制的核心思想是普适性，即同一份数据能普遍适合于其他位置。 在验证普适性之前，需要先将张量 shape 靠右对齐， 然后进行普适性判断：</p>
<ul>
<li>对于<strong>长度为1</strong>的维度，默认这个数据普遍适合于当前维度的其他位置</li>
<li>对于<strong>不存在</strong>的维度， 则在增加新维度后默认当前数据也是普适于新维度的， 从而可以扩展为更多维度数、 任意长度的张量形状</li>
</ul>
</blockquote>
<p>在进行张量运算时，有些运算在处理不同 shape 的张量时，会隐式地自动调用Broadcasting 机制，如+， -， *， /等运算，将参与运算的张量 Broadcasting 成一个公共shape，再进行相应的计算。</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200503181730592.png" alt="加法运算时自动Broadcasting示意图"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.random.normal([<span class="number">2</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.random.normal([<span class="number">32</span>,<span class="number">32</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a+b,a-b,a*b,a/b <span class="comment"># 测试加减乘除运算的 Broadcasting 机制</span></span><br></pre></td></tr></table></figure>
<ul>
<li>这些运算都能 Broadcasting 成[2,32,32,32]的公共 shape，再进行运算</li>
</ul>
<h3 id="9、数学运算">9、数学运算</h3>
<h4 id="1-加减乘除">1.加减乘除</h4>
<p>加、 减、 乘、 除是最基本的数学运算，分别通过 <code>tf.add</code>, <code>tf.subtract</code>, <code>tf.multiply</code>, <code>tf.divide</code>函数实现， TensorFlow 已经重载了+、 - 、 ∗ 、 /运算符，推荐直接使用运算符来完成加、 减、 乘、 除运算</p>
<p>整除和余除也是常见的运算之一，分别通过<code>//</code>和<code>%</code>运算符实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.<span class="built_in">range</span>(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.constant(<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a//b <span class="comment"># 整除运算</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">115</span>, shape=(<span class="number">5</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>])&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a%b <span class="comment"># 余除运算</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">117</span>, shape=(<span class="number">5</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])&gt;</span><br></pre></td></tr></table></figure>
<h4 id="2-乘方">2.乘方</h4>
<p>通过 <code>tf.pow(x, a)</code>可以方便地完成<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><msup><mi>x</mi><mi>a</mi></msup></mrow><annotation encoding="application/x-tex">y=x^a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.664392em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">a</span></span></span></span></span></span></span></span></span></span></span>的乘方运算，也可以通过运算符<code>**</code>实现<code>x∗∗a</code>运算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.<span class="built_in">range</span>(<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.<span class="built_in">pow</span>(x,<span class="number">3</span>) <span class="comment"># 乘方运算</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">124</span>, shape=(<span class="number">4</span>,), dtype=int32, numpy=array([ <span class="number">0</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">27</span>])&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x**<span class="number">2</span> <span class="comment"># 乘方运算符</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">127</span>, shape=(<span class="number">4</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>])&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x=tf.constant([<span class="number">1.</span>,<span class="number">4.</span>,<span class="number">9.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x**(<span class="number">0.5</span>) <span class="comment"># 平方根</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">139</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>设置指数为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mi>a</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{a}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>形式， 即可实现<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mroot><mi>x</mi><mi>a</mi></mroot></mrow><annotation encoding="application/x-tex">\sqrt[a]{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.23972em;"></span><span class="mord sqrt"><span class="root"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.5516160000000001em;"><span style="top:-2.836336em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size6 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span></span></span></span></span></span></span></span><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8002800000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathdefault">x</span></span></span><span style="top:-2.76028em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"><path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.23972em;"><span></span></span></span></span></span></span></span></span>根号运算</li>
<li>对于常见的平方和平方根运算，可以使用 <code>tf.square(x)</code>和 <code>tf.sqrt(x)</code>实现</li>
</ul>
<h4 id="3-指数和对数">3.指数和对数</h4>
<p>通过 <code>tf.pow(a, x)</code>或者<code>**</code>运算符也可以方便地实现指数运算<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>a</mi><mi>x</mi></msup></mrow><annotation encoding="application/x-tex">a^x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.664392em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.constant([<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">2</span>**x <span class="comment"># 指数运算</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">179</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">2.</span>, <span class="number">4.</span>, <span class="number">8.</span>], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.exp(<span class="number">1.</span>) <span class="comment"># 自然指数运算</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">182</span>, shape=(), dtype=float32, numpy=<span class="number">2.7182817</span>&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>对于自然指数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>e</mi><mi>x</mi></msup></mrow><annotation encoding="application/x-tex">e^x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.664392em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span>， 可以通过 <code>tf.exp(x)</code>实现</li>
</ul>
<p>自然对数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mi>e</mi></msub><mi>x</mi></mrow><annotation encoding="application/x-tex">log_ex</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">x</span></span></span></span>可以通过 <code>tf.math.log(x)</code>实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x=tf.exp(<span class="number">3.</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.math.log(x) <span class="comment"># 对数运算</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">186</span>, shape=(), dtype=float32, numpy=<span class="number">3.0</span>&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>如果希望计算其它底数的对数，可以根据对数的换底公式<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mi>a</mi></msub><mi>x</mi><mo>=</mo><mfrac><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mi>e</mi></msub><mi>x</mi></mrow><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mi>e</mi></msub><mi>a</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">log_ax=\frac{log_ex}{log_ea}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.4133239999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">o</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mathdefault mtight">a</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">o</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mathdefault mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>间接实现</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.constant([<span class="number">1.</span>,<span class="number">2.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = <span class="number">10</span>**x</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.math.log(x)/tf.math.log(<span class="number">10.</span>) <span class="comment"># 换底公式</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">6</span>, shape=(<span class="number">2</span>,), dtype=float32, numpy=array([<span class="number">1.</span>, <span class="number">2.</span>], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="4-矩阵乘法">4.矩阵乘法</h4>
<p>通过<code>@</code>运算符可以方便的实现矩阵相乘，还可以通过 <code>tf.matmul(a, b)</code>函数实现</p>
<p>需要注意的是， TensorFlow 中的矩阵相乘可以使用批量方式，也就是张量A和B的维度数可以大于 2。当张量A和B维度数大于 2 时， TensorFlow 会选择A和B的最后两个维度进行矩阵相乘，前面所有的维度都视作Batch 维度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">32</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a@b <span class="comment"># 批量形式的矩阵相乘</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">236</span>, shape=(<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[[[-<span class="number">1.66706240e+00</span>, -<span class="number">8.32602978e+00</span>],</span><br><span class="line">         [ <span class="number">9.83304405e+00</span>, <span class="number">8.15909767e+00</span>],</span><br><span class="line">         [ <span class="number">6.31014729e+00</span>, <span class="number">9.26124632e-01</span>],...</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>矩阵相乘函数同样支持自动 Broadcasting 机制</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>,<span class="number">32</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.random.normal([<span class="number">32</span>,<span class="number">16</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.matmul(a,b) <span class="comment"># 先自动扩展，再矩阵相乘</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">264</span>, shape=(<span class="number">4</span>, <span class="number">28</span>, <span class="number">16</span>), dtype=float32, numpy=</span><br><span class="line">array([[[-<span class="number">1.11323869e+00</span>, -<span class="number">9.48194981e+00</span>, <span class="number">6.48123884e+00</span>, ...,</span><br><span class="line">         <span class="number">6.53280640e+00</span>, -<span class="number">3.10894990e+00</span>, <span class="number">1.53050375e+00</span>],</span><br><span class="line">        [ <span class="number">4.35898495e+00</span>, -<span class="number">1.03704405e+01</span>, <span class="number">8.90656471e+00</span>, ...,</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="二、TensorFlow2进阶操作">二、TensorFlow2进阶操作</h2>
<h3 id="1、合并与分割">1、合并与分割</h3>
<h4 id="1-合并">1.合并</h4>
<p>合并：将多个张量在某个维度上合并为一个张量</p>
<p>张量的合并可以使用拼接(Concatenate)和堆叠(Stack)操作实现：</p>
<ul>
<li>拼接：不会产生新的维度， 仅在现有的维度上合并</li>
<li>堆叠：会创建新维度</li>
</ul>
<p>选择使用拼接还是堆叠操作来合并张量，取决于具体的场景是否需要创建新维度</p>
<h5 id="拼接">拼接</h5>
<p>通过<code>tf.concat(tensors, axis)</code>函数拼接张量，其中参数tensors 保存了所有需要合并的张量 List， axis 参数指定需要合并的维度索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.random.normal([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>]) <span class="comment"># 模拟成绩册 A</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.random.normal([<span class="number">6</span>,<span class="number">35</span>,<span class="number">8</span>]) <span class="comment"># 模拟成绩册 B</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.concat([a,b],axis=<span class="number">0</span>) <span class="comment"># 拼接合并成绩册</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">13</span>, shape=(<span class="number">10</span>, <span class="number">35</span>, <span class="number">8</span>), dtype=float32, numpy=</span><br><span class="line">array([[[ <span class="number">1.95299834e-01</span>, <span class="number">6.87859178e-01</span>, -<span class="number">5.80048323e-01</span>, ...,</span><br><span class="line">          <span class="number">1.29430830e+00</span>, <span class="number">2.56610274e-01</span>, -<span class="number">1.27798581e+00</span>],</span><br><span class="line">        [ <span class="number">4.29753691e-01</span>, <span class="number">9.11329567e-01</span>, -<span class="number">4.47975427e-01</span>, ...,</span><br></pre></td></tr></table></figure>
<ul>
<li>从语法上来说，拼接合并操作可以在任意的维度上进行，唯一的约束是非合并维度的长度<strong>必须一致</strong></li>
</ul>
<h5 id="堆叠">堆叠</h5>
<p>使用<code>tf.stack(tensors, axis)</code>可以堆叠方式合并多个张量，通过 tensors 列表表示， 参数axis 指定新维度插入的位置（用法与 <code>tf.expand_dims</code> 一致，当axis ≥ 0时，在 axis之前插入； 当axis &lt; 0时，在 axis 之后插入新维度）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.random.normal([<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.random.normal([<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.stack([a,b],axis=<span class="number">0</span>) <span class="comment"># 堆叠合并为 2 个班级，班级维度插入在最前</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">55</span>, shape=(<span class="number">2</span>, <span class="number">35</span>, <span class="number">8</span>), dtype=float32, numpy=</span><br><span class="line">array([[[ <span class="number">3.68728966e-01</span>, -<span class="number">8.54765773e-01</span>, -<span class="number">4.77824420e-01</span>,</span><br><span class="line">         -<span class="number">3.83714020e-01</span>, -<span class="number">1.73216307e+00</span>, <span class="number">2.03872994e-02</span>,</span><br><span class="line">          <span class="number">2.63810277e+00</span>, -<span class="number">1.12998331e+00</span>],...</span><br></pre></td></tr></table></figure>
<ul>
<li>需要所有待合并的张量 shape <strong>完全一致</strong>才可合并</li>
</ul>
<h4 id="2-分割">2.分割</h4>
<p>合并操作的逆过程就是分割，将一个张量分拆为多个张量</p>
<p>通过<code>tf.split(x, num_or_size_splits, axis)</code>可以完成张量的分割操作</p>
<ul>
<li>x：待分割张量</li>
<li>num_or_size_splits：切割方案。当 num_or_size_splits 为单个数值时，如 10，表<br>
示等长切割为 10 份；当 num_or_size_splits 为 List 时， List 的每个元素表示每份的长度，如[2,4,2,2]表示切割为 4 份，每份的长度依次是 2、 4、 2、 2</li>
<li>axis：指定分割的维度索引号</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">10</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line"><span class="comment"># 等长切割为 10 份</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result = tf.split(x, num_or_size_splits=<span class="number">10</span>, axis=<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">len</span>(result) <span class="comment"># 返回的列表为 10 个张量的列表</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result[<span class="number">0</span>] <span class="comment"># 查看第一个班级的成绩册张量</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">136</span>, shape=(<span class="number">1</span>, <span class="number">35</span>, <span class="number">8</span>), dtype=float32, numpy=</span><br><span class="line">array([[[-<span class="number">1.7786729</span> , <span class="number">0.2970506</span> , <span class="number">0.02983334</span>, <span class="number">1.3970423</span> ,</span><br><span class="line">          <span class="number">1.315918</span> , -<span class="number">0.79110134</span>, -<span class="number">0.8501629</span> , -<span class="number">1.5549672</span> ],</span><br><span class="line">        [ <span class="number">0.5398711</span> , <span class="number">0.21478991</span>, -<span class="number">0.08685189</span>, <span class="number">0.7730989</span> ,...</span><br></pre></td></tr></table></figure>
<ul>
<li>仍保留了第一个维度</li>
</ul>
<p>如果希望在某个维度上全部按长度为 1 的方式分割，还可以使用<code>tf.unstack(x, axis)</code>函数。这种方式是<code>tf.split</code>的一种特殊情况，切割长度固定为 1，只需要指定切割维度的索引号即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">10</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result = tf.unstack(x,axis=<span class="number">0</span>) <span class="comment"># Unstack 为长度为 1 的张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">len</span>(result) <span class="comment"># 返回 10 个张量的列表</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result[<span class="number">0</span>] <span class="comment"># 第一个班级</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">166</span>, shape=(<span class="number">35</span>, <span class="number">8</span>), dtype=float32, numpy=</span><br><span class="line">array([[-<span class="number">0.2034383</span> , <span class="number">1.1851563</span> , <span class="number">0.25327438</span>, -<span class="number">0.10160723</span>, <span class="number">2.094969</span> , -<span class="number">0.8571669</span> , -<span class="number">0.48985648</span>, <span class="number">0.55798006</span>],...</span><br></pre></td></tr></table></figure>
<ul>
<li>通过<code>tf.unstack</code>切割后，shape 变为[35,8]，即第一个维度消失了，这是与tf.split区别之处</li>
</ul>
<h3 id="2、数据统计">2、数据统计</h3>
<h4 id="1-向量范数">1.向量范数</h4>
<p>向量范数(Vector Norm)是表征向量“长度”的一种度量方法， 它可以推广到张量上。在神经网络中，常用来表示张量的权值大小，梯度大小等。常用的向量范数有：</p>
<ul>
<li>L1 范数，定义为向量x的所有元素绝对值之和：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mo fence="true">∥</mo><mi>x</mi><mo fence="true">∥</mo></mrow><mn>1</mn></msub><mo>=</mo><msub><mo>∑</mo><mi>i</mi></msub><mrow><mo fence="true">∣</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence="true">∣</mo></mrow></mrow><annotation encoding="application/x-tex">\left \| x \right \|_1=\sum_{i}\left | x_i \right |</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.29969999999999997em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∥</span><span class="mord mathdefault">x</span><span class="mclose delimcenter" style="top:0em;">∥</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151408em;"><span style="top:-2.4003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29969999999999997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0497100000000001em;vertical-align:-0.29971000000000003em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span></span></span></span></li>
<li>L2 范数，定义为向量x的所有元素的平方和，再开根号：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mo fence="true">∥</mo><mi>x</mi><mo fence="true">∥</mo></mrow><mn>2</mn></msub><mo>=</mo><msqrt><mrow><msub><mo>∑</mo><mi>i</mi></msub><msup><mrow><mo fence="true">∣</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence="true">∣</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\left \| x \right \|_2=\sqrt{\sum_{i}\left | x_i \right |^2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.29969999999999997em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∥</span><span class="mord mathdefault">x</span><span class="mclose delimcenter" style="top:0em;">∥</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151408em;"><span style="top:-2.4003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29969999999999997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.8400000000000003em;vertical-align:-0.527851em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3121490000000002em;"><span class="svg-align" style="top:-3.8em;"><span class="pstrut" style="height:3.8em;"></span><span class="mord" style="padding-left:1em;"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.954008em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-3.2721489999999998em;"><span class="pstrut" style="height:3.8em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.8800000000000001em;"><svg width="400em" height="1.8800000000000001em" viewbox="0 0 400000 1944" preserveaspectratio="xMinYMin slice"><path d="M1001,80H400000v40H1013.1s-83.4,268,-264.1,840c-180.7,
572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,
-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744c-10,12,-21,25,-33,39s-32,39,-32,39
c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30c26.7,-32.7,52,-63,76,-91s52,-60,52,-60
s208,722,208,722c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,
-658.5c53.7,-170.3,84.5,-266.8,92.5,-289.5c4,-6.7,10,-10,18,-10z
M1001 80H400000v40H1013z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.527851em;"><span></span></span></span></span></span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord">∞</span></span></span></span>-范数，定义为向量x的所有元素绝对值的最大值：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mo fence="true">∥</mo><mi>x</mi><mo fence="true">∥</mo></mrow><mi mathvariant="normal">∞</mi></msub><mo>=</mo><mi>m</mi><mi>a</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mrow><mo fence="true">∣</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence="true">∣</mo></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\left \| x \right \|_\infty=max_i(\left | x_i \right |)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.29969999999999997em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∥</span><span class="mord mathdefault">x</span><span class="mclose delimcenter" style="top:0em;">∥</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.0016920000000000268em;"><span style="top:-2.4003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">∞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29969999999999997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span><span class="mclose">)</span></span></span></span></li>
</ul>
<p>对于矩阵和张量，同样可以利用向量范数的计算公式，等价于将矩阵和张量打平成向量后计算</p>
<p>通过<code>tf.norm(x, ord)</code>求解张量的 L1、 L2、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord">∞</span></span></span></span>等范数，其中参数ord 指定为 1、 2时计算 L1、 L2 范数，指定为<code>np.inf</code>时计算<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord">∞</span></span></span></span>-范数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.ones([<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.norm(x,<span class="built_in">ord</span>=<span class="number">1</span>) <span class="comment"># 计算 L1 范数</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">183</span>, shape=(), dtype=float32, numpy=<span class="number">4.0</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.norm(x,<span class="built_in">ord</span>=<span class="number">2</span>) <span class="comment"># 计算 L2 范数</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">189</span>, shape=(), dtype=float32, numpy=<span class="number">2.0</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.norm(x,<span class="built_in">ord</span>=np.inf) <span class="comment"># 计算∞范数</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">194</span>, shape=(), dtype=float32, numpy=<span class="number">1.0</span>&gt;</span><br></pre></td></tr></table></figure>
<h4 id="2-最值、均值、和">2.最值、均值、和</h4>
<p>通过 <code>tf.reduce_max</code>、 <code>tf.reduce_min</code>、 <code>tf.reduce_mean</code>、 <code>tf.reduce_sum</code> 函数可以求解张量在某个维度上的最大、最小、 均值、和，也可以求全局最大、最小、均值、和信息</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设第一个维度为样本数量，第二个维度为当前样本分别属于 10 个类别的概率</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">4</span>,<span class="number">10</span>]) <span class="comment"># 模型生成概率</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_max(x,axis=<span class="number">1</span>) <span class="comment"># 统计概率维度上的最大值</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">203</span>, shape=(<span class="number">4</span>,), dtype=float32, numpy=array([<span class="number">1.2410722</span> , <span class="number">0.88495886</span>, <span class="number">1.4170984</span> , <span class="number">0.9550192</span> ], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_min(x,axis=<span class="number">1</span>) <span class="comment"># 统计概率维度上的最小值</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">206</span>, shape=(<span class="number">4</span>,), dtype=float32, numpy=array([-<span class="number">0.27862206</span>, -<span class="number">2.4480672</span> , -<span class="number">1.9983795</span> , -<span class="number">1.5287997</span> ], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_mean(x,axis=<span class="number">1</span>) <span class="comment"># 统计概率维度上的均值</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">209</span>, shape=(<span class="number">4</span>,), dtype=float32, numpy=array([ <span class="number">0.39526337</span>, -<span class="number">0.17684573</span>, -<span class="number">0.148988</span> , -<span class="number">0.43544054</span>], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计全局的最大、最小、均值、和，返回的张量均为标量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">4</span>,<span class="number">10</span>]) <span class="comment"># 模型生成概率</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_max(x),tf.reduce_min(x),tf.reduce_mean(x)</span><br><span class="line">(&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">218</span>, shape=(), dtype=float32, numpy=<span class="number">1.8653786</span>&gt;, &lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">220</span>, shape=(), dtype=float32, numpy=-<span class="number">1.9751656</span>&gt;, &lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">222</span>, shape=(), dtype=float32, numpy=<span class="number">0.014772797</span>&gt;)</span><br></pre></td></tr></table></figure>
<ul>
<li>不指定 axis 参数时， <code>tf.reduce_*</code>函数会求解出全局元素的最大、最小、 均值、和等数据</li>
</ul>
<p>通过 <code>tf.argmax(x, axis)</code>和 <code>tf.argmin(x, axis)</code>可以求解在 axis 轴上， x 的最大值、 最小值所在的索引号</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># out：</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">257</span>, shape=(<span class="number">2</span>, <span class="number">10</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">0.18773547</span>, <span class="number">0.1510464</span> , <span class="number">0.09431915</span>, <span class="number">0.13652141</span>, <span class="number">0.06579739</span>,</span><br><span class="line">        <span class="number">0.02033597</span>, <span class="number">0.06067333</span>, <span class="number">0.0666793</span> , <span class="number">0.14594753</span>, <span class="number">0.07094406</span>],</span><br><span class="line">       [<span class="number">0.5092072</span> , <span class="number">0.03887136</span>, <span class="number">0.0390687</span> , <span class="number">0.01911005</span>, <span class="number">0.03850609</span>,</span><br><span class="line">        <span class="number">0.03442522</span>, <span class="number">0.08060656</span>, <span class="number">0.10171875</span>, <span class="number">0.08244187</span>, <span class="number">0.05604421</span>]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pred = tf.argmax(out, axis=<span class="number">1</span>) <span class="comment"># 选取概率最大的位置</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">262</span>, shape=(<span class="number">2</span>,), dtype=int64, numpy=array([<span class="number">0</span>, <span class="number">0</span>], dtype=int64)&gt;</span><br></pre></td></tr></table></figure>
<h3 id="3、张量比较">3、张量比较</h3>
<p>通过<code>tf.equal(a, b)</code>(或<code>tf.math.equal(a, b)</code>，两者等价)函数可以比较2个张量是否相等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pred：</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">272</span>, shape=(<span class="number">100</span>,), dtype=int64, numpy=</span><br><span class="line">array([<span class="number">0</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">4</span>,...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = tf.random.uniform([<span class="number">100</span>],dtype=tf.int64,maxval=<span class="number">10</span>)</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">281</span>, shape=(<span class="number">100</span>,), dtype=int64, numpy=</span><br><span class="line">array([<span class="number">0</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>,...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = tf.equal(pred,y) <span class="comment"># 预测值与真实值比较，返回布尔类型的张量</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">288</span>, shape=(<span class="number">100</span>,), dtype=<span class="built_in">bool</span>, numpy=</span><br><span class="line">array([<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>,...</span><br></pre></td></tr></table></figure>
<p>相关函数汇总表：</p>
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">比较逻辑</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">tf.equal(a, b)</td>
<td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>=</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a=b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span></td>
</tr>
<tr>
<td style="text-align:center">tf.math.greater(a, b)</td>
<td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>&gt;</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a&gt;b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span></td>
</tr>
<tr>
<td style="text-align:center">tf.math.less(a, b)</td>
<td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>&lt;</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a&lt;b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span></td>
</tr>
<tr>
<td style="text-align:center">tf.math.greater_equal(a, b)</td>
<td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>⩾</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a\geqslant b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7733399999999999em;vertical-align:-0.13667em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel amsrm">⩾</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span></td>
</tr>
<tr>
<td style="text-align:center">tf.math.less_equal(a, b)</td>
<td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>⩽</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a\leqslant b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7733399999999999em;vertical-align:-0.13667em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel amsrm">⩽</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span></td>
</tr>
<tr>
<td style="text-align:center">tf.math.not_equal(a, b)</td>
<td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mi mathvariant="normal">≠</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">a\neq b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="rlap"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="inner"><span class="mrel"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span></td>
</tr>
<tr>
<td style="text-align:center">tf.math.is_nan(a, b)</td>
<td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>=</mo><mi>n</mi><mi>a</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">a=nan</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span></span></span></span></td>
</tr>
</tbody>
</table>
<h3 id="4、复制和填充">4、复制和填充</h3>
<h4 id="1-填充">1.填充</h4>
<p>之前我们介绍了通过复制的方式可以增加数据的长度，但是重复复制数据会破坏原有的数据结构，并不适合于此处。通常的做法是，在需要补充长度的数据开始或结束处填充足够数量的特定数值， 这些特定数值一般代表了<strong>无效意义</strong>，例如 0，使得填充后的长度满足系统要求。那么这种操作就叫作填充(Padding)</p>
<p>填充操作可以通过<code>tf.pad(x, paddings)</code>函数实现， 参数 paddings 是包含了多个[Left Padding, Right Padding]的嵌套方案 List，如[[0,0], [2,1], [1,2]]表示第一个维度不填充， 第二个维度左边(起始处)填充两个单元， 右边(结束处)填充一个单元， 第三个维度左边填充一个单元， 右边填充两个单元</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]) <span class="comment"># 第一个句子</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.constant([<span class="number">7</span>,<span class="number">8</span>,<span class="number">1</span>,<span class="number">6</span>]) <span class="comment"># 第二个句子</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.pad(b, [[<span class="number">0</span>,<span class="number">2</span>]]) <span class="comment"># 句子末尾填充 2 个 0</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">3</span>, shape=(<span class="number">6</span>,), dtype=int32, numpy=array([<span class="number">7</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">0</span>])&gt;</span><br></pre></td></tr></table></figure>
<h4 id="2-复制">2.复制</h4>
<p>即<code>tf.tile()</code>函数。参考<code>基础操作-&gt;维度变换-&gt;复制数据</code>章节</p>
<h3 id="5、数据限幅">5、数据限幅</h3>
<p>可以通过<code>tf.maximum(x, a)</code>实现数据的下限幅，即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>∈</mo><mo stretchy="false">[</mo><mi>a</mi><mo separator="true">,</mo><mo>+</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x\in [a,+\infty )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">+</span><span class="mord">∞</span><span class="mclose">)</span></span></span></span>。可以通过<code>tf.minimum(x, a)</code>实现数据的上限幅，即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>∈</mo><mo stretchy="false">(</mo><mo>−</mo><mi mathvariant="normal">∞</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">x\in (-\infty ,a]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">−</span><span class="mord">∞</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">a</span><span class="mclose">]</span></span></span></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.<span class="built_in">range</span>(<span class="number">9</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.maximum(x,<span class="number">2</span>) <span class="comment"># 下限幅到 2</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">48</span>, shape=(<span class="number">9</span>,), dtype=int32, numpy=array([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.minimum(x,<span class="number">7</span>) <span class="comment"># 上限幅到 7</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">41</span>, shape=(<span class="number">9</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>])&gt;</span><br></pre></td></tr></table></figure>
<p>通过组合<code>tf.maximum(x, a)</code>和<code>tf.minimum(x, b)</code>可以实现同时对数据的上下边界限幅，即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>∈</mo><mo stretchy="false">[</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">x\in [a,b]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">b</span><span class="mclose">]</span></span></span></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.<span class="built_in">range</span>(<span class="number">9</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.minimum(tf.maximum(x,<span class="number">2</span>),<span class="number">7</span>) <span class="comment"># 限幅为 2~7</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">57</span>, shape=(<span class="number">9</span>,), dtype=int32, numpy=array([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>])&gt;</span><br></pre></td></tr></table></figure>
<p>更方便地，我们可以使用<code>tf.clip_by_value</code>函数实现上下限幅：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.<span class="built_in">range</span>(<span class="number">9</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.clip_by_value(x,<span class="number">2</span>,<span class="number">7</span>) <span class="comment"># 限幅为 2~7</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">66</span>, shape=(<span class="number">9</span>,), dtype=int32, numpy=array([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>])&gt;</span><br></pre></td></tr></table></figure>
<h3 id="6、高级操作">6、高级操作</h3>
<h4 id="1-tf-gather">1.tf.gather</h4>
<p><code>tf.gather</code>可以实现根据索引号收集数据的目的（与切片类似，但是对于<strong>不规则</strong>的索引方式，切片实现起来非常麻烦， 而<code>tf.gather</code>则更加方便）</p>
<ul>
<li>参数1：带收集的张量</li>
<li>参数2：指定需要收集的索引号</li>
<li>参数3：指定收集的维度</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.uniform([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>],maxval=<span class="number">100</span>,dtype=tf.int32) <span class="comment"># 成绩册张量（4个班级 35个学生 8门科目）</span></span><br><span class="line"><span class="comment"># 收集第 1,4,9,12,13,27 号同学成绩</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.gather(x,[<span class="number">0</span>,<span class="number">3</span>,<span class="number">8</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">26</span>],axis=<span class="number">1</span>)</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">87</span>, shape=(<span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>), dtype=int32, numpy=</span><br><span class="line">array([[[<span class="number">43</span>, <span class="number">10</span>, <span class="number">93</span>, <span class="number">85</span>, <span class="number">75</span>, <span class="number">87</span>, <span class="number">28</span>, <span class="number">19</span>],</span><br><span class="line">        [<span class="number">74</span>, <span class="number">11</span>, <span class="number">25</span>, <span class="number">64</span>, <span class="number">84</span>, <span class="number">89</span>, <span class="number">79</span>, <span class="number">85</span>],...</span><br></pre></td></tr></table></figure>
<p>索引号可以<strong>乱序</strong>排列，此时收集的数据也是<strong>对应顺序</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=tf.<span class="built_in">range</span>(<span class="number">8</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=tf.reshape(a,[<span class="number">4</span>,<span class="number">2</span>]) <span class="comment"># 生成张量 a</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">115</span>, shape=(<span class="number">4</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">       [<span class="number">6</span>, <span class="number">7</span>]])&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.gather(a,[<span class="number">3</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>],axis=<span class="number">0</span>) <span class="comment"># 收集第 4,2,1,3 号元素</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">119</span>, shape=(<span class="number">4</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>]])&gt;</span><br></pre></td></tr></table></figure>
<h4 id="2-tf-gather-nd">2.tf.gather_nd</h4>
<p>通过 tf.gather_nd 函数，可以通过指定每次采样点的多维坐标来实现采样多个点的目的（利用手动一个一个提取然后stack合并的方式也可以达到同样效果，但是效率极低）</p>
<ul>
<li>参数1：带收集的张量</li>
<li>参数2：指定的采样点的索引坐标</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.uniform([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>],maxval=<span class="number">100</span>,dtype=tf.int32) <span class="comment"># 成绩册张量（4个班级 35个学生 8门科目）</span></span><br><span class="line"><span class="comment"># 根据多维坐标收集数据</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.gather_nd(x,[[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">3</span>]])<span class="comment">#抽查第 2 个班级的第 2 个同学的所有科目，第 3 个班级的第 3 个同学的所有科目，第 4 个班级的第 4 个同学的所有科目</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">256</span>, shape=(<span class="number">3</span>, <span class="number">8</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">45</span>, <span class="number">34</span>, <span class="number">99</span>, <span class="number">17</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">43</span>, <span class="number">86</span>],</span><br><span class="line">       [<span class="number">11</span>, <span class="number">25</span>, <span class="number">84</span>, <span class="number">95</span>, <span class="number">97</span>, <span class="number">95</span>, <span class="number">69</span>, <span class="number">69</span>],</span><br><span class="line">       [ <span class="number">0</span>, <span class="number">89</span>, <span class="number">52</span>, <span class="number">29</span>, <span class="number">76</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">98</span>]])&gt;</span><br></pre></td></tr></table></figure>
<p>一般地，在使用 <code>tf.gather_nd</code> 采样多个样本时， 例如希望采样i号班级，j个学生，k门科目的成绩，则可以表达为[. . . , [i,j,k], . . . ]， 外层的括号长度为采样样本的个数，内层列表包含了每个采样点的索引坐标</p>
<h4 id="3-tf-boolean-mask">3.tf.boolean_mask</h4>
<p>除了可以通过给定索引号的方式采样，还可以通过给定掩码(Mask)的方式进行采样。通过 <code>tf.boolean_mask(x, mask, axis)</code>可以在 axis 轴上根据mask 方案进行采样</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.uniform([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>],maxval=<span class="number">100</span>,dtype=tf.int32) <span class="comment"># 成绩册张量（4个班级 35个学生 8门科目）</span></span><br><span class="line"><span class="comment"># 根据掩码方式采样班级，给出掩码和维度索引</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.boolean_mask(x,mask=[<span class="literal">True</span>, <span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>],axis=<span class="number">0</span>)</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">288</span>, shape=(<span class="number">2</span>, <span class="number">35</span>, <span class="number">8</span>), dtype=int32, numpy=</span><br><span class="line">array([[[<span class="number">43</span>, <span class="number">10</span>, <span class="number">93</span>, <span class="number">85</span>, <span class="number">75</span>, <span class="number">87</span>, <span class="number">28</span>, <span class="number">19</span>],...</span><br></pre></td></tr></table></figure>
<ul>
<li>掩码的长度必须与对应维度的<strong>长度一致</strong></li>
<li>掩码可以是List嵌套，此时效果与<code>tf.gather_nd</code>类似</li>
</ul>
<p><code>tf.boolean_mask</code>既可以实现了<code>tf.gather</code>方式的一维掩码采样， 又可以实现<code>tf.gather_nd</code>方式的多维掩码采样</p>
<h4 id="4-tf-where">4.tf.where</h4>
<p>通过<code>tf.where(cond, a, b)</code>操作可以根据 cond 条件的真假从参数A或B中读取数据， 条件判定规则如下：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>o</mi><mi>i</mi></msub><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>a</mi><mi>i</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>c</mi><mi>o</mi><mi>n</mi><msub><mi>d</mi><mi>i</mi></msub><mi mathvariant="normal">为</mi><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>b</mi><mi>i</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>c</mi><mi>o</mi><mi>n</mi><msub><mi>d</mi><mi>i</mi></msub><mi mathvariant="normal">为</mi><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">o_i=\left\{\begin{matrix}
a_i &amp; cond_i为True\\ 
b_i &amp; cond_i为False
\end{matrix}\right.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">{</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">为</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">u</span><span class="mord mathdefault">e</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">为</span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>。其中i为张量的元素索引， 返回的张量大小与A和B一致， 当对应位置的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mi>o</mi><mi>n</mi><msub><mi>d</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">cond_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>为 True， <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>o</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">o_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>从<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">a_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>中复制数据；当对应位置的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mi>o</mi><mi>n</mi><msub><mi>d</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">cond_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>为 False， <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>o</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">o_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>从<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>中复制数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.ones([<span class="number">3</span>,<span class="number">3</span>]) <span class="comment"># 构造 a 为全 1 矩阵</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.zeros([<span class="number">3</span>,<span class="number">3</span>]) <span class="comment"># 构造 b 为全 0 矩阵</span></span><br><span class="line"><span class="comment"># 构造采样条件</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cond = tf.constant([[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>],[<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">False</span>],[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.where(cond,a,b) <span class="comment"># 根据条件从 a,b 中采样</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">384</span>, shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<p>当参数a=b=None时，tf.where 会返回 cond 张量中所有 True 的元素的索引坐标</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>cond = tf.constant([[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>],[<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">False</span>],[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.where(cond) <span class="comment"># 获取 cond 中为 True 的元素索引</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">387</span>, shape=(<span class="number">4</span>, <span class="number">2</span>), dtype=int64, numpy=</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">1</span>]], dtype=int64)&gt;</span><br></pre></td></tr></table></figure>
<hr>
<h5 id="例子">例子</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">3</span>,<span class="number">3</span>]) <span class="comment"># 构造 a</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">403</span>, shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[-<span class="number">2.2946844</span> , <span class="number">0.6708417</span> , -<span class="number">0.5222212</span> ],</span><br><span class="line">       [-<span class="number">0.6919401</span> , -<span class="number">1.9418817</span> , <span class="number">0.3559235</span> ],</span><br><span class="line">       [-<span class="number">0.8005251</span> , <span class="number">1.0603906</span> , -<span class="number">0.68819374</span>]], dtype=float32)&gt;</span><br><span class="line"><span class="comment"># 通过比较运算，得到所有正数的掩码：</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask=x&gt;<span class="number">0</span> <span class="comment"># 比较操作，等同于 tf.math.greater()</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">405</span>, shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=<span class="built_in">bool</span>, numpy=</span><br><span class="line">array([[<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">       [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>],</span><br><span class="line">       [<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>]])&gt;</span><br><span class="line"><span class="comment"># 通过 tf.where 提取此掩码处 True 元素的索引坐标：</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>indices=tf.where(mask) <span class="comment"># 提取所有大于 0 的元素索引</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">407</span>, shape=(<span class="number">3</span>, <span class="number">2</span>), dtype=int64, numpy=</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">1</span>]], dtype=int64)&gt;</span><br><span class="line"><span class="comment"># 拿到索引后，通过 tf.gather_nd 即可恢复出所有正数的元素：</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.gather_nd(x,indices) <span class="comment"># 提取正数的元素值</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">410</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">0.6708417</span>, <span class="number">0.3559235</span>, <span class="number">1.0603906</span>], dtype=float32)&gt;</span><br><span class="line"><span class="comment"># 实际上，也可以直接通过 tf.boolean_mask 获取所有正数的元素向量:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.boolean_mask(x,mask) <span class="comment"># 通过掩码提取正数的元素值</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">439</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">0.6708417</span>, <span class="number">0.3559235</span>, <span class="number">1.0603906</span>], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h4 id="5-scatter-nd">5.scatter_nd</h4>
<p>通过<code>tf.scatter_nd(indices, updates, shape)</code>函数可以高效地刷新张量的部分数据，但是这个函数只能在<strong>全0</strong>的白板张量上面执行刷新操作，因此可能需要结合其它操作来实现现有张量的数据刷新功能</p>
<p>白板的形状通过 shape 参数表示，需要刷新的数据索引号通过 indices 表示，新数据为 updates。 根据 indices 给出的索引位置将 updates 中新的数据依次写入白板中，并返回更新后的结果张量</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200504220326506.png" alt="scatter_nd更新数据示意图"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造需要刷新数据的位置参数，即为 4、 3、 1 和 7 号位置</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>indices = tf.constant([[<span class="number">4</span>], [<span class="number">3</span>], [<span class="number">1</span>], [<span class="number">7</span>]])</span><br><span class="line"><span class="comment"># 构造需要写入的数据， 4 号位写入 4.4,3 号位写入 3.3，以此类推</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>updates = tf.constant([<span class="number">4.4</span>, <span class="number">3.3</span>, <span class="number">1.1</span>, <span class="number">7.7</span>])</span><br><span class="line"><span class="comment"># 在长度为 8 的全 0 向量上根据 indices 写入 updates 数据</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.scatter_nd(indices, updates, [<span class="number">8</span>])</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">467</span>, shape=(<span class="number">8</span>,), dtype=float32, numpy=array([<span class="number">0.</span> , <span class="number">1.1</span>, <span class="number">0.</span> , <span class="number">3.3</span>, <span class="number">4.4</span>, <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">7.7</span>], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<p>3维张量刷新例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造写入位置，即 2 个位置</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>indices = tf.constant([[<span class="number">1</span>],[<span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>updates = tf.constant([<span class="comment"># 构造写入数据，即 2 个矩阵</span></span><br><span class="line">[[<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">7</span>,<span class="number">7</span>,<span class="number">7</span>],[<span class="number">8</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">8</span>]],</span><br><span class="line">[[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>]]</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 在 shape 为[4,4,4]白板上根据 indices 写入 updates</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.scatter_nd(indices,updates,[<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">477</span>, shape=(<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>), dtype=int32, numpy=</span><br><span class="line">array([[[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line">       [[<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>], <span class="comment"># 写入的新数据 1</span></span><br><span class="line">        [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">7</span>],</span><br><span class="line">        [<span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>]],</span><br><span class="line">       [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line">       [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], <span class="comment"># 写入的新数据 2</span></span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]]])&gt;</span><br></pre></td></tr></table></figure>
<h4 id="6-meshgrid">6.meshgrid</h4>
<p>通过<code>tf.meshgrid</code>函数可以方便地生成二维网格的采样点坐标，方便可视化等应用场合</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.linspace(-<span class="number">8.</span>,<span class="number">8</span>,<span class="number">100</span>) <span class="comment"># 设置 x 轴的采样点</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = tf.linspace(-<span class="number">8.</span>,<span class="number">8</span>,<span class="number">100</span>) <span class="comment"># 设置 y 轴的采样点</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x,y = tf.meshgrid(x,y) <span class="comment"># 生成网格点，并内部拆分后返回</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.shape,y.shape <span class="comment"># 打印拆分后的所有点的 x,y 坐标张量 shape</span></span><br><span class="line">(TensorShape([<span class="number">100</span>, <span class="number">100</span>]), TensorShape([<span class="number">100</span>, <span class="number">100</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = tf.sqrt(x**<span class="number">2</span>+y**<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = tf.sin(z)/z <span class="comment"># sinc 函数实现</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 导入 3D 坐标轴支持</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fig = plt.figure()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ax = Axes3D(fig) <span class="comment"># 设置 3D 坐标轴</span></span><br><span class="line"><span class="comment"># 根据网格点绘制 sinc 函数 3D 曲面</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ax.contour3D(x.numpy(), y.numpy(), z.numpy(), <span class="number">50</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>plt.show()</span><br></pre></td></tr></table></figure>
<ul>
<li>通过在 x 轴上进行采样 100 个数据点， y 轴上采样 100 个数据点，然后利用tf.meshgrid(x, y)即可返回这 10000 个数据点的张量数据， 保存在 shape 为[100,100,2]的张量中。为了方便计算， tf.meshgrid 会返回在 axis=2 维度切割后的 2 个张量A和B，其中张量A包含了所有点的 x 坐标， B包含了所有点的 y 坐标， shape 都为[100,100]</li>
</ul>
<h3 id="7、经典数据集加载">7、经典数据集加载</h3>
<p><code>keras.datasets</code> 模块提供了常用经典数据集的自动下载、 管理、 加载与转换功能，并且提供了 <code>tf.data.Dataset</code> 数据集对象， 方便实现多线程(Multi-threading)、 预处理(Preprocessing)、 随机打散(Shuffle)和批训练(Training on Batch)等常用数据集的功能</p>
<p>常用的经典数据集：</p>
<ul>
<li>Boston Housing， 波士顿房价趋势数据集，用于回归模型训练与测试</li>
<li>CIFAR10/100， 真实图片数据集，用于图片分类任务</li>
<li>MNIST/Fashion_MNIST， 手写数字图片数据集，用于图片分类任务</li>
<li>IMDB， 情感分类任务数据集，用于文本分类任务</li>
</ul>
<hr>
<p>通过<code>datasets.xxx.load_data()</code>函数即可实现经典数据集的<strong>自动加载</strong>，其中 xxx 代表具体的数据集名称，如“CIFAR10”、“MNIST”。TensorFlow会默认将数据缓存在用户目录下的<code>.keras/datasets</code> 文件夹，用户无需关心数据集是如何保存的。如果当前数据集不在缓存中，则会自动从网络下载、 解压和加载数据集；如果已经在缓存中， 则自动完成加载</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets <span class="comment"># 导入经典数据集加载模块</span></span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>(x, y), (x_test, y_test) = datasets.mnist.load_data()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">&#x27;x:&#x27;</span>, x.shape, <span class="string">&#x27;y:&#x27;</span>, y.shape, <span class="string">&#x27;x test:&#x27;</span>, x_test.shape, <span class="string">&#x27;y test:&#x27;</span>, y_test)</span><br><span class="line"><span class="comment"># 返回数组的形状</span></span><br><span class="line">x: (<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>) y: (<span class="number">60000</span>,) x test: (<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>) y test: [<span class="number">7</span> <span class="number">2</span> <span class="number">1</span> ... <span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>通过 load_data()函数会返回相应格式的数据，对于图片数据集 MNIST、 CIFAR10 等，会返回 2 个 tuple，第一个 tuple 保存了用于训练的数据 x 和 y 训练集对象；第 2 个 tuple 则保存了用于测试的数据 x_test 和 y_test 测试集对象，所有的数据都用 Numpy 数组容器保存</li>
</ul>
<p>数据加载进入内存后，需要转换成 <code>Dataset</code> 对象。通过<code>Dataset.from_tensor_slices</code>可以将数据<strong>转换成Dataset对象</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>train_db = tf.data.Dataset.from_tensor_slices((x, y)) <span class="comment"># 以将训练部分的数据图片x和标签y转换成Dataset对象</span></span><br></pre></td></tr></table></figure>
<h4 id="1-随机打散">1.随机打散</h4>
<p>通过<code>Dataset.shuffle(buffer_size)</code>工具可以设置 Dataset 对象随机打散数据之间的顺序，防止每次训练时数据按固定顺序产生，从而使得模型尝试“记忆”住标签信息</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_db为Dataset对象</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>train_db = train_db.shuffle(<span class="number">10000</span>) <span class="comment"># 随机打散样本，不会打乱样本与标签映射关系</span></span><br></pre></td></tr></table></figure>
<p>buffer_size 参数指定缓冲池的大小，一般设置为一个较大的常数即可。 调用 Dataset提供的这些工具函数会<strong>返回新的Dataset对象</strong>，可以通过<code>db = db.step1().step2().step3()</code>方式按序完成所有的数据处理步骤</p>
<h4 id="2-批训练">2.批训练</h4>
<p>为了利用显卡的并行计算能力，一般在网络的计算过程中会同时计算多个样本，把这种训练方式叫做批训练，其中一个批中样本的数量叫做<code>Batch Size</code></p>
<p>为了一次能够从Dataset 中产生 Batch Size 数量的样本，需要设置 Dataset 为批训练方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_db为Dataset对象</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>train_db = train_db.batch(<span class="number">128</span>) <span class="comment"># 设置批训练， batch size 为 128</span></span><br></pre></td></tr></table></figure>
<ul>
<li>一次并行计算 128 个样本的数据</li>
</ul>
<p>Batch Size 一般根据用户的 GPU 显存资源来设置，当显存不足时，可以适量减少 Batch Size 来减少显存使用量</p>
<h4 id="3-预处理">3.预处理</h4>
<p>Dataset 对象通过提供<code>map(func)</code>工具函数， 可以非常方便地调用用户自定义的预处理逻辑， 它实现在func函数里</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params">x, y</span>):</span> <span class="comment"># 自定义的预处理函数</span></span><br><span class="line">    <span class="comment"># 调用此函数时会自动传入 x,y 对象， shape 为[b, 28, 28], [b]</span></span><br><span class="line">    x = tf.cast(x, dtype=tf.float32) / <span class="number">255.</span><span class="comment"># 标准化到 0~1</span></span><br><span class="line">    x = tf.reshape(x, [-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>]) <span class="comment"># 打平</span></span><br><span class="line">    y = tf.cast(y, dtype=tf.int32) <span class="comment"># 转成整型张量</span></span><br><span class="line">    y = tf.one_hot(y, depth=<span class="number">10</span>) <span class="comment"># one-hot 编码</span></span><br><span class="line">    <span class="comment"># 返回的 x,y 将替换传入的 x,y 参数，从而实现数据的预处理功能</span></span><br><span class="line">    <span class="keyword">return</span> x,y</span><br><span class="line"><span class="comment"># 预处理函数实现在 preprocess 函数中，传入函数名即可</span></span><br><span class="line">train_db = train_db.<span class="built_in">map</span>(preprocess)</span><br></pre></td></tr></table></figure>
<ul>
<li>one_hot：one hot编码会使得y的维度由[b]变为[b, 10]，在后面与网络输出结果out（维度[b, 10]）进行相减然后求计算误差</li>
</ul>
<h4 id="4-循环训练">4.循环训练</h4>
<p>对于 Dataset 对象， 在使用时可以通过</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step, (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db): <span class="comment"># 迭代数据集对象，带 step 参数</span></span><br></pre></td></tr></table></figure>
<p>或</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> train_db: <span class="comment"># 迭代数据集对象</span></span><br></pre></td></tr></table></figure>
<p>方式进行迭代，每次返回的 x 和 y 对象即为批量样本和标签，当对 train_db 的所有样本完成一次迭代后， for 循环终止退出</p>
<p>这样完成一个 Batch 的数据训练（执行一次循环体），叫做一个<strong>Step</strong></p>
<p>通过多个 step 来完成整个训练集的一次迭代（执行一次整个循环），叫做一个<strong>Epoch</strong></p>
<p>在实际训练时，通常需要对数据集迭代多个 Epoch 才能取得较好地训练效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>): <span class="comment"># 训练 Epoch 数</span></span><br><span class="line">    <span class="keyword">for</span> step, (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db): <span class="comment"># 迭代 Step 数</span></span><br><span class="line">        <span class="comment"># training...</span></span><br><span class="line"><span class="comment"># 可以通过repeat设置epoch的迭代次数（上面与下面等价）</span></span><br><span class="line">train_db = train_db.repeat(<span class="number">20</span>) <span class="comment"># 数据集迭代 20 遍才终止</span></span><br><span class="line"><span class="keyword">for</span> step, (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db): <span class="comment"># 迭代 Step 数</span></span><br><span class="line">    <span class="comment"># training...</span></span><br></pre></td></tr></table></figure>
<ul>
<li>通过repeat，使得数据集对象内部遍历多次才会退出</li>
</ul>
<h3 id="8、MNIST实战">8、MNIST实战</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="三、神经网络">三、神经网络</h2>
<p>神经网络属于机器学习的一个研究分支，它特指利用多个神经元去参数化映射函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>f</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">f_\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的模型</p>
<h3 id="1、感知机">1、感知机</h3>
<p>感知机是线性模型，并不能处理线性不可分问题。通过在线性模型后添加激活函数后得到<br>
活性值(Activation) <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span></span></span></span>：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>W</mi><mi>T</mi></msup><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a=\sigma (z)=\sigma (W^Tx+b)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span></span></span></span>，其中激活函数可以是阶跃函数(Step function)，也可以是符号函数(Sign function)</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505012316465.png" alt="感知机模型"></p>
<ul>
<li>接受长度为𝑛的一维向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mo stretchy="false">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">x=[x_1,x_2,...,x_n]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></li>
<li>每个输入节点通过权值为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>i</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">w_i,i\in [1,n]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mclose">]</span></span></span></span>的连接汇集为变量𝑧，即：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi><mo>=</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo>+</mo><msub><mi>w</mi><mi>n</mi></msub><msub><mi>x</mi><mi>n</mi></msub><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">z=w_1x_1+w_2x_2+...+w_nx_n+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>
<ul>
<li>𝑏称为感知机的偏置(Bias)</li>
<li>一维向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi><mo>=</mo><mo stretchy="false">[</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>w</mi><mi>n</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">W=[w_1,w_2,..w_n]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>称为感知机的权值(Weight)</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span>称为感知机的净活性值(Net Activation)</li>
</ul>
</li>
</ul>
<p>但是阶跃函数和符号函数在0处是不连续的， 其他位置导数为 0，无法利用梯度下降算法进行参数优化</p>
<p>以感知机为代表的线性模型不能解决异或(XOR)等线性不可分问题</p>
<h3 id="2、全连接层">2、全连接层</h3>
<p>现代深度学习的核心结构在感知机的基础上，将不连续的阶跃激活函数换成了其它平滑连续可导的激活函数， 并通过堆叠多个网络层来增强网络的表达能力</p>
<p>如下所示的整个网络层可以通过矩阵关系式表达<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>=</mo><mi>X</mi><mi mathvariant="normal">@</mi><mi>W</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">O=X@W+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord">@</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span></p>
<ul>
<li>输入矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span>的 shape 定义为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[b,d_{in}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>为样本数量，此处只有1个样本参与前向运算，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{in}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>为输入节点数（即<strong>输入特征长度</strong>）</li>
<li>权值矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>的 shape 定义为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[d_{in},d_{out}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>为输出节点数（即<strong>输出特征长度</strong>）</li>
<li>偏置向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>的 shape 定义为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[d_{out}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></li>
<li>输出矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi></mrow><annotation encoding="application/x-tex">O</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span></span></span></span>包含了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>个样本的输出特征， shape 为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[b,d_{out}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></li>
</ul>
<p>由于每个输出节点与全部的输入节点相连接，这种网络层称为<strong>全连接层</strong>(Fully-connected Layer)，或者稠密连接层(Dense Layer)，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>矩阵叫做全连接层的权值矩阵，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>向量叫做全连接层的偏置向量</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505013252398.png" alt="全连接层"></p>
<h4 id="1-张量方式实现">1.张量方式实现</h4>
<p>要实现全连接层，只需要定义好权值张量𝑾和偏置张量𝒃，并利用批量矩阵相乘函数<code>tf.matmul()</code>即可完成网络层的计算</p>
<p>例如， 创建输入𝑿矩阵为𝑏 = 2个样本，每个样本的输入特征长度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{in}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> = 784，输出节点数为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> = 256，故定义权值矩阵𝑾的 shape 为[784,256]，并采用正态分布初始化𝑾；偏置向量𝒃的 shape 定义为[256]，在计算完𝑿@𝑾后相加即可，最终全连接层的输出𝑶的 shape 为[2,256]，即 2 个样本的特征，每个特征长度为 256，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 W,b 张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">2</span>,<span class="number">784</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>w1 = tf.Variable(tf.random.truncated_normal([<span class="number">784</span>, <span class="number">256</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b1 = tf.Variable(tf.zeros([<span class="number">256</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>o1 = tf.matmul(x,w1) + b1 <span class="comment"># 线性变换</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>o1 = tf.nn.relu(o1) <span class="comment"># 激活函数</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">31</span>, shape=(<span class="number">2</span>, <span class="number">256</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">1.51279330e+00</span>, <span class="number">2.36286330e+00</span>, <span class="number">8.16453278e-01</span>,</span><br><span class="line">         <span class="number">1.80338228e+00</span>, <span class="number">4.58602428e+00</span>, <span class="number">2.54454136e+00</span>,...</span><br></pre></td></tr></table></figure>
<h4 id="2-层方式实现">2.层方式实现</h4>
<p>作为最常用的网络层之一，TensorFlow中有更高层、使用更方便的层实现方式： <code>layers.Dense(units, activation)</code>。</p>
<p>通过<code>layer.Dense</code>类， 只需要指定输出节点数 Units 和激活函数类型 activation 即可</p>
<p>注意：</p>
<ul>
<li>输入节点数会根据第一次运算时的输入 shape 确定，同时根据输入、输出节点数自动创建并初始化权值张量𝑾和偏置张量𝒃（因此在新建类 Dense 实例时，并不会立即创建权值张量𝑾和偏置张量𝒃， 而是需要调用 build 函数或者直接进行一次前向计算，才能完成网络参数的创建）</li>
<li>activation参数指定当前层的激活函数，可以为常见的激活函数或自定义激活函数，也可以指定为 None，即无激活函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers <span class="comment"># 导入层模块</span></span><br><span class="line"><span class="comment"># 创建全连接层，指定输出节点数和激活函数</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fc = layers.Dense(<span class="number">512</span>, activation=tf.nn.relu)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h1 = fc(x) <span class="comment"># 通过 fc 类实例完成一次全连接层的计算，返回输出张量</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">72</span>, shape=(<span class="number">4</span>, <span class="number">512</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">0.63339347</span>, <span class="number">0.21663809</span>, <span class="number">0.</span> , ..., <span class="number">1.7361937</span> , <span class="number">0.39962345</span>, <span class="number">2.4346168</span> ],...</span><br></pre></td></tr></table></figure>
<p>通过类内部的成员名 kernel 和 bias 来获取权值张量W和偏置张量b对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>fc.kernel <span class="comment"># 获取 Dense 类的权值矩阵</span></span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;dense_1/kernel:0&#x27;</span> shape=(<span class="number">784</span>, <span class="number">512</span>) dtype=float32, numpy=</span><br><span class="line">array([[-<span class="number">0.04067389</span>, <span class="number">0.05240148</span>, <span class="number">0.03931375</span>, ..., -<span class="number">0.01595572</span>, -<span class="number">0.01075954</span>, -<span class="number">0.06222073</span>],</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fc.bias <span class="comment"># 获取 Dense 类的偏置向量</span></span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;dense_1/bias:0&#x27;</span> shape=(<span class="number">512</span>,) dtype=float32, numpy=</span><br><span class="line">array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,</span><br></pre></td></tr></table></figure>
<p>在优化参数时，需要获得网络的所有待优化的张量参数列表，可以通过类的<code>trainable_variables</code>来返回<strong>待优化参数列表</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>fc.trainable_variables <span class="comment"># 返回待优化参数列表</span></span><br><span class="line">[&lt;tf.Variable <span class="string">&#x27;dense_1/kernel:0&#x27;</span> shape=(<span class="number">784</span>, <span class="number">512</span>) dtype=float32,...,</span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;dense_1/bias:0&#x27;</span> shape=(<span class="number">512</span>,) dtype=float32, numpy=...]</span><br></pre></td></tr></table></figure>
<p>网络层除了保存了待优化张量列表 <code>trainable_variables</code>，还有部分层包含了不参与梯度优化的张量，如后续介绍的 Batch Normalization 层， 可以通过<code>non_trainable_variables</code>成员返回所有<strong>不需要优化参数列表</strong>。如果希望获得<strong>所有参数列表</strong>， 可以通过类的<code>variables</code>返回</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>fc.variables <span class="comment"># 返回所有参数列表</span></span><br><span class="line">[&lt;tf.Variable <span class="string">&#x27;dense_1/kernel:0&#x27;</span> shape=(<span class="number">784</span>, <span class="number">512</span>) dtype=float32,...,</span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;dense_1/bias:0&#x27;</span> shape=(<span class="number">512</span>,) dtype=float32, numpy=...]</span><br></pre></td></tr></table></figure>
<ul>
<li>对于全连接层，内部张量都参与梯度优化</li>
</ul>
<p>利用网络层类对象进行前向计算时，只需要调用类的<code>__call__</code>方法即可，即写成<code>fc(x)</code>方式便可（会自动调用类的<code>__call__</code>方法，在<code>__call__</code>方法中会自动调用call方法，由 TensorFlow 框架自动完成）</p>
<h3 id="3、神经网络">3、神经网络</h3>
<p>通过层层堆叠全连接层，保证前一层的输出节点数与当前层的输入节点数匹配，即可堆叠出任意层数的网络。把这种由神经元相互连接而成的网络叫做神经网络</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505021027758.png" alt="4层神经网络结构"></p>
<p>通过堆叠 4 个全连接层，可以获得层数为 4 的神经网络，由于每层均为全连接层， 称为<strong>全连接网络</strong>。其中第 1~3 个全连接层在网络中间，称之为<strong>隐藏层</strong>1、 2、3，最后一个全连接层的输出作为网络的输出，称为<strong>输出层</strong></p>
<p>在设计全连接网络时，网络的结构配置等超参数可以按着经验法则自由设置，只需要遵循少量的约束即可。例如：</p>
<ul>
<li>隐藏层 1 的输入节点数需和数据的实际特征长度匹配</li>
<li>每层的输入层节点数与上一层输出节点数匹配</li>
<li>输出层的激活函数和节点数需要根据任务的具体设定进行设计。</li>
</ul>
<p>总的来说，神经网络模型的结构设计自由度较大，至于与哪一组超参数是最优的，这需要很多的领域经验知识和大量的实验尝试</p>
<h4 id="1-张量方式实现-2">1.张量方式实现</h4>
<p>对于多层神经网络，以上图4层网络结构为例， 需要分别定义各层的权值矩阵𝑾和偏置向量𝒃，且每层的参数只能用于对应的层，不能混淆使用。在计算时，只需要按照网络层的顺序，将上一层的输出作为当前层的输入即可。最后一层是否需要添加激活函数通常视具体的任务而定</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 隐藏层 1 张量</span></span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">784</span>, <span class="number">256</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">b1 = tf.Variable(tf.zeros([<span class="number">256</span>]))</span><br><span class="line"><span class="comment"># 隐藏层 2 张量</span></span><br><span class="line">w2 = tf.Variable(tf.random.truncated_normal([<span class="number">256</span>, <span class="number">128</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">128</span>]))</span><br><span class="line"><span class="comment"># 隐藏层 3 张量</span></span><br><span class="line">w3 = tf.Variable(tf.random.truncated_normal([<span class="number">128</span>, <span class="number">64</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">b3 = tf.Variable(tf.zeros([<span class="number">64</span>]))</span><br><span class="line"><span class="comment"># 输出层张量</span></span><br><span class="line">w4 = tf.Variable(tf.random.truncated_normal([<span class="number">64</span>, <span class="number">10</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">b4 = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape: <span class="comment"># 梯度记录器</span></span><br><span class="line">    <span class="comment"># x: [b, 28*28]</span></span><br><span class="line">    <span class="comment"># 隐藏层 1 前向计算， [b, 28*28] =&gt; [b, 256]</span></span><br><span class="line">    h1 = x@w1 + tf.broadcast_to(b1, [x.shape[<span class="number">0</span>], <span class="number">256</span>])</span><br><span class="line">    h1 = tf.nn.relu(h1)</span><br><span class="line">    <span class="comment"># 隐藏层 2 前向计算， [b, 256] =&gt; [b, 128]</span></span><br><span class="line">    h2 = h1@w2 + b2</span><br><span class="line">    h2 = tf.nn.relu(h2)</span><br><span class="line">    <span class="comment"># 隐藏层 3 前向计算， [b, 128] =&gt; [b, 64]</span></span><br><span class="line">    h3 = h2@w3 + b3</span><br><span class="line">    h3 = tf.nn.relu(h3)</span><br><span class="line">    <span class="comment"># 输出层前向计算， [b, 64] =&gt; [b, 10]</span></span><br><span class="line">    h4 = h3@w4 + b4</span><br></pre></td></tr></table></figure>
<ul>
<li>在使用 TensorFlow 自动求导功能计算梯度时，需要将前向计算过程放置在<code>tf.GradientTape()</code>环境中，从而利用 GradientTape 对象的 gradient()方法自动求解参数的梯度，并利用 optimizers 对象更新参数</li>
</ul>
<h4 id="2-层方式实现-2">2.层方式实现</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入常用网络层 layers</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,Sequential</span><br><span class="line">fc1 = layers.Dense(<span class="number">256</span>, activation=tf.nn.relu) <span class="comment"># 隐藏层 1</span></span><br><span class="line">fc2 = layers.Dense(<span class="number">128</span>, activation=tf.nn.relu) <span class="comment"># 隐藏层 2</span></span><br><span class="line">fc3 = layers.Dense(<span class="number">64</span>, activation=tf.nn.relu) <span class="comment"># 隐藏层 3</span></span><br><span class="line">fc4 = layers.Dense(<span class="number">10</span>, activation=<span class="literal">None</span>) <span class="comment"># 输出层</span></span><br><span class="line"><span class="comment"># 前向计算</span></span><br><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">h1 = fc1(x) <span class="comment"># 通过隐藏层 1 得到输出</span></span><br><span class="line">h2 = fc2(h1) <span class="comment"># 通过隐藏层 2 得到输出</span></span><br><span class="line">h3 = fc3(h2) <span class="comment"># 通过隐藏层 3 得到输出</span></span><br><span class="line">h4 = fc4(h3) <span class="comment"># 通过输出层得到网络输出</span></span><br></pre></td></tr></table></figure>
<p>对于这种数据<strong>依次向前传播</strong>的网络， 也可以通过<code>Sequential</code>容器封装成一个网络大类对象，调用大类的前向计算函数一次即可完成所有层的前向计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 Sequential 容器</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,Sequential</span><br><span class="line"><span class="comment"># 通过 Sequential 容器封装为一个网络类</span></span><br><span class="line">model = Sequential([</span><br><span class="line">    layers.Dense(<span class="number">256</span>, activation=tf.nn.relu) , <span class="comment"># 创建隐藏层 1</span></span><br><span class="line">    layers.Dense(<span class="number">128</span>, activation=tf.nn.relu) , <span class="comment"># 创建隐藏层 2</span></span><br><span class="line">    layers.Dense(<span class="number">64</span>, activation=tf.nn.relu) , <span class="comment"># 创建隐藏层 3</span></span><br><span class="line">    layers.Dense(<span class="number">10</span>, activation=<span class="literal">None</span>) , <span class="comment"># 创建输出层</span></span><br><span class="line">])</span><br><span class="line"><span class="comment"># 前向计算得到输出</span></span><br><span class="line">out = model(x)</span><br></pre></td></tr></table></figure>
<h4 id="3-优化目标">3.优化目标</h4>
<p>我们把神经网络从输入到输出的计算过程叫做<strong>前向传播</strong>(Forward Propagation)或<strong>前向计算</strong>。神经网络的前向传播过程，也是数据张量(Tensor)从第一层流动(Flow)至输出层的过程，即从输入数据开始，途径每个隐藏层，直至得到输出并计算误差，这也是 TensorFlow框架名字由来</p>
<p>前向传播的最后一步就是完成误差的计算<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="script">L</mi><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msub><mi>𝑓</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">ℒ=g(𝑓_\theta(x),y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7em;vertical-align:0em;"></span><span class="mord"><span class="mord mathscr" style="margin-right:0.19189em;">L</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></p>
<ul>
<li>其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑓</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">𝑓_\theta(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span>代表了利用𝜃参数化的神经网络模型</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span>称之为误差函数，用来描述当前网络的预测值<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑓</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">𝑓_\theta(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>与真实标签𝒚之间的差距度量， 比如常用的均方差误差函数</li>
<li>ℒ称为网络的误差(Error，或损失 Loss)，一般为标量</li>
</ul>
<p>我们希望通过在训练集<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi mathvariant="double-struck">D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbb{D}^{train}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">D</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span></span></span></span></span>上面学习到一组参数𝜃使得训练的误差ℒ最小：</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mo>∗</mo></msup><mo>=</mo><munder><munder><mrow><mi>a</mi><mi>r</mi><mi>g</mi><mtext> </mtext><mi>m</mi><mi>i</mi><mi>n</mi></mrow><mo stretchy="true">⏟</mo></munder><mi>θ</mi></munder><mi>g</mi><mo stretchy="false">(</mo><msub><mi>𝑓</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>x</mi><mo>∈</mo><msup><mi mathvariant="double-struck">D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\theta^*=\underbrace{arg\ min}_{\theta}g(𝑓_\theta(x),y),x\in \mathbb{D}^{train}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.278548em;vertical-align:-1.528548em;"></span><span class="mord munder"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6595199999999999em;"><span style="top:-1.471452em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord munder"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.65952em;"><span class="svg-align" style="top:-2.15756em;"><span class="pstrut" style="height:3em;"></span><span class="stretchy" style="height:0.548em;min-width:1.6em;"><span class="brace-left" style="height:0.548em;"><svg width="400em" height="0.548em" viewbox="0 0 400000 548" preserveaspectratio="xMinYMin slice"><path d="M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13
 35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688
 0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7
-331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"/></svg></span><span class="brace-center" style="height:0.548em;"><svg width="400em" height="0.548em" viewbox="0 0 400000 548" preserveaspectratio="xMidYMin slice"><path d="M199572 214
c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14
 53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3
 11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0
-5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"/></svg></span><span class="brace-right" style="height:0.548em;"><svg width="400em" height="0.548em" viewbox="0 0 400000 548" preserveaspectratio="xMaxYMin slice"><path d="M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3
 28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237
-174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"/></svg></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mspace"> </span><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8424400000000001em;"><span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.528548em;"><span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">D</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>上述的最小化优化问题一般采用误差反向传播(Backward Propagation，简称 BP)算法来求解网络参数𝜃的梯度信息，并利用梯度下降(Gradient Descent，简称 GD)算法迭代更新参数：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mo mathvariant="normal">′</mo></msup><mo>=</mo><mi>θ</mi><mo>−</mo><mi>η</mi><mo>⋅</mo><msub><mo>▽</mo><mi>θ</mi></msub><mi mathvariant="script">L</mi></mrow><annotation encoding="application/x-tex">\theta&#x27;=\theta-\eta\cdot\bigtriangledown_\thetaℒ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.751892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.63889em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">η</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8944399999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mbin">▽</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathscr" style="margin-right:0.19189em;">L</span></span></span></span></span>，其中𝜂为学习率</p>
<p>网络的参数量是衡量网络规模的重要指标。计算全连接层的参数量方法：</p>
<p>考虑权值矩阵𝑾，偏置向量𝒃，输入特征长度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{in}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>， 输出特征长度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的网络层， 𝑾的参数量为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>⋅</mo><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{in}\cdot d_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，再加上偏置𝒃的参数， 总参数量为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>⋅</mo><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>+</mo><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{in}\cdot d_{out}+d_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。对于多层的全连接神经网络，总参数量的计算应累加所有全连接层的总参数量</p>
<h3 id="4、激活函数">4、激活函数</h3>
<p>激活函数都是平滑可导的，适合于梯度下降算法</p>
<h4 id="1-Sigmoid">1.Sigmoid</h4>
<p>Sigmoid 函数也叫 Logistic 函数，定义为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>≐</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">Sigmoid(x)\doteq\frac{1}{1+e^{-x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">m</span><span class="mord mathdefault">o</span><span class="mord mathdefault">i</span><span class="mord mathdefault">d</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≐</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2484389999999999em;vertical-align:-0.403331em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7026642857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.403331em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<p>它的一个优良特性就是能够把𝑥∈𝑅的输入“压缩” 到𝑥∈(0,1)区间，这个区间的数值在机器学习常用来表示以下意义：</p>
<ul>
<li>概率分布 (0,1)区间的输出和概率的分布范围[0,1]契合，可以通过 Sigmoid 函数将输出转译为<strong>概率输出</strong></li>
<li>信号强度：一般可以将 0~1 理解为某种信号的强度（如像素的颜色强度， 1 代表当前通道颜色最强， 0 代表当前通道无颜色；抑或代表门控值(Gate)的强度， 1 代表当前门控全部开放， 0 代表门控关闭）</li>
</ul>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505030303847.png" alt="Sigmoid函数曲线"></p>
<p>可以通过<code>tf.nn.sigmoid</code>实现 Sigmoid 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.linspace(-<span class="number">6.</span>,<span class="number">6.</span>,<span class="number">10</span>) <span class="comment"># 构造-6~6 的输入向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.nn.sigmoid(x) <span class="comment"># 通过 Sigmoid 函数</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">7</span>, shape=(<span class="number">10</span>,), dtype=float32, numpy=</span><br><span class="line">array([<span class="number">0.00247264</span>, <span class="number">0.00931597</span>, <span class="number">0.03444517</span>, <span class="number">0.11920291</span>, <span class="number">0.33924365</span>, <span class="number">0.6607564</span> , <span class="number">0.8807971</span> , <span class="number">0.96555483</span>, <span class="number">0.99068403</span>, <span class="number">0.9975274</span> ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h4 id="2-ReLU">2.ReLU</h4>
<p>在 ReLU(REctified Linear Unit， 修正线性单元)激活函数提出之前， Sigmoid 函数通常<br>
是神经网络的激活函数首选。</p>
<p>Sigmoid 函数在输入值较大或较小时容易出现梯度值接近于 0 的现象，称为<strong>梯度弥散</strong>现象。出现梯度弥散现象时， 网络参数长时间得不到更新，导致训练不收敛或停滞不动的现象发生， 较深层次的网络模型中更容易出现梯度弥散现象</p>
<p>ReLU定义为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>≐</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">ReLU(x)\doteq max(0,x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault">e</span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≐</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>，函数曲线如下：</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505030816893.png" alt="ReLU函数曲线"></p>
<p>可以通过<code>tf.nn.relu</code>实现 ReLU 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.linspace(-<span class="number">6.</span>,<span class="number">6.</span>,<span class="number">10</span>) <span class="comment"># 构造-6~6 的输入向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.nn.relu(x) <span class="comment"># 通过 ReLU 激活函数</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">11</span>, shape=(<span class="number">10</span>,), dtype=float32, numpy=</span><br><span class="line">array([<span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.666667</span>, <span class="number">2.</span> , <span class="number">3.333334</span>, <span class="number">4.666667</span>, <span class="number">6.</span> ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<p>除了可以使用函数式接口 tf.nn.relu 实现 ReLU 函数外，还可以像 Dense 层一样将ReLU 函数作为一个网络层添加到网络中，对应的类为 <code>layers.ReLU()</code>类。一般来说，激活<br>
函数类并不是主要的网络运算层，不计入网络的层数</p>
<p>ReLU 函数有着优良的梯度特性，在大量的深度学习应用中被验证非常有效，是应用最广泛的激活函数之一</p>
<h4 id="3-LeakyReLU">3.LeakyReLU</h4>
<p>ReLU 函数在𝑥 &lt; 0时导数值恒为 0，也可能会造成梯度弥散现象，为了克服这个问<br>
题， LeakyReLU 函数被提出：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mi>e</mi><mi>a</mi><mi>k</mi><mi>y</mi><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo>≐</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>x</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo>⩾</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>p</mi><mi>x</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo>&lt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">LeakyReLU\doteq \left\{\begin{matrix}
x &amp; x\geqslant 0\\ 
px &amp; x&lt;0
\end{matrix}\right.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">L</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault">e</span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≐</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">{</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="mord mathdefault">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel amsrm">⩾</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<p>其中𝑝为用户自行设置的某较小数值的超参数，如 0.02 等。当𝑝 = 0时， LeayReLU 函数退化为 ReLU 函数；当𝑝 ≠ 0时， 𝑥 &lt; 0处能够获得较小的导数值𝑝，从而避免出现梯度弥散现象</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505031651370.png" alt="LeakyReLU函数曲线"></p>
<p>可以通过<code>tf.nn.leaky_relu</code>实现 LeakyReLU 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.linspace(-<span class="number">6.</span>,<span class="number">6.</span>,<span class="number">10</span>) <span class="comment"># 构造-6~6 的输入向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.nn.leaky_relu(x, alpha=<span class="number">0.1</span>) <span class="comment"># 通过 LeakyReLU 激活函数</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">13</span>, shape=(<span class="number">10</span>,), dtype=float32, numpy=</span><br><span class="line">array([-<span class="number">0.6</span> , -<span class="number">0.46666667</span>, -<span class="number">0.33333334</span>, -<span class="number">0.2</span> , -<span class="number">0.06666666</span>, <span class="number">0.666667</span> , <span class="number">2.</span> , <span class="number">3.333334</span> , <span class="number">4.666667</span> , <span class="number">6.</span> ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>alpha 参数代表𝑝</li>
</ul>
<p>tf.nn.leaky_relu 对应的类为 <code>layers.LeakyReLU</code>，可以通过LeakyReLU(alpha)创建LeakyReLU 网络层，并设置𝑝参数，像 Dense 层一样将 LeakyReLU层放置在网络的合适位置</p>
<h4 id="4-Tanh">4.Tanh</h4>
<p>Tanh 函数能够将𝑥 ∈ 𝑅的输入“压缩” 到(-1,1)区间，定义为：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>≐</mo><mfrac><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac><mo>=</mo><mn>2</mn><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo stretchy="false">(</mo><mn>2</mn><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">tanh(x)\doteq\frac{e^x-e^{-x}}{e^x+e^{-x}}=2sigmoid(2x)-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≐</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.3906960000000002em;vertical-align:-0.403331em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.987365em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.5935428571428571em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7026642857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7385428571428572em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8476642857142858em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.403331em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">2</span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">m</span><span class="mord mathdefault">o</span><span class="mord mathdefault">i</span><span class="mord mathdefault">d</span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>（tanh 激活函数可通过 Sigmoid 函数缩放平移后实现）</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505032201454.png" alt="tanh函数曲线"></p>
<p>可以通过 tf.nn.tanh 实现 tanh 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.linspace(-<span class="number">6.</span>,<span class="number">6.</span>,<span class="number">10</span>) <span class="comment"># 构造-6~6 的输入向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.nn.tanh(x) <span class="comment"># 通过 tanh 激活函数</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">15</span>, shape=(<span class="number">10</span>,), dtype=float32, numpy=</span><br><span class="line">array([-<span class="number">0.9999877</span> , -<span class="number">0.99982315</span>, -<span class="number">0.997458</span> , -<span class="number">0.9640276</span> , -<span class="number">0.58278286</span>, <span class="number">0.5827831</span> , <span class="number">0.9640276</span> , <span class="number">0.997458</span> , <span class="number">0.99982315</span>, <span class="number">0.9999877</span> ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h3 id="5、输出层设计">5、输出层设计</h3>
<p>常见的几种输出类型包括：</p>
<ul>
<li>𝑜𝑖 ∈ <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>𝑅</mi><mi>𝑑</mi></msup></mrow><annotation encoding="application/x-tex">𝑅^𝑑</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span> 输出属于整个实数空间，或者某段普通的实数空间，比如函数值趋势的预测，年龄的预测问题等</li>
<li>𝑜𝑖 ∈ [0,1] 输出值特别地落在[0, 1]的区间， 如图片生成，图片像素值一般用[0, 1]区间的值表示；或者二分类问题的概率，如硬币正反面的概率预测问题</li>
<li>𝑜𝑖 ∈ [0,  1],   𝑖 𝑜𝑖 = 1 输出值落在[0, 1]的区间， 并且所有输出值之和为 1， 常见的如多分类问题，如 MNIST 手写数字图片识别，图片属于 10 个类别的概率之和应为 1</li>
<li>𝑜𝑖 ∈ [-1,  1] 输出值在[-1, 1]之间</li>
</ul>
<h4 id="1-普通实数空间">1.普通实数空间</h4>
<p>该类问题较普遍，输出层可以不加激活函数。</p>
<p>误差的计算直接基于最后一层的输出𝒐和真实值𝒚进行计算， 如采用均方差误差函数度量输出值𝒐与真实值𝒚之间的距离：ℒ = 𝑔(𝒐, 𝒚)，其中𝑔代表了某个具体的误差计算函数，例如 MSE 等</p>
<h4 id="2-0-1-区间">2.[0,1]区间</h4>
<p>为了让像素的值范围映射到[0,1]的有效实数空间，需要在输出层后添加某个合适的激活函数𝜎，其中 Sigmoid 函数刚好具有此功能</p>
<p>对于二分类问题，如硬币的正反面的预测， 输出层可以<strong>只设置一个节点</strong>，表示某个事件 A 发生的概率𝑃(A|𝒙)， 𝒙为网络输入。假设网络的输出标量𝑜表示正面事件出现的概率，则反面事件出现的概率即为1 - 𝑜，网络结构如下所示</p>
<ul>
<li>𝑃(正面|𝒙) = 𝑜</li>
<li>𝑃(反面|𝒙) = 1 - 𝑜</li>
</ul>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505180332392.png" alt="单输出节点的二分类网络"></p>
<ul>
<li>输出层的净活性值𝑧后添加 Sigmoid 函数即可将输出转译为概率值</li>
</ul>
<h4 id="3-0-1-区间，和为1">3.[0,1]区间，和为1</h4>
<p>输出值𝑜𝑖 ∈ [0,1]， 且所有输出值之和为 1，这种设定以多分类问题最为常见</p>
<p>可以通过在输出层添加 Softmax 函数实现：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>Z</mi><mi>i</mi></msub></msup><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></msubsup><msup><mi>e</mi><msub><mi>Z</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">Softmax(z_i)=\frac{e^{Z_i}}{\sum^{d_{out}}_{j=1}e^{Z_j}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.8939049999999997em;vertical-align:-0.85654em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0373649999999999em;"><span style="top:-2.4662800000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0338857142857143em;"><span style="top:-2.177714285714286em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.0378571428571433em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3447999999999998em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.61508em;"></span><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.27027999999999996em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.46117142857142857em;"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.19516666666666668em;"></span><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9595285714285715em;"><span style="top:-2.9714357142857146em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:-0.07153em;margin-right:0.1em;"><span class="pstrut" style="height:2.65952em;"></span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5091600000000001em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9190928571428572em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:-0.07153em;margin-right:0.1em;"><span class="pstrut" style="height:2.65952em;"></span><span class="mord mathdefault mtight">i</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.31472em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.85654em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<ul>
<li>Softmax 函数不仅可以将输出值映射到[0,1]区间，还满足所有的输出值之和为 1 的特性</li>
<li>通过 Softmax函数可以将输出层的输出转译为类别概率，在分类问题中使用的非常频繁</li>
</ul>
<p>可以通过 tf.nn.softmax 实现 Softmax 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = tf.constant([<span class="number">2.</span>,<span class="number">1.</span>,<span class="number">0.1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.nn.softmax(z) <span class="comment"># 通过 Softmax 函数</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">19</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">0.6590012</span>, <span class="number">0.242433</span> , <span class="number">0.0985659</span>], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<p>与 Dense 层类似， Softmax 函数也可以作为网络层类使用， 通过类<code>layers.Softmax(axis=-1)</code>可以方便添加 Softmax 层，其中 axis 参数指定需要进行计算的维度</p>
<p>在 Softmax 函数的数值计算过程中，容易因输入值偏大发生<strong>数值溢出</strong>现象；在计算交叉熵时，也会出现数值溢出的问题。为了数值计算的稳定性， TensorFlow 中提供了一个统一的接口，将 Softmax 与交叉熵损失函数同时实现，同时也处理了数值不稳定的异常，一般推荐使用这些接口函数，避免分开使用 Softmax 函数与交叉熵损失函数。函数式接口为<code>tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False)</code></p>
<ul>
<li>y_true：One-hot 编码后的真实标签</li>
<li>y_pred：网络的预测值
<ul>
<li>当 from_logits 设置为 True 时，y_pred 表示须为未经过 Softmax 函数的变量 z</li>
<li>当 from_logits 设置为 False 时， y_pred 表示为经过 Softmax 函数的输出</li>
</ul>
</li>
<li>为了数值计算稳定性，一般设置 from_logits 为 True（此时<code>tf.keras.losses.categorical_crossentropy</code>将在内部进行 Softmax 函数计算，所以不需要在模型中显式调用 Softmax 函数）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = tf.random.normal([<span class="number">2</span>,<span class="number">10</span>]) <span class="comment"># 构造输出层的输出</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_onehot = tf.constant([<span class="number">1</span>,<span class="number">3</span>]) <span class="comment"># 构造真实值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_onehot = tf.one_hot(y_onehot, depth=<span class="number">10</span>) <span class="comment"># one-hot 编码</span></span><br><span class="line"><span class="comment"># 输出层未使用 Softmax 函数，故 from_logits 设置为 True</span></span><br><span class="line"><span class="comment"># 这样 categorical_crossentropy 函数在计算损失函数前，会先内部调用 Softmax 函数</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = keras.losses.categorical_crossentropy(y_onehot,z,from_logits=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = tf.reduce_mean(loss) <span class="comment"># 计算平均交叉熵损失</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">210</span>, shape=(), dtype=float32, numpy= <span class="number">2.4201946</span>&gt;</span><br></pre></td></tr></table></figure>
<p>除了函数式接口， 也可以利用<code>losses.CategoricalCrossentropy(from_logits)</code>类方式同时实现 Softmax 与交叉熵损失函数的计算， from_logits 参数的设置方式相同</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 Softmax 与交叉熵计算类，输出层的输出 z 未使用 softmax</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>criteon = keras.losses.CategoricalCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = criteon(y_onehot,z) <span class="comment"># 计算损失</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">258</span>, shape=(), dtype=float32, numpy= <span class="number">2.4201946</span>&gt;</span><br></pre></td></tr></table></figure>
<h4 id="4-1-1-区间">4.[-1,1]区间</h4>
<p>使用 tanh 激活函数即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.linspace(-<span class="number">6.</span>,<span class="number">6.</span>,<span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.tanh(x) <span class="comment"># tanh 激活函数</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">264</span>, shape=(<span class="number">10</span>,), dtype=float32, numpy=</span><br><span class="line">array([-<span class="number">0.9999877</span> , -<span class="number">0.99982315</span>, -<span class="number">0.997458</span> , -<span class="number">0.9640276</span> , -<span class="number">0.58278286</span>, <span class="number">0.5827831</span> , <span class="number">0.9640276</span> , <span class="number">0.997458</span> , <span class="number">0.99982315</span>, <span class="number">0.9999877</span> ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h3 id="6、误差计算">6、误差计算</h3>
<p>常见的误差函数有均方差、 交叉熵、 KL 散度、 Hinge Loss 函数等，其中均方差函数和交叉熵函数在深度学习中比较常见，均方差函数主要用于回归问题，交叉熵函数主要用于分类问题</p>
<h4 id="1-均方差误差函数">1.均方差误差函数</h4>
<p>均方差(Mean Squared Error，简称 MSE)误差函数：把输出向量和真实向量映射到笛卡尔坐标系的两个点上，通过计算这两个点之间的欧式距离(准确地说是欧式距离的平方)来衡量两个向量之间的差距：</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mi>S</mi><mi>E</mi><mo stretchy="false">(</mo><mi>y</mi><mo separator="true">,</mo><mi>o</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></msubsup><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>o</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">MSE(y,o)=\frac{1}{d_{out}}\sum^{d_{out}}_{i=1}(y_i-o_i)^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">o</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.434108em;vertical-align:-0.44509999999999994em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.989008em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></p>
<ul>
<li>MSE 误差函数的值总是大于等于 0</li>
<li>当 MSE 函数达到最小值 0 时， 输出等于真实标签，此时神经网络的参数达到最优状态</li>
</ul>
<p>可以通过函数方式或层方式实现 MSE 误差计算。通过函数式调用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>o = tf.random.normal([<span class="number">2</span>,<span class="number">10</span>]) <span class="comment"># 构造网络输出</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_onehot = tf.constant([<span class="number">1</span>,<span class="number">3</span>]) <span class="comment"># 构造真实值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_onehot = tf.one_hot(y_onehot, depth=<span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = keras.losses.MSE(y_onehot, o) <span class="comment"># 计算均方差</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">27</span>, shape=(<span class="number">2</span>,), dtype=float32, numpy=array([<span class="number">0.779179</span> ,</span><br><span class="line"><span class="number">1.6585705</span>], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>MSE 函数返回的是每个样本的均方差</p>
</li>
<li>
<p>可以在样本维度上再次平均来获得平均样本的均方差</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = tf.reduce_mean(loss) <span class="comment"># 计算 batch 均方差</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">30</span>, shape=(), dtype=float32, numpy=<span class="number">1.2188747</span>&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>通过层方式实现，对应的类为<code>keras.losses.MeanSquaredError()</code>，和其他层的类一样，调用<code>__call__</code>函数即可完成前向计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 MSE 类</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>criteon = keras.losses.MeanSquaredError()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = criteon(y_onehot,o) <span class="comment"># 计算 batch 均方差</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">54</span>, shape=(), dtype=float32, numpy=<span class="number">1.2188747</span>&gt;</span><br></pre></td></tr></table></figure>
<h4 id="2-交叉熵误差函数">2.交叉熵误差函数</h4>
<p>熵，在信息论中，用来衡量信息的不确定度。 熵在信息学科中也叫信息熵，或者香农熵。熵越大，代表不确定性越大，信息量也就越大。 某个分布𝑃(𝑖)的熵定义为：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><msub><mo>∑</mo><mi>i</mi></msub><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mi>l</mi><mi>o</mi><msub><mi>g</mi><mn>2</mn></msub><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(P)=-\sum_iP(i)log_2P(i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0497100000000001em;vertical-align:-0.29971000000000003em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mclose">)</span></span></span></span></p>
<ul>
<li>对于确定的分布，熵取得最小值0，不确定性为0</li>
<li>由于𝑃(𝑖) ∈ [0,1], <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mn>2</mn></msub><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">log_2P(i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mclose">)</span></span></span></span> ≤ 0，因此熵𝐻(𝑃)总是大于等于 0</li>
<li>在 TensorFlow 中，可以用<code>tf.math.log</code>来计算熵</li>
</ul>
<p>交叉熵(Cross Entropy)的定义：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>+</mo><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(p||q)=H(p)+D_{KL}(p||q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span></span></span></span></p>
<ul>
<li>其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">D_{KL}(p||q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span></span></span></span>为𝑝与𝑞的 KL 散度(Kullback-Leibler Divergence)：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo>∑</mo><mi>i</mi></msub><mi>p</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><mrow><mi>q</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">D_{KL}(p||q)=\sum_ip(i)log(\frac{p(i)}{q(i)})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span>
<ul>
<li>KL 散度是用于衡量 2 个分布之间距离的指标：𝑝 = 𝑞时，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">D_{KL}(p||q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span></span></span></span>取得最小值 0， 𝑝与𝑞之间的差距越大，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">D_{KL}(p||q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span></span></span></span>也越大</li>
<li>交叉熵和 KL 散度都不是对称的：
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo><mi mathvariant="normal">≠</mi><mi>H</mi><mo stretchy="false">(</mo><mi>q</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(p||q)\neq H(q||p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="rlap"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="inner"><span class="mrel"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault">p</span><span class="mclose">)</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo><mi mathvariant="normal">≠</mi><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>q</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">D_{KL}(p||q)\neq D_{KL}(q||p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="rlap"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="inner"><span class="mrel"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault">p</span><span class="mclose">)</span></span></span></span></li>
</ul>
</li>
</ul>
</li>
<li>当分类问题中 y 的编码分布𝑝采用 One-hot 编码𝒚时： 𝐻(𝑝) = 0</li>
</ul>
<p>分类问题中交叉熵的计算表达式：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><msub><mi>o</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">H(p||q)=D_{KL}(p||q)=-logo_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord">−</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<ul>
<li>𝑖为 One-hot 编码中为 1 的索引号，也是当前输入的真实类别</li>
<li>ℒ只与真实类别𝑖上的概率𝑜𝑖有关， 对应概率𝑜𝑖越大， 𝐻(𝑝||𝑞)越小</li>
<li>当对应类别上的概率为 1 时， 交叉熵𝐻(𝑝||𝑞)取得最小值 0，此时网络输出𝒐与真实标签𝒚完全一致，神经网络取得最优状态</li>
<li>最小化交叉熵损失函数的过程也是最大化正确类别的预测概率的过程</li>
</ul>
<h3 id="7、神经网络类型">7、神经网络类型</h3>
<p>全连接层是神经网络最基本的网络类型，优点是全连接层前向计算流程相对简单，梯度求导也较简单，缺点是在处理较大特征长度的数据时， 全连接层的参数量往往较大</p>
<h4 id="1-卷积神经网络">1.卷积神经网络</h4>
<p>全连接层在处理高维度的图片、 视频数据时往往出现网络参数量巨大，训练非常困难。通过利用局部相关性和权值共享的思想，Yann Lecun在1986年提出了卷积神经网络(Convolutional Neural Network， 简称 CNN)</p>
<p>其中比较流行的模型：</p>
<ul>
<li>用于图片分类的 AlexNet、 VGG、 GoogLeNet、 ResNet、 DenseNet 等</li>
<li>用于目标识别的 RCNN、 Fast RCNN、 Faster RCNN、 Mask RCNN、 YOLO、 SSD 等</li>
</ul>
<h4 id="2-循环神经网络">2.循环神经网络</h4>
<p>除了具有空间结构的图片、 视频等数据外，序列信号也是非常常见的一种数据类型，其中一个最具代表性的序列信号就是文本数据。卷积神经网络由于缺乏 Memory 机制和处理不定长序列信号的能力，并不擅长序列信号的任务。循环神经网络(Recurrent Neural Network)，被证明非常擅长处理序列信号</p>
<p>1997年提出的LSTM网络，作为 RNN 的变种，较好地克服了 RNN 缺乏长期记忆、 不擅长处理长序列的问题，在自然语言处理中得到了广泛的应用。基于LSTM 模型， Google 提出了用于机器翻译的 Seq2Seq 模型，并成功商用于谷歌神经机器翻译系统(GNMT)</p>
<p>其他的 RNN 变种还有 GRU、 双向 RNN 等</p>
<h4 id="3-注意力（机制）网络">3.注意力（机制）网络</h4>
<p>RNN 并不是自然语言处理的最终解决方案，近年来随着注意力机制(Attention Mechanism)的提出，克服了 RNN 训练不稳定、 难以并行化等缺陷，在自然语言处理和图片生成等领域中逐渐崭露头角</p>
<p>2017 年， Google 提出了第一个利用纯注意力机制实现的网络模型Transformer，随后基于 Transformer 模型相继提出了一系列的用于机器翻译的注意力网络模型，如 GPT、 BERT、 GPT-2 等</p>
<p>在其它领域，基于注意力机制，尤其是自注意力(SelfAttention)机制构建的网络也取得了不错的效果，比如基于自注意力机制的 BigGAN 模型等</p>
<h4 id="4-图卷积神经网络">4.图卷积神经网络</h4>
<p>图片、 文本等数据具有规则的空间、时间结构，称为 Euclidean Data(欧几里德数据)。卷积神经网络和循环神经网络被证明非常擅长处理这种类型的数据。而像类似于社交网络、 通信网络、 蛋白质分子结构等一系列的不规则空间拓扑结构的数据， 它们显得力不从心。 2016 年，基于前人在一阶近似的谱卷积算法上提出了图卷积网络(Graph Convolution Network， GCN)模型。 GCN 算法实现简单，从空间一阶邻居信息聚合的角度也能直观地理解，在半监督任务上取得了不错效果。随后，一系列的网络模型相继被提出，如 GAT， EdgeConv， DeepGCN 等</p>
<h2 id="四、Keras高层接口">四、Keras高层接口</h2>
<p>Keras 与 tf.keras 的区别与联系：</p>
<ul>
<li>Keras 可以理解为一套搭建与训练神经网络的高层 API 协议， Keras 本身已经实现了此协议， 安装标准的 Keras 库就可以方便地调用TensorFlow、 CNTK 等后端完成加速计算</li>
<li>在 TensorFlow 中，也实现了一套 Keras 协议，即 tf.keras，它与 TensorFlow 深度融合，且只能基于 TensorFlow 后端运算， 并对TensorFlow 的支持更完美。 对于使用 TensorFlow 的开发者来说， tf.keras 可以理解为一个普通的子模块，与其他子模块，如 tf.math， tf.data 等并没有什么差别。 下文如无特别说明，Keras <strong>均指代 tf.keras</strong>，而不是标准的 Keras 库</li>
</ul>
<h3 id="1、常见功能模块">1、常见功能模块</h3>
<p>Keras 提供了一系列高层的神经网络相关类和函数，如经典数据集加载函数（进阶操作–&gt;经典数据集加载 章节中讲到过）、 网络层类、 模型容器、 损失函数类、 优化器类、 经典模型类等</p>
<h4 id="1-常见网络层类">1.常见网络层类</h4>
<p>对于常见的神经网络层，可以使用张量方式的底层接口函数来实现，这些接口函数一般在<code>tf.nn</code>模块中。</p>
<p>对于常见的<strong>网络层</strong>，我们一般直接使用层方式来完成模型的搭建，在<code>tf.keras.layers</code>命名空间(下文使用 <code>layers</code> 指代 <code>tf.keras.layers</code>)中提供了大量常见网络层的类，如全连接层、 激活函数层、 池化层、 卷积层、 循环神经网络层等。对于这些网络层类，只需要在创建时指定网络层的相关参数， 并调用<code>__call__</code>方法即可完成前向计算。在调用<code>__call__</code>方法时， Keras 会自动调用每个层的前向传播逻辑，这些逻辑一般实现在类的call 函数中。</p>
<p>以 Softmax 层为例， 它既可以使用<code>tf.nn.softmax</code>函数在前向传播逻辑中完成Softmax运算， 也可以通过<code>layers.Softmax(axis)</code>类搭建Softmax网络层，其中<code>axis</code>参数指定进行softmax 运算的维度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 导入 keras 模型，不能使用 import keras，它导入的是标准的 Keras 库</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers <span class="comment"># 导入常见网络层类</span></span><br><span class="line"><span class="comment"># 创建 Softmax 层，并调用__call__方法完成前向计算</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.constant([<span class="number">2.</span>,<span class="number">1.</span>,<span class="number">0.1</span>]) <span class="comment"># 创建输入张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>layer = layers.Softmax(axis=-<span class="number">1</span>) <span class="comment"># 创建 Softmax 层</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = layer(x) <span class="comment"># 调用 softmax 前向计算，输出为 out</span></span><br><span class="line"><span class="comment"># 经过 Softmax 网络层后， 得到概率分布 out 为：</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">2</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">0.6590012</span>, <span class="number">0.242433</span> , <span class="number">0.0985659</span>], dtype=float32)&gt;</span><br><span class="line"><span class="comment"># 当然，也可以直接通过 tf.nn.softmax()函数完成计算，代码如下：</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = tf.nn.softmax(x) <span class="comment"># 调用 softmax 函数完成前向计算</span></span><br></pre></td></tr></table></figure>
<h4 id="2-网络容器">2.网络容器</h4>
<p>当网络层数变得较深时，手动调用每一层的类实例完成前向传播运算这部分代码显得非常臃肿。可以通过 Keras 提供的网络容器 <code>Sequential</code> 将多个网络层封装成一个大网络模型，只需要调用网络模型的实例一次即可完成数据从第一层到最末层的顺序传播运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 Sequential 容器</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, Sequential</span><br><span class="line">network = Sequential([ <span class="comment"># 封装为一个网络</span></span><br><span class="line">    layers.Dense(<span class="number">3</span>, activation=<span class="literal">None</span>), <span class="comment"># 全连接层，此处不使用激活函数</span></span><br><span class="line">    layers.ReLU(),<span class="comment">#激活函数层</span></span><br><span class="line">    layers.Dense(<span class="number">2</span>, activation=<span class="literal">None</span>), <span class="comment"># 全连接层，此处不使用激活函数</span></span><br><span class="line">    layers.ReLU() <span class="comment">#激活函数层</span></span><br><span class="line">])</span><br><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line">out = network(x) <span class="comment"># 输入从第一层开始， 逐层传播至输出层，并返回输出层的输出</span></span><br></pre></td></tr></table></figure>
<p>Sequential 容器也可以通过<code>add()</code>方法继续追加新的网络层， 实现动态创建网络的功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">layers_num = <span class="number">2</span> <span class="comment"># 堆叠 2 次</span></span><br><span class="line">network = Sequential([]) <span class="comment"># 先创建空的网络容器</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(layers_num):</span><br><span class="line">    network.add(layers.Dense(<span class="number">3</span>)) <span class="comment"># 添加全连接层</span></span><br><span class="line">    network.add(layers.ReLU())<span class="comment"># 添加激活函数层</span></span><br><span class="line">network.build(input_shape=(<span class="number">4</span>, <span class="number">4</span>)) <span class="comment"># 创建网络参数</span></span><br><span class="line">network.summary()</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>在完成网络创建时， 网络层类并没有创建内部权值张量等成员变量，此时通过调用类的<code>build</code>方法并指定输入大小，即可自动创建所有层的内部张量</p>
</li>
<li>
<p>通过Sequential容量封装多个网络层时，每层的参数列表将会自动并入Sequential容器的参数列表中，不需要人为合并网络参数列表</p>
</li>
<li>
<p>Sequential 对象的<code>trainable_variables</code>和<code>variables</code>包含了所有层的待优化张量列表和全部张量列表</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印网络的待优化参数名与 shape</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> p <span class="keyword">in</span> network.trainable_variables:</span><br><span class="line"><span class="meta">... </span>    print(p.name, p.shape) <span class="comment"># 参数名和形状</span></span><br><span class="line">...</span><br><span class="line">dense/kernel:<span class="number">0</span> (<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">dense/bias:<span class="number">0</span> (<span class="number">3</span>,)</span><br><span class="line">dense_1/kernel:<span class="number">0</span> (<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">dense_1/bias:<span class="number">0</span> (<span class="number">3</span>,)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>通过<code>summary()</code>函数可以方便打印出网络结构和参数量，输出：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Model: <span class="string">&quot;sequential&quot;</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                 Output Shape              Param <span class="comment">#</span></span><br><span class="line">=================================================================</span><br><span class="line">dense (Dense)                multiple                  <span class="number">15</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">re_lu (ReLU)                 multiple                  <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              multiple                  <span class="number">12</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">re_lu_1 (ReLU)               multiple                  <span class="number">0</span></span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">27</span></span><br><span class="line">Trainable params: <span class="number">27</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>
<ul>
<li><code>Layer</code>：每层的名字，由 TensorFlow 内部维护，与 Python 的对象名并不一样</li>
<li><code>Param#</code>：层的参数个数</li>
<li><code>Total params</code>：统计出了总的参数量</li>
<li><code>Trainable params</code>：总的待优化参数量</li>
<li><code>Non-trainable params</code>：总的不需要优化的参数量</li>
</ul>
</li>
</ul>
<h3 id="2、模型装配、训练与测试">2、模型装配、训练与测试</h3>
<p>在训练网络时，一般的流程是通过前向计算获得网络的输出值， 再通过损失函数计算网络误差，然后通过自动求导工具计算梯度并更新，同时间隔性地测试网络的性能。对于这种常用的训练逻辑，可以直接通过 Keras 提供的模型装配与训练等高层接口实现</p>
<h4 id="1-模型装配">1.模型装配</h4>
<p>在 Keras 中，有 2 个比较特殊的类：</p>
<ul>
<li>keras.Model类：<strong>网络的母类</strong>，除了具有Layer类的功能，还添加了保存模型、加载模型、 训练与测试模型等便捷功能。<u>Sequential也是Model的子类</u>（具有Model类的所有功能）</li>
<li>keras.layers.Layer类：<strong>网络层的母类</strong>，定义了网络层的一些常见功能，如添加权值、 管理权值列表等</li>
</ul>
<p>下面介绍 Model 及其子类的模型装配与训练功能</p>
<p>创建网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 5 层的全连接网络</span></span><br><span class="line">network = Sequential([layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                      layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                      layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                      layers.Dense(<span class="number">32</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                      layers.Dense(<span class="number">10</span>)])</span><br><span class="line">network.build(input_shape=(<span class="number">4</span>, <span class="number">28</span>*<span class="number">28</span>))  <span class="comment"># 构建网络（此时网络中的权重w与偏差b才会生成）</span></span><br><span class="line">network.summary()  <span class="comment"># 打印出网络结构和参数量</span></span><br></pre></td></tr></table></figure>
<p>通过<code>compile</code>函数指定网络使用的优化器对象、 损失函数类型， 评价指标等设定，这一步称为<strong>装配</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入优化器，损失函数模块</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers,losses</span><br><span class="line"><span class="comment"># 模型装配</span></span><br><span class="line"><span class="comment"># 采用 Adam 优化器，学习率为 0.01;采用交叉熵损失函数，包含 Softmax</span></span><br><span class="line">network.<span class="built_in">compile</span>(optimizer=optimizers.Adam(lr=<span class="number">0.01</span>),</span><br><span class="line">                loss=losses.CategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">                metrics=[<span class="string">&#x27;accuracy&#x27;</span>] <span class="comment"># 设置测量指标为准确率</span></span><br><span class="line">               )</span><br></pre></td></tr></table></figure>
<h4 id="2-模型训练">2.模型训练</h4>
<p>模型装配完成后，可通过<code>fit()</code>函数送入待训练的数据集和验证用的数据集，实现网络的训练与验证，这一步称为<strong>模型训练</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定训练集为 train_db，验证集为 val_db,训练 5 个 epochs，每 2 个 epoch 验证一次</span></span><br><span class="line"><span class="comment"># 返回训练轨迹信息保存在 history 对象中</span></span><br><span class="line">history = network.fit(train_db, epochs=<span class="number">5</span>, validation_data=val_db,</span><br><span class="line">                      validation_freq=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>train_db：tf.data.Dataset对象，也可以传入Numpy Array类型的数据</p>
</li>
<li>
<p>epochs：指定训练迭代的Epoch数量</p>
</li>
<li>
<p>validation_data：指定用于验证(测试)的数据集</p>
</li>
<li>
<p>validation_freq：验证的频率</p>
</li>
<li>
<p>history：训练过程的数据记录，其中<code>history.history</code>为字典对象，包含了训练过程中的loss、测量指标等记录项</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>history.history <span class="comment"># 打印训练记录</span></span><br><span class="line"><span class="comment"># 历史训练准确率</span></span><br><span class="line">&#123;<span class="string">&#x27;accuracy&#x27;</span>: [<span class="number">0.00011666667</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.010666667</span>, <span class="number">0.02495</span>],</span><br><span class="line"> <span class="string">&#x27;loss&#x27;</span>: [<span class="number">2465719710540.5845</span>, <span class="comment"># 历史训练误差</span></span><br><span class="line">          <span class="number">78167808898516.03</span>,</span><br><span class="line">          <span class="number">404488834518159.6</span>,</span><br><span class="line">          <span class="number">1049151145155144.4</span>,</span><br><span class="line">          <span class="number">1969370184858451.0</span>],</span><br><span class="line"> <span class="string">&#x27;val_accuracy&#x27;</span>: [<span class="number">0.0</span>, <span class="number">0.0</span>], <span class="comment"># 历史验证准确率</span></span><br><span class="line"> <span class="comment"># 历史验证误差</span></span><br><span class="line"> <span class="string">&#x27;val_loss&#x27;</span>: [<span class="number">197178788071657.3</span>, <span class="number">1506234836955706.2</span>]&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><code>fit()</code>函数的运行代表了网络的训练过程，会消耗相当的训练时间，并在训练结束后才返回</p>
<h4 id="3-模型测试">3.模型测试</h4>
<p>关于验证和测试的区别，会在过拟合一章详细阐述，此处可以将验证和测试理解为模型评估的一种方式</p>
<p>通过<code>Model.predict(x)</code>方法即可完成模型的<strong>预测</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载一个 batch 的测试数据</span></span><br><span class="line">x,y = <span class="built_in">next</span>(<span class="built_in">iter</span>(db_test))</span><br><span class="line">print(<span class="string">&#x27;predict x:&#x27;</span>, x.shape) <span class="comment"># 打印当前 batch 的形状</span></span><br><span class="line">out = network.predict(x) <span class="comment"># 模型预测，预测结果保存在 out 中</span></span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure>
<p>如果只是简单的测试模型的<strong>性能</strong>，可以通过<code>Model.evaluate(db)</code>循环测试完db数据集上所有样本，并打印出性能指标：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">network.evaluate(db_test) <span class="comment"># 模型测试，测试在 db_test 上的性能表现</span></span><br></pre></td></tr></table></figure>
<h3 id="3、模型保存与加载">3、模型保存与加载</h3>
<h4 id="1-张量方式">1.张量方式</h4>
<p>网络的状态主要体现在网络的结构以及网络层内部张量数据上，因此在<u>拥有网络结构源文件</u>的条件下，直接保存网络张量参数到文件系统上是最轻量级的一种方式</p>
<p>通过调用<code>Model.save_weights(path)</code>方法，可将当前的网络参数保存到path文件上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">network.save_weights(<span class="string">&#x27;weights.ckpt&#x27;</span>) <span class="comment"># 保存模型的所有张量数据</span></span><br></pre></td></tr></table></figure>
<p>在需要的时候，先创建好网络对象，然后调用网络对象的<code>load_weights(path)</code>方法即可将指定的模型文件中保存的张量数值写入到当前网络参数中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新创建相同的网络结构</span></span><br><span class="line">new_network = Sequential([layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                          layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                          layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                          layers.Dense(<span class="number">32</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                          layers.Dense(<span class="number">10</span>)])</span><br><span class="line">new_network.<span class="built_in">compile</span>(optimizer=optimizers.Adam(lr=<span class="number">0.01</span>),</span><br><span class="line">                    loss=tf.losses.CategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">                    metrics=[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">                   )</span><br><span class="line"><span class="comment"># 从参数文件中读取数据并写入当前网络</span></span><br><span class="line">new_network.load_weights(<span class="string">&#x27;weights.ckpt&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;loaded weights!&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="2-网络方式">2.网络方式</h4>
<p>通过<code>Model.save(path)</code>函数可以将模型的<strong>结构</strong>以及模型的<strong>参数</strong>保存到path文件上，在<u>不需要网络源文件</u>的条件下，通过<code>keras.models.load_model(path)</code>即可恢复网络结构和网络参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构与模型参数到文件</span></span><br><span class="line">network.save(<span class="string">&#x27;model.h5&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;saved total model.&#x27;</span>)</span><br><span class="line"><span class="keyword">del</span> network <span class="comment"># 删除网络对象</span></span><br><span class="line"><span class="comment"># 从文件恢复网络结构与网络参数</span></span><br><span class="line">network = keras.models.load_model(<span class="string">&#x27;model.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="3-SavedModel方式">3.SavedModel方式</h4>
<p>当需要将模型部署到其他平台时，采用SavedModel方式更具有平台无关性。</p>
<p>通过<code>tf.saved_model.save(network, path)</code>即可将模型以SavedModel方式保存到path<strong>目录</strong>中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构与模型参数到文件</span></span><br><span class="line">tf.saved_model.save(network, <span class="string">&#x27;model-savedmodel&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;saving savedmodel.&#x27;</span>)</span><br><span class="line"><span class="keyword">del</span> network <span class="comment"># 删除网络对象</span></span><br></pre></td></tr></table></figure>
<p>通过<code>tf.saved_model.load</code>函数即可恢复出模型对象，我们在恢复出模型实例后，完成测试准确率的计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从文件恢复网络结构与网络参数</span></span><br><span class="line">network = tf.saved_model.load(<span class="string">&#x27;model-savedmodel&#x27;</span>)</span><br><span class="line"><span class="comment"># 准确率计量器</span></span><br><span class="line">acc_meter = metrics.CategoricalAccuracy()</span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> ds_val: <span class="comment"># 遍历测试集</span></span><br><span class="line">    pred = network(x) <span class="comment"># 前向计算</span></span><br><span class="line">    acc_meter.update_state(y_true=y, y_pred=pred) <span class="comment"># 更新准确率统计</span></span><br><span class="line"><span class="comment"># 打印准确率</span></span><br><span class="line">print(<span class="string">&quot;Test Accuracy:%f&quot;</span> % acc_meter.result())</span><br></pre></td></tr></table></figure>
<h3 id="4、自定义网络">4、自定义网络</h3>
<p>对于需要创建自定义逻辑的网络层，可以通过<strong>自定义类</strong>来实现</p>
<ul>
<li>在创建自定义<strong>网络层类</strong>时，需要继承自 layers.Layer 基类</li>
<li>创建自定义的<strong>网络类</strong>时，需要继承自 keras.Model 基类</li>
</ul>
<h4 id="1-自定义网络层">1.自定义网络层</h4>
<p>对于自定义的网络层， 需要实现初始化<code>__init__</code>方法和前向传播逻辑<code>call</code>方法</p>
<p>以某个具体的自定义网络层为例，假设需要一个没有偏置向量的全连接层，同时固定激活函数为 ReLU 函数：</p>
<ol>
<li>
<p>首先创建类，并继承自 Layer 基类。创建初始化方法，并调用母类的初始化函数。由于是全连接层， 因此需要设置两个参数：输入特征的长度inp_dim和输出特征的长度outp_dim，并通过<code>self.add_variable(name, shape)</code>创建 shape 大小，名字为 name 的张量𝑾，并设置为需要优化</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDense</span>(<span class="params">layers.Layer</span>):</span></span><br><span class="line">    <span class="comment"># 自定义网络层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, inp_dim, outp_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyDense, self).__init__() <span class="comment"># 调用母类的初始化函数</span></span><br><span class="line">        <span class="comment"># 创建权值张量并添加到类管理列表中，设置为需要优化</span></span><br><span class="line">        self.kernel = self.add_variable(<span class="string">&#x27;w&#x27;</span>, [inp_dim, outp_dim],</span><br><span class="line">                                        trainable=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 此外，通过 tf.Variable 创建的类成员也会自动加入类参数列表</span></span><br><span class="line">        <span class="comment"># self.kernel = tf.Variable(tf.random.normal([inp_dim, outp_dim]),</span></span><br><span class="line">        <span class="comment">#                           trainable=False)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>self.add_variable会返回张量𝑾的 Python 引用</li>
<li>变量名 name 由TensorFlow 内部维护， 使用的比较少</li>
<li>trainable：创建的张量是否需要优化</li>
</ul>
</li>
<li>
<p>设计自定义类的前向运算逻辑。对于本例，只需要完成𝑶 = 𝑿@𝑾矩阵运算，并通过固定的ReLU激活函数即可</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># 实现自定义类的前向计算逻辑</span></span><br><span class="line">    <span class="comment"># X@W</span></span><br><span class="line">    out = inputs @ self.kernel</span><br><span class="line">    <span class="comment"># 执行激活函数运算</span></span><br><span class="line">    out = tf.nn.relu(out)</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<ul>
<li>inputs：输入， 由用户在调用时传入</li>
<li>training：用于指定模型的状态：
<ul>
<li>True：执行训练模式</li>
<li>False：执行测试模式，默认参数为 None，即测试模式</li>
</ul>
</li>
<li>由于全连接层的训练模式和测试模式逻辑一致，此处不需要额外处理。对于部份测试模式和训练模式不一致的网络层，需要根据 training 参数来设计需要执行的逻辑</li>
</ul>
</li>
</ol>
<p>此时可以实例化 MyDense 类，并查看其参数列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = MyDense(<span class="number">4</span>,<span class="number">3</span>) <span class="comment"># 创建输入为 4，输出为 3 节点的自定义层</span></span><br><span class="line">net.variables,net.trainable_variables <span class="comment"># 查看自定义层的参数列表</span></span><br></pre></td></tr></table></figure>
<h4 id="2-自定义网络">2.自定义网络</h4>
<p>自定义网络类可以和其他标准类一样，通过 Sequential 容器方便地封装成一个网络模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">network = Sequential([MyDense(<span class="number">784</span>, <span class="number">256</span>), <span class="comment"># 使用自定义的层</span></span><br><span class="line">                      MyDense(<span class="number">256</span>, <span class="number">128</span>),</span><br><span class="line">                      MyDense(<span class="number">128</span>, <span class="number">64</span>),</span><br><span class="line">                      MyDense(<span class="number">64</span>, <span class="number">32</span>),</span><br><span class="line">                      MyDense(<span class="number">32</span>, <span class="number">10</span>)])</span><br><span class="line">network.build(input_shape=(<span class="literal">None</span>, <span class="number">28</span>*<span class="number">28</span>))</span><br><span class="line">network.summary()</span><br></pre></td></tr></table></figure>
<p>通过堆叠自定义网络层类，可以实现 5 层的全连接层网络，每层全连接层无偏置张量，同时激活函数固定地使用 ReLU 函数</p>
<hr>
<p>Sequential 容器适合于数据按序从第一层传播到第二层，再从第二层传播到第三层，以此规律传播的网络模型。对于复杂的网络结构，例如第三层的输入不仅是第二层的输出，还有第一层的输出，此时使用自定义网络更加灵活：</p>
<ol>
<li>
<p>创建自定义网络类，首先创建类， 并继承自 Model 基类，分别创建对应的网络层对象：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="comment"># 自定义网络类，继承自 Model 基类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__() <span class="comment"># 调用母类的初始化函数</span></span><br><span class="line">        <span class="comment"># 完成网络内需要的网络层的创建工作</span></span><br><span class="line">        self.fc1 = MyDense(<span class="number">28</span>*<span class="number">28</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc2 = MyDense(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc3 = MyDense(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.fc4 = MyDense(<span class="number">64</span>, <span class="number">32</span>)</span><br><span class="line">        self.fc5 = MyDense(<span class="number">32</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>实现自定义网络的前向运算逻辑：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># 自定义前向运算逻辑</span></span><br><span class="line">    x = self.fc1(inputs)</span><br><span class="line">    x = self.fc2(x)</span><br><span class="line">    x = self.fc3(x)</span><br><span class="line">    x = self.fc4(x)</span><br><span class="line">    x = self.fc5(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="5、模型乐园">5、模型乐园</h3>
<p>对于常用的网络模型，如 ResNet、 VGG 等，不需要手动创建网络，可以直接从<code>keras.applications</code>子模块中通过一行代码即可创建并使用这些经典模型，同时还可以通过设置 weights 参数加载预训练的网络参数</p>
<h4 id="1-加载模型">1.加载模型</h4>
<p>暂无，等待施工</p>
<h3 id="6、测量工具">6、测量工具</h3>
<p>Keras 提供了一些常用的测量工具，位于<code>keras.metrics</code>模块中，专门用于统计训练过程中常用的指标数据。Keras 的测量工具的使用方法一般有 4 个主要步骤：</p>
<ul>
<li>新建测量器</li>
<li>写入数据</li>
<li>读取统计数据</li>
<li>清零测量器</li>
</ul>
<h4 id="1-新建测量器">1.新建测量器</h4>
<p>在<code>keras.metrics</code>模块中，提供了较多的常用测量器类， 如统计平均值的 <code>Mean</code> 类，统计准确率的 <code>Accuracy</code> 类，统计余弦相似度的 <code>CosineSimilarity</code> 类等</p>
<p>例子：统计误差值</p>
<p>在前向运算时，会得到每一个 Batch 的平均误差，但是希望统计每个Step的平均误差，因此选择使用Mean测量器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建平均测量器，适合 Loss 数据</span></span><br><span class="line">loss_meter = metrics.Mean()</span><br></pre></td></tr></table></figure>
<h4 id="2-写入数据">2.写入数据</h4>
<p>通过测量器的<code>update_state</code>函数可以写入新的数据，测量器会根据自身逻辑记录并处理采样数据。例如，在每个 Step 结束时采集一次 loss 值，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 记录采样的数据，通过 float()函数将张量转换为普通数值</span></span><br><span class="line">loss_meter.update_state(<span class="built_in">float</span>(loss))</span><br></pre></td></tr></table></figure>
<ul>
<li>放置在每个 Batch 运算结束后即可， 测量器会自动根据采样的数据来统计平均值</li>
</ul>
<h4 id="3-读取统计信息">3.读取统计信息</h4>
<p>在采样多次数据后，可以选择在需要的地方调用测量器的<code>result()</code>函数，来获取统计值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印统计期间的平均 loss</span></span><br><span class="line">print(step, <span class="string">&#x27;loss:&#x27;</span>, loss_meter.result())</span><br></pre></td></tr></table></figure>
<h4 id="4-清除状态">4.清除状态</h4>
<p>测量器会统计<strong>所有历史记录</strong>的数据，因此在启动新一轮统计时，有必要清除历史状态。通过 <code>reset_states()</code> 即可实现清除状态功能</p>
<p>例如，在每次读取完平均误差后， 清零统计信息，以便下一轮统计的开始</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># 打印统计的平均 loss</span></span><br><span class="line">    print(step, <span class="string">&#x27;loss:&#x27;</span>, loss_meter.result())</span><br><span class="line">    loss_meter.reset_states() <span class="comment"># 打印完后， 清零测量器</span></span><br></pre></td></tr></table></figure>
<h4 id="5-准确率统计实战">5.准确率统计实战</h4>
<p>新建准确率测量器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">acc_meter = metrics.Accuracy() <span class="comment"># 创建准确率测量器</span></span><br></pre></td></tr></table></figure>
<p>Accuracy 类的 update_state 函数的参数为预测值和真实值，而不是当前 Batch 的准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [b, 784] =&gt; [b, 10]，网络输出值</span></span><br><span class="line">out = network(x)</span><br><span class="line"><span class="comment"># [b, 10] =&gt; [b]，经过 argmax 后计算预测值</span></span><br><span class="line">pred = tf.argmax(out, axis=<span class="number">1</span>)</span><br><span class="line">pred = tf.cast(pred, dtype=tf.int32)</span><br><span class="line"><span class="comment"># 根据预测值与真实值写入测量器</span></span><br><span class="line">acc_meter.update_state(y, pred)</span><br></pre></td></tr></table></figure>
<p>在统计完测试集所有 Batch 的预测值后， 打印统计的平均准确率， 并清零测量器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取统计结果</span></span><br><span class="line">print(step, <span class="string">&#x27;Evaluate Acc:&#x27;</span>, acc_meter.result().numpy())</span><br><span class="line">acc_meter.reset_states() <span class="comment"># 清零测量器</span></span><br></pre></td></tr></table></figure>
<h3 id="7、可视化">7、可视化</h3>
<p>TensorFlow 提供了一个可视化工具<code>TensorBoard</code>。原理是通过将监控数据写入到文件系统， 并利用Web后端监控对应的文件目录， 从而可以允许用户从远程查看网络的监控数据。</p>
<p>TensorBoard 的使用需要模型代码和浏览器相互配合。在使用 TensorBoard 之前，需要安装 TensorBoard 库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 TensorBoard</span></span><br><span class="line">pip install tensorboard</span><br></pre></td></tr></table></figure>
<h4 id="1-模型端">1.模型端</h4>
<p>创建写入监控数据的<code>Summary</code>类， 并在需要的时候写入监控数据即可。</p>
<p>首先通过<code>tf.summary.create_file_writer</code>创建监控对象类实例，并指定监控数据的写入<u>目录</u></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建监控类，监控数据将写入 log_dir 目录</span></span><br><span class="line">summary_writer = tf.summary.create_file_writer(log_dir)</span><br></pre></td></tr></table></figure>
<hr>
<p>例子：监控误差数据和可视化数据</p>
<p>在前向计算完成后，对于误差这种<strong>标量</strong>数据， 我们通过<code>tf.summary.scalar</code>函数记录监控数据，并指定时间戳 step 参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> summary_writer.as_default(): <span class="comment"># 写入环境</span></span><br><span class="line">    <span class="comment"># 当前时间戳 step 上的数据为 loss，写入到名为 train-loss 数据库中</span></span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;train-loss&#x27;</span>, <span class="built_in">float</span>(loss), step=step)</span><br></pre></td></tr></table></figure>
<ul>
<li>step：类似于每个数据对应的时间刻度信息（可以理解为数据曲线的x坐标），不宜重复。</li>
<li>每类数据通过字符串名字来区分，同类的数据需要写入相同名字的数据库中</li>
</ul>
<p>对于<strong>图片</strong>类型的数据， 可以通过<code>tf.summary.image</code>函数监控多个图片的张量数据，并通过设置<code>max_outputs</code>参数来选择最多显示的图片数量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> summary_writer.as_default():<span class="comment"># 写入环境</span></span><br><span class="line">    <span class="comment"># 写入测试准确率</span></span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;test-acc&#x27;</span>, <span class="built_in">float</span>(total_correct/total), step=step)</span><br><span class="line">    <span class="comment"># 可视化测试用的图片，设置最多可视化 9 张图片</span></span><br><span class="line">    tf.summary.image(<span class="string">&quot;val-onebyone-images:&quot;</span>, val_images, max_outputs=<span class="number">9</span>, step=step)</span><br></pre></td></tr></table></figure>
<h4 id="2-浏览器端">2.浏览器端</h4>
<p>打开 Web 后端：通过在 cmd 终端运行<code>tensorboard --logdir path</code>指定 Web 后端监控的文件目录 path， 即可打开 Web 后端监控进程</p>
<p>之后打开浏览器，输入网址<code>http://localhost:6006</code>(也可通过 IP 地址远程访问， 具体端口号可能会变动，可查看命令提示) 即可监控网络训练进度</p>
<p>除了监控标量数据和图片数据外， TensorBoard 还支持通过<code>tf.summary.histogram</code>查看张量数据的直方图分布，以及通过<code>tf.summary.text</code>打印文本信息等功能</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> summary_writer.as_default():</span><br><span class="line">    <span class="comment"># 当前时间戳 step 上的数据为 loss，写入到 ID 位 train-loss 对象中</span></span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;train-loss&#x27;</span>, <span class="built_in">float</span>(loss), step=step)</span><br><span class="line">    <span class="comment"># 可视化真实标签的直方图分布</span></span><br><span class="line">    tf.summary.histogram(<span class="string">&#x27;y-hist&#x27;</span>,y, step=step)</span><br><span class="line">    <span class="comment"># 查看文本信息</span></span><br><span class="line">    tf.summary.text(<span class="string">&#x27;loss-text&#x27;</span>,<span class="built_in">str</span>(<span class="built_in">float</span>(loss)))</span><br></pre></td></tr></table></figure>
<p>实际上，除了 TensorBoard 外，Visdom 工具（具有更加丰富的可视化手段和实时性）同样可以方便可视化数据。Visdom 可以直接接受PyTorch 的张量类型的数据，但不能直接接受 TensorFlow 的张量类型数据，需要转换为Numpy 数组</p>
<h2 id="五、过拟合">五、过拟合</h2>
<p>机器学习的主要目的是从训练集上学习到数据的真实模型， 从而能够在未见过的测试集上也能够表现良好，我们把这种能力叫做<strong>泛化能力</strong></p>
<h3 id="1、模型的容量">1、模型的容量</h3>
<p>通俗地讲，模型的容量或表达能力，是指模型拟合复杂函数的能力。一种体现模型容量的指标为模型的假设空间(Hypothesis Space)大小，即模型可以表示的函数集的大小。</p>
<p>假设空间越大越完备， 从假设空间中搜索出逼近真实模型的函数也就越有可能； 反之，如果假设空间非常受限，就很难从中找到逼近真实模型的函数</p>
<p>实际上，较大的假设空间并不一定能搜索出更好的函数模型。 由于观测误差的存在，较大的假设空间中可能包含了大量表达能力过强的函数， 能够将训练样本的观测误差也学习进来，从而伤害了模型的泛化能力。挑选合适容量的学习模型是一个很大的难题</p>
<h3 id="2、过拟合与欠拟合">2、过拟合与欠拟合</h3>
<ul>
<li>过拟合(Overfitting)：当模型的容量过大时，网络模型除了学习到训练集数据的模态之外，还把额外的观测误差也学习进来，导致学习的模型在训练集上面表现较好，但是在未见的样本上表现不佳，也就是模型泛化能力偏弱</li>
<li>欠拟合(Underfitting)：当模型的容量过小时，模型不能够很好地学习到训练集数据的模态，导致训练集上表现不佳，同时在未见的样本上表现也不佳</li>
</ul>
<p>那么如何去选择模型的容量？</p>
<ul>
<li>
<p>统计学习理论中的 VC 维度(Vapnik-Chervonenkis 维度)是一个应用比较广泛的度量函数容量的方法。但是该方法却很少应用到深度学习中去，一部分原因是神经网络过于复杂，很难去确定网络结构背后的数学模型的 VC 维度</p>
</li>
<li>
<p>可以根据奥卡姆剃刀原理(Occam’s razor)来指导神经网络的设计和训练。即“切勿浪费较多东西，去做‘用较少的东西，同样可以做好的事情’。”。也就是说，如果两层的神经网络结构能够很好的表达真实模型，那么三层的神经网络也能够很好的表达，但是我们应该优先选择使用更简单的两层神经网络，因为它的参数量更少，更容易训练，也更容易通过较少的训练样本获得不错的泛化误差</p>
</li>
</ul>
<h4 id="1-欠拟合">1.欠拟合</h4>
<p>当我们发现当前的模型在训练集上出现：</p>
<ol>
<li>误差一直维持较高的状态，很难优化减少</li>
<li>在测试集上表现也不佳</li>
</ol>
<p>就应该考虑是否出现了欠拟合的现象</p>
<p>解决方法：</p>
<ul>
<li>增加神经网络的层数</li>
<li>增大中间维度的大小</li>
</ul>
<p>在实际使用过程中，更多的是出现过拟合现象</p>
<h4 id="2-过拟合">2.过拟合</h4>
<p>现代深度神经网络中过拟合现象非常容易出现，主要是因为：</p>
<ul>
<li>神经网络的表达能力非常强，</li>
<li>训练集样本数不够</li>
</ul>
<h3 id="3、数据集划分">3、数据集划分</h3>
<p>前面我们介绍了数据集需要划分为训练集(Train set)和测试集(Test set)，但是为了挑选模型超参数和检测过拟合现象，一般需要将原来的训练集再次切分为新的训练集和验证集(Validation set)，即数据集需要切分为：</p>
<ul>
<li>训练集：训练模型的参数</li>
<li>验证集：选择模型的超参数，提升模型的泛化能力</li>
<li>测试集：仅仅测试最后结果</li>
</ul>
<h4 id="1-验证集与超参数">1.验证集与超参数</h4>
<p>验证集：选择模型的超参数(模型选择， Model selection)，功能包括：</p>
<ul>
<li>根据验证集的性能表现来调整学习率、 权值衰减系数、 训练次数等</li>
<li>根据验证集的性能表现来重新调整网络拓扑结构</li>
<li>根据验证集的性能表现判断是否过拟合和欠拟合</li>
</ul>
<p>训练集、验证集和测试集可以按着自定义的比例来划分，比如常见的 60%-20%-20%的划分</p>
<p>验证集与测试集的区别：</p>
<ul>
<li>算法设计人员可以根据<strong>验证集</strong>的表现来调整模型的各种超参数的设置，提升模型的<strong>泛化能力</strong>（测试泛化性能）</li>
<li>测试集的表现不能用来反馈模型的调整，否则测试集将和验证集的功能重合， 因此在测试集上的性能表现将<strong>无法</strong>代表模型的泛化能力</li>
</ul>
<h4 id="2-提前停止">2.提前停止</h4>
<p>一般把对训练集中的一个<strong>Batch</strong>运算更新一次叫做一个<strong>Step</strong>， 对训练集的所有样本循环迭代一次叫做一个<strong>Epoch</strong>。验证集可以在数次 Step 或数次 Epoch 后使用，计算模型的验证性能（一般建议几个 Epoch 后进行一次验证运算）</p>
<ul>
<li>训练时，一般关注的指标有训练误差、 训练准确率等</li>
<li>验证时，也有验证误差和验证准确率等</li>
<li>测试时，也有测试误差和测试准确率等</li>
</ul>
<p>通过观测<u>训练准确率</u>和<u>验证准确率</u>可以大致推断模型是否出现过拟合和欠拟合</p>
<ul>
<li>过拟合：如果模型的训练误差较低，训练准确率较高，但是验证误差较高，验证准确率较低
<ul>
<li>解决方法：可以从新设计网络模型的容量，如降低网络的层数、降低网络的参数量、 添加正则化手段、 添加假设空间的约束等，使得模型的实际容量降低</li>
</ul>
</li>
<li>欠拟合：如果训练集和验证集上面的误差都较高，准确率较低
<ul>
<li>解决方法：尝试增大网络的容量，如加深网络的层数、 增加网络的参数量，尝试更复杂的网络结构</li>
</ul>
</li>
</ul>
<p>实际上， 由于网络的实际容量可以随着训练的进行发生改变，因此在相同的网络设定下，随着训练的进行， 可能观测到不同的过拟合、 欠拟合状况</p>
<ul>
<li>在训练的前期，随着训练的进行，模型的训练准确率和测试准确率都呈现增大的趋势，此时并没有出现过拟合现象</li>
<li>在训练后期，即使是相同网络结构下， 由于模型的实际容量发生改变，我们观察到了过拟合的现象，具体表现为<u>训练准确度继续改善</u>，但是泛化能力变弱(<u>测试准确率减低</u>)</li>
</ul>
<p>记录模型的验证准确率，并监控验证准确率的变化， 当发现验证准确率连续𝑛个 Epoch 没有下降时，可以预测可能已经达到了最适合的 Epoch 附近，从而<strong>提前终止</strong>训练</p>
<h3 id="4、模型设计">4、模型设计</h3>
<p>对于神经网络来说，网络的层数和参数量是网络容量很重要的参考指标</p>
<ul>
<li>减少网络的层数， 减少每层中网络参数量的规模， 可以有效降低网络的容量</li>
<li>如果发现模型欠拟合，需要增大网络的容量，可以通过增加层数，增大每层的参数量等方式实现</li>
</ul>
<h3 id="5、正则化">5、正则化</h3>
<p>通过设计不同层数、大小的网络模型可以为优化算法提供初始的函数假设空间，但是模型的实际容量可以随着网络参数的优化更新而产生变化。以多项式函数模型为例：</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><mi>x</mi><mo>+</mo><msub><mi>β</mi><mn>2</mn></msub><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><msub><mi>β</mi><mn>3</mn></msub><msup><mi>x</mi><mn>3</mn></msup><mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>β</mi><mi>n</mi></msub><msup><mi>x</mi><mi>n</mi></msup><mo>+</mo><mi>ε</mi></mrow><annotation encoding="application/x-tex">y=\beta_{0}+\beta_{1} x+\beta_{2} x^{2}+\beta_{3} x^{3}+\cdots+\beta_{n} x^{n}+\varepsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.008548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.008548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ε</span></span></span></span></p>
<p>上述模型的容量可以通过n简单衡量。在训练的过程中，如果网络参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>β</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>β</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">\beta_{k+1}, \cdots, \beta_{n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.902771em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>均为0， 那么网络的实际容量退化到k次多项式的函数容量。因此，通过限制网络参数的稀疏性， 可以来约束网络的实际容量。</p>
<p>这种约束一般通过在损失函数上添加额外的参数稀疏性惩罚项实现，在未加约束之前的优化目标是：</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>min</mi><mo>⁡</mo><mi mathvariant="script">L</mi><mrow><mo fence="true">(</mo><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>y</mi><mo fence="true">)</mo></mrow><mo separator="true">,</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∈</mo><msup><mi mathvariant="double-struck">D</mi><mtext>train </mtext></msup></mrow><annotation encoding="application/x-tex">\min \mathcal{L}\left(f_{\theta}(\boldsymbol{x}), y\right),(\boldsymbol{x}, y) \in \mathbb{D}^{\text {train }}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">min</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathcal">L</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.830502em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">D</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.830502em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">train </span></span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>对模型的参数添加额外的约束后，优化的目标变为</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>min</mi><mo>⁡</mo><mi mathvariant="script">L</mi><mrow><mo fence="true">(</mo><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>y</mi><mo fence="true">)</mo></mrow><mo>+</mo><mi>λ</mi><mo>⋅</mo><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∈</mo><msup><mi mathvariant="double-struck">D</mi><mtext>train </mtext></msup></mrow><annotation encoding="application/x-tex">\min \mathcal{L}\left(f_{\theta}(\boldsymbol{x}), y\right)+\lambda \cdot \Omega(\theta),(\boldsymbol{x}, y) \in \mathbb{D}^{\text {train }}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">min</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathcal">L</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Ω</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.830502em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">D</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.830502em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">train </span></span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\Omega(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Ω</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span> 表示对网络参数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span> 的稀疏性约束函数。一般地，参数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span> 的稀疏性约束通过约束参数 $ \theta $ 的 $ L $ 范数实现，即：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><msub><mi>θ</mi><mi>i</mi></msub></munder><msub><mrow><mo fence="true">∥</mo><msub><mi>θ</mi><mi>i</mi></msub><mo fence="true">∥</mo></mrow><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">\Omega(\theta)=\sum_{\theta_{i}}\left\|\theta_{i}\right\|_{l} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Ω</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.4522180000000002em;vertical-align:-1.402213em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000003em;"><span style="top:-1.847887em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.402213em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∥</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∥</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.18640799999999996em;"><span style="top:-2.4003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29969999999999997em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中 $ \left|\theta_{i}\right|<em>{l} $ 表示参数 $ \theta</em>{i} $ 的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 范数。</p>
<p>新的优化目标除了要最小化原来的损失函数 $ \mathcal{L}(\boldsymbol{x}, y) $ 之外，还需要约束网络参数的稀疏性 $ \Omega(\theta) $，优化算法会在降低 $ \mathcal{L}(\boldsymbol{x}, y) $ 的同时，尽可能地迫使网络参数 $ \theta_{i} $ 变得稀疏，它们之间的权重关系通过超参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span>来平衡。较大的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span>意味着网络的稀疏性更重要; 较小的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span>则意味着网络的训练误差更重要。通过选择合适的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span>超参数，可以获得较好的训练性能，同时保证网络的稀疏性，从而获得不错的泛化能力。</p>
<p>常用的正则化方式有 L0、L1、L2 正则化。</p>
<h4 id="L0正则化">L0正则化</h4>
<p>L0 正则化是指采用 $ \mathrm{L} 0 $ 范数作为稀疏性惩罚项 $ \Omega(\theta) $ 的正则化计算方式，即</p>
<p>$ \Omega(\theta)=\sum_{\theta_{i}}\left|\theta_{i}\right|_{0} $</p>
<p>其中 $ \mathrm{L} 0 $ 范数 $ \left|\theta_{i}\right|<em>{0} $ 定义为 $ \theta</em>{i} $ 中非零元素的个数。通过约束 $ \Sigma_{\theta_{i}}\left|\theta_{i}\right|<em>{0} $ 的大小可以迫使网络中的连接权值大部分为 0，从而降低网络的实际参数量和网络容量。但是由于 $ \mathrm{L} 0 $ 范数 $ \left|\theta</em>{i}\right|_{0} $ 并不可导，不能利用梯度下降算法进行优化，在神经网络中使用的并不多。</p>
<h4 id="L1正则化">L1正则化</h4>
<p>L1 正则化是指采用 $ \mathrm{L} 1 $ 范数作为稀疏性惩罚项 $ \Omega(\theta) $ 的正则化计算方式，即</p>
<p>$ \Omega(\theta)=\sum_{\theta_{i}}\left|\theta_{i}\right|_{1} $</p>
<p>其中 $ \mathrm{L} 1 $ 范数 $ \left|\theta_{i}\right|<em>{1} $ 定义为张量 $ \theta</em>{i} $ 中所有元素的绝对值之和。L1正则化也叫 Lasso Regularization，它是连续可导的，在神经网络中使用广泛</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建网络参数 w1,w2</span></span><br><span class="line">w1 = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line">w2 = tf.random.normal([<span class="number">4</span>,<span class="number">2</span>])</span><br><span class="line"><span class="comment"># 计算 L1 正则化项</span></span><br><span class="line">loss_reg = tf.reduce_sum(tf.math.<span class="built_in">abs</span>(w1)) + tf.reduce_sum(tf.math.<span class="built_in">abs</span>(w2))</span><br></pre></td></tr></table></figure>
<h4 id="L2正则化">L2正则化</h4>
<p>L2 正则化是指采用 $ \mathrm{L} 2 $ 范数作为稀疏性惩罚项 $ \Omega(\theta) $ 的正则化计算方式，即</p>
<p>$ \Omega(\theta)=\sum_{\theta_{i}}\left|\theta_{i}\right|_{2} $</p>
<p>其中 $ \mathrm{L} 2 $ 范数 $ \left|\theta_{i}\right|<em>{2} $ 定义为张量 $ \theta</em>{i} $ 中所有元素的平方和。L2正则化也叫 Ridge Regularization，它是连续可导的，在神经网络中使用广泛</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建网络参数 w1,w2</span></span><br><span class="line">w1 = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line">w2 = tf.random.normal([<span class="number">4</span>,<span class="number">2</span>])</span><br><span class="line"><span class="comment"># 计算 L2 正则化项</span></span><br><span class="line">loss_reg = tf.reduce_sum(tf.square(w1)) + tf.reduce_sum(tf.square(w2))</span><br></pre></td></tr></table></figure>
<h3 id="6、Dropout">6、Dropout</h3>
<p>Dropout 通过随机断开神经网络的连接，减少每次训练时实际参与计算的模型的参数量；但是在测试时， Dropout 会恢复所有的连接，保证模型测试时获得最好的性能</p>
<p>在 TensorFlow 中，可以通过<code>tf.nn.dropout(x, rate)</code>函数实现某条连接的 Dropout 功能，其中 rate 参数设置断开的概率值𝑝</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加 dropout 操作，断开概率为 0.5</span></span><br><span class="line">x = tf.nn.dropout(x, rate=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
<p>也可以将 Dropout 作为一个网络层使用， 在网络中间插入一个 Dropout 层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加 Dropout 层，断开概率为 0.5</span></span><br><span class="line">model.add(layers.Dropout(rate=<span class="number">0.5</span>))</span><br></pre></td></tr></table></figure>
<p>随着 Dropout 层的增加，网络模型训练时的实际容量减少，泛化能力变强</p>
<h3 id="7、数据增强">7、数据增强</h3>
<p>增加数据集规模是解决过拟合最重要的途径。在有限的数据集上，通过数据增强技术可以增加训练的样本数量，获得一定程度上的性能提升</p>
<p>数据增强(Data Augmentation)是指在维持样本标签不变的条件下，根据先验知识改变样本的特征， 使得新产生的样本也符合或者近似符合数据的真实分布</p>
<p>以图片数据为例。数据集中的图片大小往往是不一致的，为了方便神经网络处理，需要将图片缩放到某个固定的大小，如缩放后的固定224 × 224大小的图片。对于图中的人物图片， 根据先验知识，我们知道旋转、缩放、 平移、裁剪、改变视角、 遮挡某局部区域都不会改变图片的主体类别标签，因此针对图片数据，可以有多种数据增强方式</p>
<p>TensorFlow 中提供了常用图片的处理函数， 位于<code>tf.image</code>子模块中。通过<code>tf.image.resize</code>函数可以实现图片的缩放功能</p>
<p>将图片从文件系统读取进来后，即可进行图片数据增强操作。通过预处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params">x,y</span>):</span></span><br><span class="line"><span class="comment"># 预处理函数</span></span><br><span class="line"><span class="comment"># x: 图片的路径， y：图片的数字编码</span></span><br><span class="line">x = tf.io.read_file(x)</span><br><span class="line">x = tf.image.decode_jpeg(x, channels=<span class="number">3</span>) <span class="comment"># RGBA</span></span><br><span class="line"><span class="comment"># 图片缩放到 244x244 大小，这个大小根据网络设定自行调整</span></span><br><span class="line">x = tf.image.resize(x, [<span class="number">244</span>, <span class="number">244</span>])</span><br></pre></td></tr></table></figure>
<h4 id="1-旋转">1.旋转</h4>
<p>通过<code>tf.image.rot90(x, k=1)</code>可以实现图片按逆时针方式旋转 k 个 90 度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图片逆时针旋转 180 度</span></span><br><span class="line">x = tf.image.rot90(x,<span class="number">2</span>)  </span><br></pre></td></tr></table></figure>
<h4 id="2-翻转">2.翻转</h4>
<p>图片的翻转分为沿水平轴翻转和竖直轴翻转，可以通过<code>tf.image.random_flip_left_right</code>和<code>tf.image.random_flip_up_down</code>实现图片在水平方向和竖直方向的随机翻转操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机水平翻转</span></span><br><span class="line">x = tf.image.random_flip_left_right(x)</span><br><span class="line"><span class="comment"># 随机竖直翻转</span></span><br><span class="line">x = tf.image.random_flip_up_down(x)</span><br></pre></td></tr></table></figure>
<h4 id="3-裁剪">3.裁剪</h4>
<p>通过在原图的左右或者上下方向去掉部分边缘像素，可以保持图片主体不变，同时获得新的图片样本。在实际裁剪时，一般先将图片缩放到略大于网络输入尺寸的大小， 再裁剪到合适大小</p>
<p>如网络的输入大小为224 × 224，那么可以先通过 resize 函数将图片缩放到244 × 244大小，再随机裁剪到224 × 224大小：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图片先缩放到稍大尺寸</span></span><br><span class="line">x = tf.image.resize(x, [<span class="number">244</span>, <span class="number">244</span>])</span><br><span class="line"><span class="comment"># 再随机裁剪到合适尺寸</span></span><br><span class="line">x = tf.image.random_crop(x, [<span class="number">224</span>,<span class="number">224</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<h4 id="4-生成数据">4.生成数据</h4>
<p>通过生成模型在原有数据上进行训练， 学习到真实数据的分布，从而利用生成模型获得新的样本，这种方式也可以在一定程度上提升网络性能。 如通过条件生成对抗网络(Conditional GAN,简称 CGAN)可以生成带标签的样本数据</p>
<h4 id="5-其他方式">5.其他方式</h4>
<p>除了上述介绍的典型图片数据增强方式以外，可以根据先验知识，在不改变图片标签信息的条件下，任意变换图片数据，获得新的图片。如在原图上叠加高斯噪声、通过改变图片的观察视角后、在原图上随机遮挡部分区域等</p>
<h3 id="8、过拟合问题">8、过拟合问题</h3>
<h4 id="1-数据集构建">1.数据集构建</h4>
<p>我们使用的数据集样本特性向量长度为 2， 标签为 0 或 1，分别代表了两种类别。借助于<code>scikit-learn</code>库中提供的<code>make_moons</code>工具， 我们可以生成任意多数据的训练集。首先安装 scikit-learn 库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip 安装 scikit-learn 库</span></span><br><span class="line">pip install -U scikit-learn</span><br></pre></td></tr></table></figure>
<p>为了演示过拟合现象，采样1000个样本数据，同时添加标准差为 0.25 的高斯噪声数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入数据集生成工具</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"><span class="comment"># 从 moon 分布中随机采样 1000 个点，并切分为训练集-测试集</span></span><br><span class="line">X, y = make_moons(n_samples = N_SAMPLES, noise=<span class="number">0.25</span>, random_state=<span class="number">100</span>)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y,</span><br><span class="line">test_size = TEST_SIZE, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>
<p>编写make_plot函数，方便根据样本的坐标 X 和样本的标签 y 绘制出数据的分布图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_plot</span>(<span class="params">X, y, plot_name, file_name, XX=<span class="literal">None</span>, YY=<span class="literal">None</span>, preds=<span class="literal">None</span></span>):</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    <span class="comment"># sns.set_style(&quot;whitegrid&quot;)</span></span><br><span class="line">    axes = plt.gca()</span><br><span class="line">    axes.set_xlim([x_min,x_max])</span><br><span class="line">    axes.set_ylim([y_min,y_max])</span><br><span class="line">    axes.<span class="built_in">set</span>(xlabel=<span class="string">&quot;$x_1$&quot;</span>, ylabel=<span class="string">&quot;$x_2$&quot;</span>)</span><br><span class="line">    <span class="comment"># 根据网络输出绘制预测曲面</span></span><br><span class="line">    <span class="keyword">if</span>(XX <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> YY <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> preds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">        plt.contourf(XX, YY, preds.reshape(XX.shape), <span class="number">25</span>, alpha = <span class="number">0.08</span>,</span><br><span class="line">                     cmap=cm.Spectral)</span><br><span class="line">        plt.contour(XX, YY, preds.reshape(XX.shape), levels=[<span class="number">.5</span>],</span><br><span class="line">                    cmap=<span class="string">&quot;Greys&quot;</span>,</span><br><span class="line">                    vmin=<span class="number">0</span>, vmax=<span class="number">.6</span>)</span><br><span class="line">    <span class="comment"># 绘制正负样本</span></span><br><span class="line">    markers = [<span class="string">&#x27;o&#x27;</span> <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">else</span> <span class="string">&#x27;s&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> y.ravel()]</span><br><span class="line">    mscatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y.ravel(), s=<span class="number">20</span>,</span><br><span class="line">             cmap=plt.cm.Spectral, edgecolors=<span class="string">&#x27;none&#x27;</span>, m=markers)</span><br><span class="line">    <span class="comment"># 保存矢量图</span></span><br><span class="line">    plt.savefig(OUTPUT_DIR+<span class="string">&#x27;/&#x27;</span>+file_name)</span><br></pre></td></tr></table></figure>
<p>绘制出采样的 1000 个样本分布：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制数据集分布</span></span><br><span class="line">make_plot(X, y, <span class="literal">None</span>, <span class="string">&quot;dataset.svg&quot;</span>)  </span><br></pre></td></tr></table></figure>
<h4 id="2-网络层数的影响">2.网络层数的影响</h4>
<p>为了探讨不同的网络深度下的过拟合程度，我们共进行了 5 次训练实验。在𝑛 ∈ [0,4]时，构建网络层数为𝑛 + 2层的全连接层网络，并通过 Adam 优化器训练 500 个 Epoch，获得网络在训练集上的分隔曲线</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>): <span class="comment"># 构建 5 种不同层数的网络</span></span><br><span class="line">    model = Sequential()<span class="comment"># 创建容器</span></span><br><span class="line">    <span class="comment"># 创建第一层</span></span><br><span class="line">    model.add(Dense(<span class="number">8</span>, input_dim=<span class="number">2</span>,activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n): <span class="comment"># 添加 n 层，共 n+2 层</span></span><br><span class="line">        model.add(Dense(<span class="number">32</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)) <span class="comment"># 创建最末层</span></span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">                  metrics=[<span class="string">&#x27;accuracy&#x27;</span>]) <span class="comment"># 模型装配与训练</span></span><br><span class="line">    history = model.fit(X_train, y_train, epochs=N_EPOCHS, verbose=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 绘制不同层数的网络决策边界曲线</span></span><br><span class="line">    preds = model.predict_classes(np.c_[XX.ravel(), YY.ravel()])</span><br><span class="line">    title = <span class="string">&quot;网络层数(&#123;&#125;)&quot;</span>.<span class="built_in">format</span>(n)</span><br><span class="line">    file = <span class="string">&quot;网络容量%f.png&quot;</span>%(<span class="number">2</span>+n*<span class="number">1</span>)</span><br><span class="line">    make_plot(X_train, y_train, title, file, XX, YY, preds)</span><br></pre></td></tr></table></figure>
<h4 id="3-Dropout的影响">3.Dropout的影响</h4>
<p>待添加</p>
<h4 id="4-正则化的影响">4.正则化的影响</h4>
<p>待添加</p>
<h2 id="六、卷积神经网络">六、卷积神经网络</h2>
<h3 id="1、全连接层的问题">1、全连接层的问题</h3>
<p>全连接层较高的内存占用量严重限制了神经网络朝着更大规模、更深层数方向的发展</p>
<h4 id="1-局部相关性">1.局部相关性</h4>
<p>网络的每个输出节点都与所有的输入节点相连接， 用于提取所有输入节点的特征信息，这种稠密的连接方式是全连接层参数量大、 计算代价高的根本原因。 全连接层也称为稠密连接层(Dense Layer)</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200511214046062.png" alt="全连接示意图"></p>
<p>基于距离的重要性分布假设特性称为<strong>局部相关性</strong>，只关注和自己距离较近的部分节点，而忽略距离较远的节点。 在这种重要性分布假设下，全连接层的连接模式变成了下图所示的状态，输出节点𝑗只与以𝑗为中心的局部区域(感受野)相连接，与其它像素无连接</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200511214621069.png" alt="局部链接的网络层示意图"></p>
<p>其中和自己距离较近的部分节点形成的窗口称为<strong>感受野</strong>(Receptive Field)，表征了每个像素对于中心像素的重要性分布情况，网格内的像素才会被考虑，网格外的像素对于中心像素会被忽略</p>
<h4 id="2-权值共享">2.权值共享</h4>
<p>如下图所示，在计算左上角位置的输出像素时，使用权值矩阵：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>11</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>12</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>13</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>21</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>22</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>23</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>31</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>32</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>33</mn></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">W=\begin{bmatrix}
w_{11} &amp; w_{12} &amp; w_{13}\\ 
w_{21} &amp; w_{22} &amp; w_{23}\\ 
w_{31} &amp; w_{32} &amp; w_{33}
\end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.60004em;vertical-align:-1.55002em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>与对应感受野内部的像素相乘累加， 作为左上角像素的输出值；在计算右下方感受野区域时，共享权值参数𝑾，即使用相同的权值参数𝑾相乘累加，得到右下角像素的输出值，此时网络层的参数量只有3*3=9个，且与输入、输出节点数无关</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200511215857765.png" alt="劝值共享矩阵示意图"></p>
<p>通过运用局部相关性和权值共享的思想，成功把网络的参数量减少到𝑘 × 𝑘(准确地说，是在单输入通道、 单卷积核的条件下)。这种共享权值的“局部连接层”网络其实就是卷积神经网络</p>
<h4 id="3-卷积运算">3.卷积运算</h4>
<p>略</p>
<h3 id="2、卷积神经网络">2、卷积神经网络</h3>
<p>卷积神经网络通过充分利用局部相关性和权值共享的思想，大大地减少了网络的参数量， 从而提高训练效率，更容易实现超大规模的深层网络</p>
<p>以图片数据为例，卷积层接受高、 宽分别为ℎ、 𝑤，通道数为𝑐𝑖𝑛的输入特征图𝑿，在𝑐𝑜𝑢𝑡个高、 宽都为𝑘，通道数为𝑐𝑖𝑛的卷积核作用下，生成高、 宽分别为ℎ′、 𝑤′，通道数为𝑐𝑜𝑢𝑡的特征图输出。需要注意的是，卷积核的高宽可以不等，为了简化讨论，这里仅讨论高宽都为𝑘的情况</p>
<h4 id="1-单通道输入和单卷积核">1.单通道输入和单卷积核</h4>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200512112621597.png" alt="示意图"></p>
<p>完成第一个感受野区域的特征提取后，感受野窗口向右移动一个步长单位(Strides， 记为𝑠， 默认为 1)</p>
<h4 id="2-多通道输入和单卷积核">2.多通道输入和单卷积核</h4>
<p>在多通道输入的情况下， 卷积核的通道数需要和输入𝑿的通道数量相匹配， 卷积核的第𝑖个通道和𝑿的第𝑖个通道运算，得到第𝑖个中间矩阵，此时可以视为单通道输入与单卷积核的情况， 所有通道的中间矩阵对应元素再次相加， 作为最终输出</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200512112150245.png" alt="多通道输入和单卷积核"></p>
<p>整个的计算示意图如下所示， 输入的每个通道处的感受野均与卷积核的对应通道相乘累加，得到与通道数量相等的中间变量，这些中间变量全部相加即得到当前位置的输出值。 输入通道的通道数量决定了卷积核的通道数。 <strong>一个卷积核只能得到一个输出矩阵，无论输入𝑿的通道数量</strong></p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200512112452481.png" alt="多通道输入和单卷积核计算示意图"></p>
<h4 id="3-多通道输入、多卷积核">3.多通道输入、多卷积核</h4>
<p>一般来说，一个卷积核只能完成某种逻辑的特征提取，当需要同时提取多种逻辑特征时， 可以通过增加多个卷积核来得到多种特征，提高神经网络的表达能力，这就是多通道输入、 多卷积核的情况</p>
<p>当出现多卷积核时， 第𝑖 (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">i \in [1,n]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69862em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mclose">]</span></span></span></span>， 𝑛为卷积核个数)个卷积核与输入𝑿运算得到第𝑖个输出矩阵(也称为输出张量𝑶的通道𝑖）， 最后全部的输出矩阵在通道维度上进行拼接(Stack 操作，创建输出通道数的新维度)，产生输出张量𝑶， 𝑶包含了𝑛个通道数</p>
<p>即： <strong>n个卷积核得到n个输出矩阵（通道数），无论输入𝑿的通道数量</strong></p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200512113018813.png" alt="多卷积核示意图"></p>
<ul>
<li>每个卷积核的大小𝑘、步长𝑠、填充设定等都是统一设置（保证输出的每个通道大小一致）</li>
</ul>
<h4 id="4-步长">4.步长</h4>
<p>感受野密度的控制手段一般是通过移动步长(Strides)实现的</p>
<p>步长是指感受野窗口每次移动的长度单位，对于2D输入来说，分为沿𝑥(向右)方向和𝑦(向下)方向的移动长度</p>
<ul>
<li>当步长设计的较小时，感受野以较小幅度移动窗口，有利于提取到更多的特征信息，输出张量的尺寸也更大</li>
<li>当步长设计的较大时， 感受野以较大幅度移动窗口，有利于减少计算代价， 过滤冗余信息，输出张量的尺寸也更小</li>
</ul>
<h4 id="5-填充">5.填充</h4>
<p>在网络模型设计时，有时希望输出𝑶的高宽能够与输入𝑿的高宽相同， 从而方便网络参数的设计、 残差连接等</p>
<p>方法：通过在原输入𝑿的高和宽维度上面进行填充(Padding)若干无效元素操作，得到增大的输入𝑿′。 通过精心设计填充单元的数量， 在𝑿′上面进行卷积运算得到输出𝑶的高宽可以和原输入𝑿相等，甚至更大</p>
<p>卷积神经层的输出尺寸<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><msup><mi>h</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><msup><mi>w</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><msub><mi>c</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[b,h&#x27;,w&#x27;,c_{out}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.001892em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>由卷积核的数量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>c</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">c_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，卷积核的大小𝑘，步长𝑠，填充数𝑝(只考虑上下填充数量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mi>h</mi></msub></mrow><annotation encoding="application/x-tex">p_h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>相同，左右填充数量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">p_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>相同的情况)以及输入𝑿的高宽ℎ/𝑤共同决定， 它们之间的数学关系可以表达为：</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>h</mi><mo mathvariant="normal">′</mo></msup><mo>=</mo><mrow><mo fence="true">⌊</mo><mfrac><mrow><mi>h</mi><mo>+</mo><mn>2</mn><mo>⋅</mo><msub><mi>p</mi><mi>h</mi></msub><mo>−</mo><mi>k</mi></mrow><mi>s</mi></mfrac><mo fence="true">⌋</mo></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">h&#x27;=\left \lfloor \frac{h+2\cdot p_h-k}{s} \right \rfloor+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.751892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.80002em;vertical-align:-0.65002em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">⌊</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mbin mtight">+</span><span class="mord mtight">2</span><span class="mbin mtight">⋅</span><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">⌋</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span></p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>w</mi><mo mathvariant="normal">′</mo></msup><mo>=</mo><mrow><mo fence="true">⌊</mo><mfrac><mrow><mi>w</mi><mo>+</mo><mn>2</mn><mo>⋅</mo><msub><mi>p</mi><mi>w</mi></msub><mo>−</mo><mi>k</mi></mrow><mi>s</mi></mfrac><mo fence="true">⌋</mo></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">w&#x27;=\left \lfloor \frac{w+2\cdot p_w-k}{s} \right \rfloor+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.751892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.80002em;vertical-align:-0.65002em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">⌊</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="mbin mtight">+</span><span class="mord mtight">2</span><span class="mbin mtight">⋅</span><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">⌋</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span></p>
<ul>
<li>其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mi>h</mi></msub></mrow><annotation encoding="application/x-tex">p_h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">p_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>分别表示高、宽方向的填充数量</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">s</span></span></span></span>为卷积移动步长</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>为卷积核大小（这里假设卷积核为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi><mo>∗</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">k*k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>）</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo fence="true">⌊</mo><mo fence="true">⌋</mo></mrow><annotation encoding="application/x-tex">\left \lfloor \right \rfloor</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">⌊</span><span class="mclose delimcenter" style="top:0em;">⌋</span></span></span></span></span>表示向下取整</li>
</ul>
<p>在 TensorFlow 中， 在𝑠 = 1 时， 如果希望输出𝑶和输入𝑿高、 宽相等， 只需要简单地设置参数 <code>padding=&quot;SAME&quot;</code> 即可使 TensorFlow 自动计算 padding 数量</p>
<h3 id="3、卷积层实现">3、卷积层实现</h3>
<p>在 TensorFlow 中，既可以通过自定义权值的底层实现方式搭建神经网络，也可以直接调用现成的卷积层类的高层方式快速搭建复杂网络</p>
<h4 id="1-自定义权值">1.自定义权值</h4>
<p>通过<code>tf.nn.conv2d</code>函数可以方便地实现 2D 卷积运算。<code>tf.nn.conv2d</code>基于输入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mo>:</mo><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi><mo separator="true">,</mo><msub><mi>c</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">X:[b,h,w,c_{in}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>和卷积核<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi><mo>:</mo><mo stretchy="false">[</mo><mi>k</mi><mo separator="true">,</mo><mi>k</mi><mo separator="true">,</mo><msub><mi>c</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>c</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">W:[k,k,c_{in},c_{out}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>进行卷积运算， 得到输出<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>:</mo><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><msup><mi>h</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><msup><mi>w</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><msub><mi>c</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">O:[b,h&#x27;,w&#x27;,c_{out}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.001892em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>c</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">c_{in}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：输入通道数</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>c</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">c_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：卷积核的数量，即输出特征图的通道数</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>：卷积核宽高</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>：图片数量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>]) <span class="comment"># 模拟输入， 3 通道，高宽为 5</span></span><br><span class="line"><span class="comment"># 需要根据[k,k,cin,cout]格式创建 W 张量， 4 个 3x3 大小卷积核</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>w = tf.random.normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="comment"># 步长为 1, padding 为 0,</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = tf.nn.conv2d(x,w,strides=<span class="number">1</span>,padding=[[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line"><span class="comment"># 输出张量的 shape</span></span><br><span class="line">TensorShape([<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>padding参数格式：<code>padding=[[0,0],[上,下],[左,右],[0,0]]</code></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上下左右各填充一个单位：</span></span><br><span class="line">padding=[[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>特别地， 通过设置参数<code>padding='SAME'</code>、<code>strides=1</code>可以直接得到输入、 输出同大小的卷积层</p>
</li>
<li>
<p>当<code>strides&gt;1</code>时， 设置<code>padding='SAME'</code>将使得输出高、宽将成<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mrow><mi>s</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>d</mi><mi>e</mi><mi>s</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{strides}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">s</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>倍地减少</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>w = tf.random.normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="comment"># 高宽先 padding 成可以整除 3 的最小整数 6，然后 6 按 3 倍减少，得到 2x2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = tf.nn.conv2d(x,w,strides=<span class="number">3</span>,padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line">TensorShape([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>卷积神经网络层与全连接层一样，可以设置网络带偏置向量。<code>tf.nn.conv2d</code>函数是没有实现偏置向量计算的， 添加偏置需要手动累加偏置张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据[cout]格式创建偏置向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.zeros([<span class="number">4</span>])</span><br><span class="line"><span class="comment"># 在卷积输出上叠加偏置向量，它会自动 broadcasting 为[b,h&#x27;,w&#x27;,cout]</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = out + b</span><br></pre></td></tr></table></figure>
<h4 id="2-卷积层类">2.卷积层类</h4>
<p>通过卷积层类<code>layers.Conv2D</code>可以直接调用类实例完成卷积层的前向计算（TensorFlow中，API的首字母大写的对象一般表示类，全部小写的一般表示函数）。使用类方式会自动创建（在创建类时或build时）需要的权值张量和偏置向量等， 用户不需要记忆卷积核张量的定义格式</p>
<p>在新建卷积层类时，只需要指定卷积核数量参数<code>filters</code>，卷积核大小<code>kernel_size</code>， 步长<code>strides</code>，填充 <code>padding</code> 等即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建了 4 个3 × 3大小的卷积核的卷积层，步长为 1，padding 方案为&#x27;SAME&#x27;</span></span><br><span class="line">layer = layers.Conv2D(<span class="number">4</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>,padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>卷积核数量即为输出特征图的通道数</li>
</ul>
<p>如果卷积核高宽不等，步长行列方向不等，此时需要将kernel_size参数设计为元组格式<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>h</mi></msub><mo separator="true">,</mo><msub><mi>k</mi><mi>w</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(k_h,k_w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，strides参数设计为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>h</mi></msub><mo separator="true">,</mo><msub><mi>s</mi><mi>w</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(s_h,s_w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 4 个3 × 4大小的卷积核，竖直方向移动步长𝑠ℎ = 2，水平方向移动步长𝑠𝑤 = 1：</span></span><br><span class="line">layer = layers.Conv2D(<span class="number">4</span>,kernel_size=(<span class="number">3</span>,<span class="number">4</span>),strides=(<span class="number">2</span>,<span class="number">1</span>),padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>创建完成后，通过调用实例(的<code>__call__</code>方法)即可完成前向计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建卷积层类</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>layer = layers.Conv2D(<span class="number">4</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>,padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = layer(x) <span class="comment"># 前向计算</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out.shape <span class="comment"># 输出张量的 shape</span></span><br><span class="line">TensorShape([<span class="number">2</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<p>在类<code>Conv2D</code>中，可以通过类成员<code>trainable_variables</code>直接返回𝑾和𝒃的列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回所有待优化张量列表</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>layer.trainable_variables</span><br><span class="line">[&lt;tf.Variable <span class="string">&#x27;conv2d/kernel:0&#x27;</span> shape=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>) dtype=float32, numpy=</span><br><span class="line"> array([[[[ <span class="number">0.13485974</span>, -<span class="number">0.22861657</span>, <span class="number">0.01000655</span>, <span class="number">0.11988598</span>],</span><br><span class="line">          [ <span class="number">0.12811887</span>, <span class="number">0.20501086</span>, -<span class="number">0.29820845</span>, -<span class="number">0.19579397</span>],</span><br><span class="line">          [ <span class="number">0.00858489</span>, -<span class="number">0.24469738</span>, -<span class="number">0.08591779</span>, -<span class="number">0.27885547</span>]], ...</span><br><span class="line"> &lt;tf.Variable <span class="string">&#x27;conv2d/bias:0&#x27;</span> shape=(<span class="number">4</span>,) dtype=float32, numpy=array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>], dtype=float32)&gt;]</span><br></pre></td></tr></table></figure>
<ul>
<li>可以直接调用类实例<code>layer.kernel</code>、<code>layer.bias</code>名访问𝑾和𝒃张量</li>
</ul>
<h3 id="4、LeNet-5实战">4、LeNet-5实战</h3>
<p>1990 年代， Yann LeCun 等人提出了用于手写数字和机器打印字符图片识别的神经网络，被命名为 LeNet-5。 LeNet-5 的提出，使得卷积神经网络在当时能够成功被商用，广泛应用在邮政编码、支票号码识别等任务中</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200513020542762.png" alt="LeNet-5网络结构"></p>
<ul>
<li>接受32*32大小的数字、字符图片，经过第一个卷积层得到[b,28,28,6]形状的张量，经过一个向下采样层，张量尺寸缩小到[b,14,14,6]</li>
<li>经过第二个卷积层，得到[b,10,10,16]形状的张量，同样经过下采样层，张量尺寸缩小到[b,5,5,16]</li>
<li>在进入全连接层之前，先将张量打成[b,400]的张量</li>
<li>送入输出节点数分别为120、84的2个全连接层，得到[b,84]的张量</li>
<li>最后通过Gaussian connections层</li>
</ul>
<p>在上述基础上进行少许调整，使得它更容易在现代深度学习框架上实现：</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200513021054068.png" alt="调整后的网络结构"></p>
<ul>
<li>将输入𝑿形状由32 × 32调整为28 × 28</li>
<li>将 2 个下采样层实现为最大池化层(降低特征图的高、宽，后续会介绍)</li>
<li>利用全连接层替换掉Gaussian connections层</li>
</ul>
<p>加载MNIST数据集：进阶操作 -&gt; 7、经典数据集加载</p>
<p>创建网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Sequential</span><br><span class="line">network = Sequential([ <span class="comment"># 网络容器</span></span><br><span class="line">    layers.Conv2D(<span class="number">6</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>), <span class="comment"># 第一个卷积层, 6 个 3x3 卷积核</span></span><br><span class="line">    layers.MaxPooling2D(pool_size=<span class="number">2</span>,strides=<span class="number">2</span>), <span class="comment"># 高宽各减半的池化层</span></span><br><span class="line">    layers.ReLU(), <span class="comment"># 激活函数</span></span><br><span class="line">    layers.Conv2D(<span class="number">16</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>), <span class="comment"># 第二个卷积层, 16 个 3x3 卷积核</span></span><br><span class="line">    layers.MaxPooling2D(pool_size=<span class="number">2</span>,strides=<span class="number">2</span>), <span class="comment"># 高宽各减半的池化层</span></span><br><span class="line">    layers.ReLU(), <span class="comment"># 激活函数</span></span><br><span class="line">    layers.Flatten(), <span class="comment"># 打平层，方便全连接层处理</span></span><br><span class="line">    layers.Dense(<span class="number">120</span>, activation=<span class="string">&#x27;relu&#x27;</span>), <span class="comment"># 全连接层， 120 个节点</span></span><br><span class="line">    layers.Dense(<span class="number">84</span>, activation=<span class="string">&#x27;relu&#x27;</span>), <span class="comment"># 全连接层， 84 节点</span></span><br><span class="line">    layers.Dense(<span class="number">10</span>) <span class="comment"># 全连接层， 10 个节点</span></span><br><span class="line">])</span><br><span class="line"><span class="comment"># build 一次网络模型，给输入 X 的形状，其中 4 为随意给的 batchsz</span></span><br><span class="line">network.build(input_shape=(<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 统计网络信息</span></span><br><span class="line">network.summary()</span><br></pre></td></tr></table></figure>
<p>网络信息如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Model: <span class="string">&quot;sequential&quot;</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line">conv2d (Conv2D)              multiple                  <span class="number">60</span>        </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d (MaxPooling2D) multiple                  <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">re_lu (ReLU)                 multiple                  <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_1 (Conv2D)            multiple                  <span class="number">880</span>       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d_1 (MaxPooling2 multiple                  <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">re_lu_1 (ReLU)               multiple                  <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten (Flatten)            multiple                  <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                multiple                  <span class="number">48120</span>     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              multiple                  <span class="number">10164</span>     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_2 (Dense)              multiple                  <span class="number">850</span>       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">60</span>,074</span><br><span class="line">Trainable params: <span class="number">60</span>,074</span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>
<ul>
<li>卷积神经网络可以降低特征维度，同时显著降低网络参数量，增加网络深度</li>
</ul>
<h3 id="5、表示学习">5、表示学习</h3>
<p>图片数据的识别过程一般认为也是表示学习 （Representation Learning）的过程，从接受到的原始像素特征开始，逐渐提取边缘、角点等底层特征，再到纹理等中层特征，再到头部、物体部件等高层特征，最后的网络层基于这些学习到的抽象特征表示（ Representation）做分类逻辑的学习。学习到的特征越高层、越准确，就越有利于分类器的分类，从而获得较好的性能。从表示学习的角度来理解，卷积神经网络通过层层堆叠来逐层提取特征，网络训练的过程可以看成特征的学习过程，基于学习到的高层抽象特征可以方便地进行分类任务。</p>
<p>应用表示学习的思想，训练好的卷积神经网络往往能够学习到较好的特征，这种特征的提取方法一般是通用的。比如在猫、狗任务上学习到头、脚、身躯等特征的表示，在其它动物上也能够一定程度上使用。基于这种思想，可以将在任务A上训练好的深层神经网络的前面数个特征提取层迁移到任务B上，只需要训练任务B的分类逻辑（表现为网络的最末数层），即可取得非常好的效果，这种方式是迁移学习的一种，从神经网络角度也称为网络微调（Fine- tuning）。</p>
<h3 id="6、梯度传播">6、梯度传播</h3>
<p>考虑一简单的情形，输入为3×3的单通道矩阵，与一个2×2的卷积核，进行卷积运算，输岀结果打平后直接与虚构的标注计算误差，如下所示：</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201204142507773.png" alt="卷积层梯度传播"></p>
<p>首先推导出输出张量 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi></mrow><annotation encoding="application/x-tex">O</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span></span></span></span> 的表达形式：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>o</mi><mn>00</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msub><mi>x</mi><mn>00</mn></msub><msub><mi>w</mi><mn>00</mn></msub><mo>+</mo><msub><mi>x</mi><mn>01</mn></msub><msub><mi>w</mi><mn>01</mn></msub><mo>+</mo><msub><mi>x</mi><mn>10</mn></msub><msub><mi>w</mi><mn>10</mn></msub><mo>+</mo><msub><mi>x</mi><mn>11</mn></msub><msub><mi>w</mi><mn>11</mn></msub><mo>+</mo><mi>b</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>o</mi><mn>01</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msub><mi>x</mi><mn>01</mn></msub><msub><mi>w</mi><mn>00</mn></msub><mo>+</mo><msub><mi>x</mi><mn>02</mn></msub><msub><mi>w</mi><mn>01</mn></msub><mo>+</mo><msub><mi>x</mi><mn>11</mn></msub><msub><mi>w</mi><mn>10</mn></msub><mo>+</mo><msub><mi>x</mi><mn>12</mn></msub><msub><mi>w</mi><mn>11</mn></msub><mo>+</mo><mi>b</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>o</mi><mn>10</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msub><mi>x</mi><mn>10</mn></msub><msub><mi>w</mi><mn>00</mn></msub><mo>+</mo><msub><mi>x</mi><mn>11</mn></msub><msub><mi>w</mi><mn>01</mn></msub><mo>+</mo><msub><mi>x</mi><mn>20</mn></msub><msub><mi>w</mi><mn>10</mn></msub><mo>+</mo><msub><mi>x</mi><mn>21</mn></msub><msub><mi>w</mi><mn>11</mn></msub><mo>+</mo><mi>b</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>o</mi><mn>11</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msub><mi>x</mi><mn>11</mn></msub><msub><mi>w</mi><mn>00</mn></msub><mo>+</mo><msub><mi>x</mi><mn>12</mn></msub><msub><mi>w</mi><mn>01</mn></msub><mo>+</mo><msub><mi>x</mi><mn>21</mn></msub><msub><mi>w</mi><mn>10</mn></msub><mo>+</mo><msub><mi>x</mi><mn>22</mn></msub><msub><mi>w</mi><mn>11</mn></msub><mo>+</mo><mi>b</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} o_{00} &amp;=x_{00} w_{00}+x_{01} w_{01}+x_{10} w_{10}+x_{11} w_{11}+b \\ o_{01} &amp;=x_{01} w_{00}+x_{02} w_{01}+x_{11} w_{10}+x_{12} w_{11}+b \\ o_{10} &amp;=x_{10} w_{00}+x_{11} w_{01}+x_{20} w_{10}+x_{21} w_{11}+b \\ o_{11} &amp;=x_{11} w_{00}+x_{12} w_{01}+x_{21} w_{10}+x_{22} w_{11}+b \end{aligned}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:6em;vertical-align:-2.7500000000000004em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.25em;"><span style="top:-5.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.4099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-0.9099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.7500000000000004em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.25em;"><span style="top:-5.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">b</span></span></span><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">b</span></span></span><span style="top:-2.4099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">b</span></span></span><span style="top:-0.9099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.7500000000000004em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>以 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>00</mn></msub></mrow><annotation encoding="application/x-tex">w_{00}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的梯度计算为例，通过链式法则分解：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mn>00</mn></msub></mrow></mfrac><mo>=</mo><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>00</mn><mo separator="true">,</mo><mn>01</mn><mo separator="true">,</mo><mn>10</mn><mo separator="true">,</mo><mn>11</mn><mo stretchy="false">}</mo></mrow></munder><mfrac><mrow><mi mathvariant="normal">∂</mi><mi mathvariant="script">L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>o</mi><mi>i</mi></msub></mrow></mfrac><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>o</mi><mi>i</mi></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mn>00</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial w_{00}}=\sum_{i \in\{00,01,10,11\}} \frac{\partial \mathcal{L}}{\partial o_{i}} \frac{\partial o_{i}}{\partial w_{00}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.20744em;vertical-align:-0.8360000000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.3139999999999996em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathcal">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8360000000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.887445em;vertical-align:-1.516005em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.808995em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">∈</span><span class="mopen mtight">{</span><span class="mord mtight">0</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">0</span><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span><span class="mord mtight">1</span><span class="mclose mtight">}</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.3139999999999996em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathcal">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8360000000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.3139999999999996em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8360000000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>其中 $ \frac{\partial \mathcal{L}}{\partial o_{i}} $ 可直接由误差函数推导出来，我们直接来考虑 $ \frac{\partial O_{i}}{\partial w_{i}} $ ，例如 :</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>o</mi><mn>00</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mn>00</mn></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mrow><mo fence="true">(</mo><msub><mi>x</mi><mn>00</mn></msub><msub><mi>w</mi><mn>00</mn></msub><mo>+</mo><msub><mi>x</mi><mn>01</mn></msub><msub><mi>w</mi><mn>01</mn></msub><mo>+</mo><msub><mi>x</mi><mn>10</mn></msub><msub><mi>w</mi><mn>10</mn></msub><mo>+</mo><msub><mi>x</mi><mn>11</mn></msub><msub><mi>w</mi><mn>11</mn></msub><mo>+</mo><mi>b</mi><mo fence="true">)</mo></mrow></mrow><msub><mi>w</mi><mn>00</mn></msub></mfrac><mo>=</mo><msub><mi>x</mi><mn>00</mn></msub></mrow><annotation encoding="application/x-tex">\frac{\partial o_{00}}{\partial w_{00}}=\frac{\partial\left(x_{00} w_{00}+x_{01} w_{01}+x_{10} w_{10}+x_{11} w_{11}+b\right)}{w_{00}}=x_{00} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.20744em;vertical-align:-0.8360000000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.3139999999999996em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8360000000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.263em;vertical-align:-0.8360000000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.3139999999999996em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">b</span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8360000000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>同样的方法有：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable rowspacing="0.15999999999999992em" columnalign="left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>o</mi><mn>01</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mn>00</mn></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mrow><mo fence="true">(</mo><msub><mi>x</mi><mn>01</mn></msub><msub><mi>w</mi><mn>00</mn></msub><mo>+</mo><msub><mi>x</mi><mn>02</mn></msub><msub><mi>w</mi><mn>01</mn></msub><mo>+</mo><msub><mi>x</mi><mn>11</mn></msub><msub><mi>w</mi><mn>10</mn></msub><mo>+</mo><msub><mi>x</mi><mn>12</mn></msub><msub><mi>w</mi><mn>11</mn></msub><mo>+</mo><mi>b</mi><mo fence="true">)</mo></mrow></mrow><msub><mi>w</mi><mn>00</mn></msub></mfrac><mo>=</mo><msub><mi>x</mi><mn>01</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>o</mi><mn>10</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mn>00</mn></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mrow><mo fence="true">(</mo><msub><mi>x</mi><mn>10</mn></msub><msub><mi>w</mi><mn>00</mn></msub><mo>+</mo><msub><mi>x</mi><mn>11</mn></msub><msub><mi>w</mi><mn>01</mn></msub><mo>+</mo><msub><mi>x</mi><mn>20</mn></msub><msub><mi>w</mi><mn>10</mn></msub><mo>+</mo><msub><mi>x</mi><mn>21</mn></msub><msub><mi>w</mi><mn>11</mn></msub><mo>+</mo><mi>b</mi><mo fence="true">)</mo></mrow></mrow><msub><mi>w</mi><mn>00</mn></msub></mfrac><mo>=</mo><msub><mi>x</mi><mn>10</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>o</mi><mn>11</mn></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mn>00</mn></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mrow><mo fence="true">(</mo><msub><mi>x</mi><mn>11</mn></msub><msub><mi>w</mi><mn>00</mn></msub><mo>+</mo><msub><mi>x</mi><mn>12</mn></msub><msub><mi>w</mi><mn>01</mn></msub><mo>+</mo><msub><mi>x</mi><mn>21</mn></msub><msub><mi>w</mi><mn>10</mn></msub><mo>+</mo><msub><mi>x</mi><mn>22</mn></msub><msub><mi>w</mi><mn>11</mn></msub><mo>+</mo><mi>b</mi><mo fence="true">)</mo></mrow></mrow><msub><mi>w</mi><mn>00</mn></msub></mfrac><mo>=</mo><msub><mi>x</mi><mn>11</mn></msub></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{array}{l} \frac{\partial o_{01}}{\partial w_{00}}=\frac{\partial\left(x_{01} w_{00}+x_{02} w_{01}+x_{11} w_{10}+x_{12} w_{11}+b\right)}{w_{00}}=x_{01} \\ \frac{\partial o_{10}}{\partial w_{00}}=\frac{\partial\left(x_{10} w_{00}+x_{11} w_{01}+x_{20} w_{10}+x_{21} w_{11}+b\right)}{w_{00}}=x_{10} \\ \frac{\partial o_{11}}{\partial w_{00}}=\frac{\partial\left(x_{11} w_{00}+x_{12} w_{01}+x_{21} w_{10}+x_{22} w_{11}+b\right)}{w_{00}}=x_{11} \end{array}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.3652999999999995em;vertical-align:-1.9326499999999998em;"></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.4326499999999998em;"><span style="top:-4.43265em;"><span class="pstrut" style="height:3.01em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8962079999999999em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="minner mtight"><span class="mopen mtight delimcenter" style="top:0em;"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mathdefault mtight">b</span><span class="mclose mtight delimcenter" style="top:0em;"><span class="mtight">)</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.97755em;"><span class="pstrut" style="height:3.01em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8962079999999999em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="minner mtight"><span class="mopen mtight delimcenter" style="top:0em;"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mathdefault mtight">b</span><span class="mclose mtight delimcenter" style="top:0em;"><span class="mtight">)</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.52245em;"><span class="pstrut" style="height:3.01em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8962079999999999em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="minner mtight"><span class="mopen mtight delimcenter" style="top:0em;"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mathdefault mtight">b</span><span class="mclose mtight delimcenter" style="top:0em;"><span class="mtight">)</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.9326499999999998em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span></span></span></span></span></p>
<p>可以观察到，通过循环移动感受野的方式并没有改变网络层可导性，同时梯度的推导也并不复杂，只是当网络层数增大以后，人工梯度推导将变得十分的繁琐。不过深度学习框架可以自动完成所有参数的梯度计算与更新，我们只需要设计好网络结构即可</p>
<h3 id="7、池化层">7、池化层</h3>
<p>在卷积层中，可以通过调节步长参数𝑠实现特征图的高宽成倍缩小，从而降低了网络的参数量。实际上，除了通过设置步长，还有一种专门的网络层可以实现尺寸缩减功能，即池化层（ Pooling Layer）</p>
<p>池化层同样基于局部相关性的思想，通过从局部相关的一组元素中进行采样或信息聚合，从而得到新的元素值。特别地：</p>
<ul>
<li>最大池化层（ Max Pooling）从局部相关元素集中选取最大的一个元素值</li>
<li>平均池化层（ Average Pooling）从局部相关元素集中计算平均值并返回</li>
</ul>
<blockquote>
<p>以5×5输入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span>的最大池化层为例，假设池化感受野窗口大小𝑘=2，步长𝑠=1的情况，如下所示。绿色虚线方框代表第一个感受野的位置，此时输出结果为$$x^{\prime}=\max ({1,-1,-1,-2})=1$$，并以此类推整个池化过程</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201204143831343.png" alt="最大池化层举例"></p>
</blockquote>
<p>由于池化层没有需要学习的参数，计算简单， 并且可以有效减低特征图的尺寸，非常适合图片这种类型的数据，在计算机视觉相关任务中得到了广泛的应用。</p>
<p>通过精心设计池化层感受野的高宽𝑘和步长𝑠参数，可以实现各种降维运算</p>
<h3 id="8、BatchNorm层">8、BatchNorm层</h3>
<p>卷积神经网络的出现，网络参数量大大减低，使得几十层的深层网络成为可能。然而，在残差网络出现之前，网络的加深使得网络训练变得非常不稳定，甚至出现网络长时间不更新甚至不收敛的现象，同时网络对超参数比较敏感，超参数的微量扰动也会导致网络的训练轨迹完全改变</p>
<p>2015年， Google研究人员 Sergey roffe等提出了一种参数标准化（ Normalize）的手段并基于参数标准化设计了 Batch nomalization（简写为 BatchNorn，或BN）层。BN层的提出，使得网络的超参数的设定更加自由，比如更大的学习率、更随意的网络初始化等，同时网络的收敛速度更快，性能也更好。BN层提岀后便广泛地应用在各种深度网络模型上，卷积层、BN层、ReLU层、池化层一度成为网络模型的标配单元块，通过堆叠Conv-BN-ReLU-Pooling方式往往可以获得不错的模型性能。</p>
<p>在 TensorFlow中，通过 <code>layers.BatchNormalization()</code>类可以非常方便地实现BN层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 BN 层</span></span><br><span class="line">layer=layers.BatchNormalization()</span><br></pre></td></tr></table></figure>
<p>与全连接层、卷积层不同，BN层的训练阶段和测试阶段的行为不同，需要通过设置<code>training</code>标志位来区分训练模式还是测试模式</p>
<blockquote>
<p>以 LeNet5的网络模型为例，在卷积层后添加BN层，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">network = Sequential([ <span class="comment"># 网络容器</span></span><br><span class="line">    layers.Conv2D(<span class="number">6</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>),</span><br><span class="line">    <span class="comment"># 插入 BN 层</span></span><br><span class="line">    layers.BatchNormalization(),</span><br><span class="line">    layers.MaxPooling2D(pool_size=<span class="number">2</span>,strides=<span class="number">2</span>),</span><br><span class="line">    layers.ReLU(),</span><br><span class="line">    layers.Conv2D(<span class="number">16</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>),</span><br><span class="line">    <span class="comment"># 插入 BN 层</span></span><br><span class="line">    layers.BatchNormalization(),</span><br><span class="line">    layers.MaxPooling2D(pool_size=<span class="number">2</span>,strides=<span class="number">2</span>),</span><br><span class="line">    layers.ReLU(),</span><br><span class="line">    layers.Flatten(),</span><br><span class="line">    layers.Dense(<span class="number">120</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    <span class="comment"># 此处也可以插入 BN 层</span></span><br><span class="line">    layers.Dense(<span class="number">84</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    <span class="comment"># 此处也可以插入 BN 层</span></span><br><span class="line">    layers.Dense(<span class="number">10</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>在训练阶段，需要设置网络的参数 <code>training=True</code> 以区分 BN 层是训练还是测试模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    <span class="comment"># 插入通道维度</span></span><br><span class="line">    x = tf.expand_dims(x,axis=<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 前向计算，设置计算模式， [b, 784] =&gt; [b, 10]</span></span><br><span class="line">    out = network(x, training=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>在测试阶段，需要设置 <code>training=False</code>， 避免 BN 层采用错误的行为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> db_test: <span class="comment"># 遍历测试集</span></span><br><span class="line">    <span class="comment"># 插入通道维度</span></span><br><span class="line">    x = tf.expand_dims(x,axis=<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 前向计算，测试模式</span></span><br><span class="line">    out = network(x, training=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="9、经典卷积网络">9、经典卷积网络</h3>
<p>自2012年 AlexNet 的提出以来，各种各样的深度卷积神经网络模型相继被提出，其中比较有代表性的有ⅤGG系列， Goog LeNet系列， ResNet系列， DenseNet系列等，他们的网络层数整体趋势逐渐增多。</p>
<h4 id="1-AlexNet">1.AlexNet</h4>
<p>2012年， ILSVRC12挑战赛 ImageNet 数据集分类任务的冠军 Alex Krichevsky 提出了8层的深度神经网络模型 AlexNet，它接收输入为224×224大小的彩色图片数据，经过五个卷积层和三个全连接层后得到样本属于1000个类别的概率分布。为了降低特征图的维度， AlexNet在第1、2、5个卷积层后添加了 Max Pooling层，如下图所示，网络的参数量达到了6000万个：</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201204170917900.png" alt="AlexNet网络结构"></p>
<p>AlexNet的创新之处在于</p>
<ul>
<li>层数达到了较深的8层</li>
<li>采用了ReLU激活函数，过去的神经网络大多采用Sigmoid激活函数，计算相对复杂，容易出现梯度弥散现象</li>
<li>引入Dropout层。Dropout提高了模型的泛化能力，防止过拟合</li>
</ul>
<h4 id="2-VGG系列">2.VGG系列</h4>
<p>2014年， ILSVRCI4挑战赛 ImageNet分类任务的亚军牛津大学ⅤGG实验室提出了ⅤGGI1、 VGG13、ⅤGGl6、VGG19等一系列的网络模型，并将网络深度最高提升至19层。</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201204171320794.png" alt="VGG系列网络结构配置"></p>
<p>以GG16为例，它接受224×224大小的彩色图片数据，经过2个Conv-Conv-Pooling单元，和3个Conv- Conv-ConV- Pooling单元的堆叠，最后通过3层全连接层输出当前图片分别属于1000类别的概率分布</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201207015811052.png" alt="VGG16网络结构"></p>
<p>VGG系列网络的创新之处在于：</p>
<ul>
<li>层数提升至19层</li>
<li>全部釆用更小的3×3卷积核，相对于 AlexNet 中7×7的卷积核，参数量更少，计算代价更低</li>
<li>采用更小的池化层2×2窗口和步长s=2，而 AlexNet中是3×3的池化窗口和步长s=2</li>
</ul>
<h4 id="3-GoogLeNet">3.GoogLeNet</h4>
<p>3×3的卷积核参数量更少，计算代价更低，同时在性能表现上甚至更优越，因此业界开始探索卷积核最小的情况：1×1卷积核。</p>
<p>如下所示，输入为3通道的5×5图片，与单个1×1的卷积核进行卷积运算，每个通道的数据与对应通道的卷积核运算，得到 3个通道的中间矩阵，对应位置相加得到最终的输出张量。对于输入shape为[b,h,w,Cin]， 1x1卷积层的输出为[b,h,w,Cout]，其中cin为输入数据的通道数，cout为输出数据的通道 数，也是1×1卷积核的数量。</p>
<p>1×1卷积核的一个特别之处在于，它可以不改变特征图的宽高，而只对通道数c进行变换。</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201207020240036.png" alt="1*1卷积核示意图"></p>
<p>2014年，ILSVRC14挑战赛的冠军Google提出了大量采用3×3和1×1卷积核的网络 模型： GoogLeNet，网络层数达到了22层。虽然GoogLeNet的层数远大于AlexNet, 但是它的参数量却只有AlexNet的1/12，同时性能也远好于AlexNet。</p>
<p>GoogLeNet网络采用模块化设计的思想，通过大量堆叠Inception模块，形成了复杂的网络结构。如下所示，Inception模块的输入为X，通过4个子网络得到4个网络 输出，在通道轴上面进行拼接合并，形成Inception模块的输出。这4个子网络是：</p>
<ul>
<li>1×1卷积层</li>
<li>1×1卷积层，再通过一个3×3卷积层</li>
<li>1×1卷积层，再通过一个5×5卷积层</li>
<li>3×3最大池化层，再通过1×1卷积层</li>
</ul>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201207020858345.png" alt="Inception模块"></p>
<p>GoogLeNet 的网络结构如下所示，其中红色框中的网络结构即为Inception模块的网络结构：</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201207021013259.png" alt="GoogLeNet网络结构"></p>
<h3 id="10、CIFAR10与VGG13实战">10、CIFAR10与VGG13实战</h3>
<p>CIFAR10 数据集由加拿大 Canadian Institute For Advanced Research 发布，它包含了飞机、汽车、鸟、猫等共 10 大类物体的彩色图片，每个种类收集了 6000 张32 × 32大小图片，共 6 万张图片。其中 5 万张作为训练数据集， 1 万张作为测试数据集。</p>
<p>在 TensorFlow 中，不需要手动下载、 解析和加载 CIFAR10 数据集，通过 <code>datasets.cifar10.load_data()</code> 函数就可以直接加载切割好的训练集和测试集</p>
<p>CIFAR10图片识别任务并不简单，这主要是由于CIFAR10的图片内容需要大量细节才能呈现，而保存的图片分辨率仅有32×32，使得部分主体信息较为模糊。浅层的神经网络表达能力有限，很难训练优化到较好的性能，这里将基于表达能力更强的VGG13网络，并根据数据集特点修改部分网络结构，完成CIFAR10图片识别。修改如下：</p>
<ul>
<li>将网络输入调整为32×32。原网络输入为224×224，导致全连接层输入特征维度过大，网络参数量过大</li>
<li>3个全连接层的维度调整为[256,64,10]，满足10分类任务的设定</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> layers, optimizers, datasets, Sequential</span><br><span class="line"><span class="keyword">import</span>  os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>]=<span class="string">&#x27;2&#x27;</span>  <span class="comment"># 消除调用显卡警告</span></span><br><span class="line">tf.random.set_seed(<span class="number">2345</span>)</span><br><span class="line"></span><br><span class="line">conv_layers = [ <span class="comment"># 5 units of conv + max pooling</span></span><br><span class="line">    <span class="comment"># unit 1</span></span><br><span class="line">    <span class="comment"># 64 个 3x3 卷积核, 输入输出同大小</span></span><br><span class="line">    layers.Conv2D(<span class="number">64</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], padding=<span class="string">&quot;same&quot;</span>, activation=tf.nn.relu),</span><br><span class="line">    layers.Conv2D(<span class="number">64</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], padding=<span class="string">&quot;same&quot;</span>, activation=tf.nn.relu),</span><br><span class="line">    layers.MaxPool2D(pool_size=[<span class="number">2</span>, <span class="number">2</span>], strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>),<span class="comment"># 高宽减半</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># unit 2</span></span><br><span class="line">    <span class="comment"># 输出通道提升至 128，高宽大小减半</span></span><br><span class="line">    layers.Conv2D(<span class="number">128</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], padding=<span class="string">&quot;same&quot;</span>, activation=tf.nn.relu),</span><br><span class="line">    layers.Conv2D(<span class="number">128</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], padding=<span class="string">&quot;same&quot;</span>, activation=tf.nn.relu),</span><br><span class="line">    layers.MaxPool2D(pool_size=[<span class="number">2</span>, <span class="number">2</span>], strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment"># unit 3</span></span><br><span class="line">    <span class="comment"># 输出通道提升至 256，高宽大小减半</span></span><br><span class="line">    layers.Conv2D(<span class="number">256</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], padding=<span class="string">&quot;same&quot;</span>, activation=tf.nn.relu),</span><br><span class="line">    layers.Conv2D(<span class="number">256</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], padding=<span class="string">&quot;same&quot;</span>, activation=tf.nn.relu),</span><br><span class="line">    layers.MaxPool2D(pool_size=[<span class="number">2</span>, <span class="number">2</span>], strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment"># unit 4</span></span><br><span class="line">    <span class="comment"># 输出通道提升至 512，高宽大小减半</span></span><br><span class="line">    layers.Conv2D(<span class="number">512</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], padding=<span class="string">&quot;same&quot;</span>, activation=tf.nn.relu),</span><br><span class="line">    layers.Conv2D(<span class="number">512</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], padding=<span class="string">&quot;same&quot;</span>, activation=tf.nn.relu),</span><br><span class="line">    layers.MaxPool2D(pool_size=[<span class="number">2</span>, <span class="number">2</span>], strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment"># unit 5</span></span><br><span class="line">    <span class="comment"># 输出通道提升至 512，高宽大小减半</span></span><br><span class="line">    layers.Conv2D(<span class="number">512</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], padding=<span class="string">&quot;same&quot;</span>, activation=tf.nn.relu),</span><br><span class="line">    layers.Conv2D(<span class="number">512</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], padding=<span class="string">&quot;same&quot;</span>, activation=tf.nn.relu),</span><br><span class="line">    layers.MaxPool2D(pool_size=[<span class="number">2</span>, <span class="number">2</span>], strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">]</span><br><span class="line"><span class="comment"># 预处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    <span class="comment"># [0~1]</span></span><br><span class="line">    x = <span class="number">2</span>*tf.cast(x, dtype=tf.float32) / <span class="number">255.</span>-<span class="number">1</span></span><br><span class="line">    y = tf.cast(y, dtype=tf.int32)</span><br><span class="line">    <span class="keyword">return</span> x,y</span><br><span class="line"></span><br><span class="line">(x,y), (x_test, y_test) = datasets.cifar10.load_data()  <span class="comment"># 在线下载，加载 CIFAR10 数据集</span></span><br><span class="line">y = tf.squeeze(y, axis=<span class="number">1</span>)  <span class="comment"># 删除 y 的一个维度， [b,1] =&gt; [b]</span></span><br><span class="line">y_test = tf.squeeze(y_test, axis=<span class="number">1</span>)</span><br><span class="line">print(x.shape, y.shape, x_test.shape, y_test.shape)  <span class="comment"># 打印训练集和测试集的形状</span></span><br><span class="line"><span class="comment"># 训练集的𝑿和𝒚形状为：(50000, 32, 32, 3)和(50000)，测试集的𝑿和𝒚形状为(10000, 32, 32, 3)和(10000)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建训练集对象，随机打乱，预处理，批量化</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x,y))</span><br><span class="line">train_db = train_db.shuffle(<span class="number">1000</span>).<span class="built_in">map</span>(preprocess).batch(<span class="number">128</span>)</span><br><span class="line"><span class="comment"># 构建测试集对象，预处理，批量化</span></span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test))</span><br><span class="line">test_db = test_db.<span class="built_in">map</span>(preprocess).batch(<span class="number">64</span>)</span><br><span class="line"><span class="comment"># 从训练集中采样一个 Batch， 并观察</span></span><br><span class="line">sample = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_db))</span><br><span class="line">print(<span class="string">&#x27;sample:&#x27;</span>, sample[<span class="number">0</span>].shape, sample[<span class="number">1</span>].shape,</span><br><span class="line">      tf.reduce_min(sample[<span class="number">0</span>]), tf.reduce_max(sample[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="comment"># [b, 32, 32, 3] =&gt; [b, 1, 1, 512]</span></span><br><span class="line">    conv_net = Sequential(conv_layers)  <span class="comment"># 利用前面创建的层列表构建网络容器</span></span><br><span class="line">	<span class="comment"># 创建 3 层全连接层子网络</span></span><br><span class="line">    fc_net = Sequential([</span><br><span class="line">        layers.Dense(<span class="number">256</span>, activation=tf.nn.relu),</span><br><span class="line">        layers.Dense(<span class="number">128</span>, activation=tf.nn.relu),</span><br><span class="line">        layers.Dense(<span class="number">10</span>, activation=<span class="literal">None</span>),</span><br><span class="line">    ])</span><br><span class="line">	<span class="comment"># build2 个子网络，并打印网络参数信息</span></span><br><span class="line">    conv_net.build(input_shape=[<span class="literal">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>])</span><br><span class="line">    fc_net.build(input_shape=[<span class="literal">None</span>, <span class="number">512</span>])</span><br><span class="line">    conv_net.summary()</span><br><span class="line">    fc_net.summary()</span><br><span class="line">    optimizer = optimizers.Adam(lr=<span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [1, 2] + [3, 4] =&gt; [1, 2, 3, 4]</span></span><br><span class="line">    <span class="comment"># 由于将网络实现为 2 个子网络，在进行梯度更新时，需要合并 2 个子网络的待优化参数列表</span></span><br><span class="line">    variables = conv_net.trainable_variables + fc_net.trainable_variables</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):</span><br><span class="line">        <span class="keyword">for</span> step, (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db):</span><br><span class="line">            <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                <span class="comment"># [b, 32, 32, 3] =&gt; [b, 1, 1, 512]</span></span><br><span class="line">                out = conv_net(x)</span><br><span class="line">                <span class="comment"># flatten, =&gt; [b, 512]</span></span><br><span class="line">                out = tf.reshape(out, [-<span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">                <span class="comment"># [b, 512] =&gt; [b, 10]</span></span><br><span class="line">                logits = fc_net(out)</span><br><span class="line">                <span class="comment"># [b] =&gt; [b, 10]</span></span><br><span class="line">                y_onehot = tf.one_hot(y, depth=<span class="number">10</span>)</span><br><span class="line">                <span class="comment"># compute loss</span></span><br><span class="line">                loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=<span class="literal">True</span>)</span><br><span class="line">                loss = tf.reduce_mean(loss)</span><br><span class="line"></span><br><span class="line">            grads = tape.gradient(loss, variables)  <span class="comment"># 对所有参数求梯度</span></span><br><span class="line">            optimizer.apply_gradients(<span class="built_in">zip</span>(grads, variables))  <span class="comment"># 自动更新</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> step %<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                print(epoch, step, <span class="string">&#x27;loss:&#x27;</span>, <span class="built_in">float</span>(loss))</span><br><span class="line"></span><br><span class="line">        total_num = <span class="number">0</span></span><br><span class="line">        total_correct = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x,y <span class="keyword">in</span> test_db:</span><br><span class="line"></span><br><span class="line">            out = conv_net(x)</span><br><span class="line">            out = tf.reshape(out, [-<span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">            logits = fc_net(out)</span><br><span class="line">            prob = tf.nn.softmax(logits, axis=<span class="number">1</span>)</span><br><span class="line">            pred = tf.argmax(prob, axis=<span class="number">1</span>)</span><br><span class="line">            pred = tf.cast(pred, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)</span><br><span class="line">            correct = tf.reduce_sum(correct)</span><br><span class="line"></span><br><span class="line">            total_num += x.shape[<span class="number">0</span>]</span><br><span class="line">            total_correct += <span class="built_in">int</span>(correct)</span><br><span class="line"></span><br><span class="line">        acc = total_correct / total_num</span><br><span class="line">        print(epoch, <span class="string">&#x27;acc:&#x27;</span>, acc)  <span class="comment"># 计算一遍epoch的测试准确率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h3 id="11、卷积层变种">11、卷积层变种</h3>
<h4 id="1-空洞卷积">1.空洞卷积</h4>
<p>普通的卷积层为了减少网络的参数量，卷积核的设计通常选择较小的1×1和3×3感受野大小。小卷积核使得网络提取特征时的感受野区域有限，但是增大感受野的区域又会增加网络的参数量和计算代价，因此需要权衡设计</p>
<p>空洞卷积（Dilated/Atrous Convolution)的提出较好地解决这个问题，空洞卷积在普通卷积的感受野上增加一个Dilation Rate参数，用于控制感受野区域的采样步长，如下所示：</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201207025041983.png" alt="感受野采样步长示意图"></p>
<ul>
<li>当感受野的采样步长Dilation Rate为1时，每个感受野采样点之间的距离为 1，此时的空洞卷积退化为普通的卷积</li>
<li>当Dilation Rate为2时，感受野每2个单元采样一 个点，如上图中间的绿色方框中绿色格子所示，每个采样格子之间的距离为2</li>
<li>如上图右边的Dilation Rate为3，采样步长为3</li>
</ul>
<p>尽管Dilation Rate的增大会使得感受野区域增大，但是实际参与运算的点数仍然保持不变</p>
<p>空洞卷积在不增加网络参数的条件下，提供了更大的感受野窗口。但是在使用空洞卷积设置网络模型时，需要精心设计Dilation Rate参数来避免出现网格效应，同时较大的 Dilation Rate 参数并不利于小物体的检测、语义分割等任务</p>
<hr>
<p>在TensorFlow中，可以通过设置<code>layers.Conv2D()</code>类的<code>dilation_rate</code>参数来选择使用普通卷积还是空洞卷积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">1</span>,<span class="number">7</span>,<span class="number">7</span>,<span class="number">1</span>]) <span class="comment"># 模拟输入</span></span><br><span class="line"><span class="comment"># 空洞卷积， 1 个 3x3 的卷积核</span></span><br><span class="line">layer = layers.Conv2D(<span class="number">1</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>,dilation_rate=<span class="number">2</span>)</span><br><span class="line">out = layer(x) <span class="comment"># 前向计算</span></span><br><span class="line">out.shape</span><br><span class="line"><span class="comment"># 输出: TensorShape([1, 3, 3, 1])</span></span><br></pre></td></tr></table></figure>
<h4 id="2-转置卷积">2.转置卷积</h4>
<p>转置卷积(Transposed Convolution 或 Fractionally Strided Convolution）：通过在输入之间填充大量的padding来实现输出高宽大于输入高宽的效果，从而实现向上采样的目的</p>
<blockquote>
<p>部分资料也称之为反卷积(Deconvolution)，实际上反卷积在数学上定义为卷积的逆过程，但转置卷积并不能恢复出原卷积的输入，因此称为反卷积并不妥当</p>
<p>转置卷积仅仅能够恢复原原始数据的大小（shape，即<strong>形状相同</strong>），但是不能还原原始数据，即：</p>
<p>原始数据(5x5) --&gt; 3x3卷积核，步长为2 --&gt; 卷积后数据(2x2)<br>
卷积后数据(2x2) --&gt; 转置卷积 --&gt; 原始数据(5x5)，但是此时数据已经不同</p>
</blockquote>
<p>转置卷积输出与输入关系可以根据以下情况分两种来看：</p>
<p><strong>1. <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>o</mi><mo>+</mo><mn>2</mn><mi>p</mi><mo>−</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">o+2p-k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">o</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">2</span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">s</span></span></span></span>倍数</strong></p>
<p>在<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>o</mi><mo>+</mo><mn>2</mn><mi>p</mi><mo>−</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">o+2p-k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">o</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">2</span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">s</span></span></span></span>倍数时，转置卷积输出与输入关系为：</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>o</mi><mo>=</mo><mo stretchy="false">(</mo><mi>i</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mi>s</mi><mo>+</mo><mi>k</mi><mo>−</mo><mn>2</mn><mi>p</mi></mrow><annotation encoding="application/x-tex">o=(i-1)s+k-2p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">o</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">2</span><span class="mord mathdefault">p</span></span></span></span></p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>o</mi></mrow><annotation encoding="application/x-tex">o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">o</span></span></span></span>：转置卷积输出大小</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span>：转置卷积输入大小</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">s</span></span></span></span>：转置卷积的步长</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>：转置卷积核大小</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span>：填充大小</li>
</ul>
<p><strong>2. <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>o</mi><mo>+</mo><mn>2</mn><mi>p</mi><mo>−</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">o+2p-k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">o</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">2</span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>不为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">s</span></span></span></span>倍数</strong></p>
<p>由于在普通的卷积运算中，有：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>o</mi><mo>=</mo><mrow><mo fence="true">⌊</mo><mfrac><mrow><mi>i</mi><mo>+</mo><mn>2</mn><mo>∗</mo><mi>p</mi><mo>−</mo><mi>k</mi></mrow><mi>s</mi></mfrac><mo fence="true">]</mo></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">o=\left\lfloor\frac{i+2 * p-k}{s}\right]+1
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">o</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">⌊</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">s</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span></span></p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>o</mi></mrow><annotation encoding="application/x-tex">o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">o</span></span></span></span>：卷积输出大小</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span>：卷积输入大小</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">s</span></span></span></span>：卷积的步长</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>：卷积核大小</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span>：填充大小</li>
</ul>
<p>此时当步长 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">s&gt;1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> 时，向下取整运算使得出现多种不同输入尺寸 𝑖 对应到相同的输出尺寸 𝑜 上。因此，不同输入大小的卷积运算可能获得相同大小的输出。</p>
<p>考虑到卷积与转置卷积输入输出大小关系互换，从转置卷积的角度来说，输入尺寸 i 经过转置卷积运算后，可能获得不同的输出 o 大小。因此转置卷积也需要相应的调整：</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>o</mi><mo>=</mo><mo stretchy="false">(</mo><mi>i</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mi>s</mi><mo>+</mo><mi>k</mi><mo>−</mo><mn>2</mn><mi>p</mi><mo>+</mo><mi>a</mi></mrow><annotation encoding="application/x-tex">o=(i-1)s+k-2p+a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">o</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">2</span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span></span></span></span></p>
<p>其中$$a=(o+2p-k)%s$$</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>o</mi></mrow><annotation encoding="application/x-tex">o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">o</span></span></span></span>：转置卷积输出大小</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span>：转置卷积输入大小</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">s</span></span></span></span>：转置卷积的步长</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>：转置卷积核大小</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span>：填充大小</li>
</ul>
<hr>
<p>在 TensorFlow 中不需要手动指定𝑎参数，只需要指定输出尺寸即可， TensorFlow 会自动推导需要填充的行列数𝑎，前提是输出尺寸合法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 恢复出 6x6 大小</span></span><br><span class="line"><span class="comment"># out为[1,2,2,1] w为[3,3,1,1]</span></span><br><span class="line">xx = tf.nn.conv2d_transpose(out, w, strides=<span class="number">2</span>,</span><br><span class="line">                            padding=<span class="string">&#x27;VALID&#x27;</span>,</span><br><span class="line">                            output_shape=[<span class="number">1</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>转置卷积的卷积核的定义格式为 [𝑘, 𝑘, 𝑐𝑜𝑢𝑡, 𝑐𝑖𝑛]</li>
<li>不支持自定义 padding 设置，只能设置为 VALID 或者 SAME
<ul>
<li>VALID：输出大小表达为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>o</mi><mo>=</mo><mo stretchy="false">(</mo><mi>i</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mi>s</mi><mo>+</mo><mi>k</mi><mo>−</mo><mn>2</mn><mi>p</mi><mo>+</mo><mi>a</mi></mrow><annotation encoding="application/x-tex">o=(i-1)s+k-2p+a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">o</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">2</span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span></span></span></span></li>
<li>SAME：输出大小表达为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>o</mi><mo>=</mo><mi>i</mi><mi mathvariant="normal">.</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">o=i.s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">o</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span><span class="mord">.</span><span class="mord mathdefault">s</span></span></span></span></li>
</ul>
</li>
<li>通过改变参数 <code>output_shape=[1,5,5,1]</code>也可以获得高宽为 5×5 的张量</li>
</ul>
<blockquote>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 4x4 大小的输入</span></span><br><span class="line">x = tf.<span class="built_in">range</span>(<span class="number">16</span>)+<span class="number">1</span></span><br><span class="line">x = tf.reshape(x,[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>])</span><br><span class="line">x = tf.cast(x, tf.float32)</span><br><span class="line"><span class="comment"># 创建 3x3 卷积核</span></span><br><span class="line">w = tf.constant([[-<span class="number">1</span>,<span class="number">2</span>,-<span class="number">3.</span>],[<span class="number">4</span>,-<span class="number">5</span>,<span class="number">6</span>],[-<span class="number">7</span>,<span class="number">8</span>,-<span class="number">9</span>]])</span><br><span class="line">w = tf.expand_dims(w,axis=<span class="number">2</span>)</span><br><span class="line">w = tf.expand_dims(w,axis=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 普通卷积运算</span></span><br><span class="line">out = tf.nn.conv2d(x,w,strides=<span class="number">1</span>,padding=<span class="string">&#x27;VALID&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">42</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">    array([[-<span class="number">56.</span>, -<span class="number">61.</span>],</span><br><span class="line">           [-<span class="number">76.</span>, -<span class="number">81.</span>]], dtyp</span><br></pre></td></tr></table></figure>
<p>恢复：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 恢复 4x4 大小的输入</span></span><br><span class="line">xx = tf.nn.conv2d_transpose(out, w, strides=<span class="number">1</span>, padding=<span class="string">&#x27;VALID&#x27;</span>,</span><br><span class="line">                            output_shape=[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>])</span><br><span class="line">tf.squeeze(xx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">44</span>, shape=(<span class="number">4</span>, <span class="number">4</span>), dtype=float32, numpy=</span><br><span class="line">    array([[ <span class="number">56.</span>, -<span class="number">51.</span>, <span class="number">46.</span>, <span class="number">183.</span>],</span><br><span class="line">           [-<span class="number">148.</span>, -<span class="number">35.</span>, <span class="number">35.</span>, -<span class="number">123.</span>],</span><br><span class="line">           [ <span class="number">88.</span>, <span class="number">35.</span>, -<span class="number">35.</span>, <span class="number">63.</span>],</span><br><span class="line">           [ <span class="number">532.</span>, -<span class="number">41.</span>, <span class="number">36.</span>, <span class="number">729.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>转置卷积也可以和其他层一样，通过 <code>layers.Conv2DTranspose</code> 类创建一个转置卷积层，然后调用实例即可完成前向计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">layer = layers.Conv2DTranspose(<span class="number">1</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>,padding=<span class="string">&#x27;VALID&#x27;</span>)</span><br><span class="line">xx2 = layer(out) <span class="comment"># 通过转置卷积层</span></span><br></pre></td></tr></table></figure>
<hr>
<p><strong>矩阵角度</strong></p>
<p>转置卷积的转置是指卷积核矩阵W产生的稀疏矩阵W’在计算过程中需要先转置 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>W</mi><mrow><msup><mrow></mrow><mo mathvariant="normal">′</mo></msup><mi>T</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W^{&#x27;T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.94248em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.94248em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8278285714285715em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span> ，再进行矩阵相乘运算，而普通卷积并没有转置W’的步骤。这也是它被称为转置卷积的名字由来</p>
<p>考虑普通Conv2d运算：X和W，需要根据步长s将卷积核在行、列方向循环移动获取参与运算的感受野的数据，串行计算每个窗口处的“相乘累加”值计算效率极低。为了加速运算，在数学上可以将卷积核W根据步长s重排成稀疏矩阵W’，再通过W’@X’一次完成运算(实际上，W’矩阵过于稀疏，导致很多无用的0乘运算，很多深度学习框架也不是通过这种方式实现的)</p>
<p>转置卷积具有“<strong>放大特征图</strong>”的功能，在生成对抗网络、语义分割等中得到了广泛应 用，如DCGAN中的生成器通过堆叠转置卷积层实现逐层“放大”特征图，最后获得十分逼真的生成图片。</p>
<h4 id="3-分离卷积">3.分离卷积</h4>
<p>这里以深度可分离卷积(Depth-wise Separable Convolution)为例。 普通卷积在对多通道输入进行运算时，卷积核的每个通道与输入的每个通道分别进行卷积运算，得到多通道的特征图，再对应元素相加产生单个卷积核的最终输出</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201207170349109.png" alt="普通卷积计算"></p>
<p>分离卷积的计算流程则不同，卷积核的每个通道与输入的每个通道进行卷积运算，得到多个通道的中间特征，如下所示。这个多通道的中间特征张量接下来进行多个1×1卷积核的普通卷积运算，得到多个高宽不变的输出，这些输出在通道轴上面进行拼接，从而产生最终的分离卷积层的输出。可以看到，分离卷积层包含了两步卷积运算，第 一步卷积运算是单个卷积核，第二个卷积运算包含了多个卷积核。</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201207170456595.png" alt="深度可分离卷积计算"></p>
<p>同样的输入和输出，采用 Separable Convolution的参数量约是普通卷积的1/3。</p>
<blockquote>
<p>考虑上图中的普通卷积和分离卷积的例子。</p>
<p>普通卷积的参数量是 3·3·3·4=108</p>
<p>分离卷积的第一部分参数量是 3·3·3·1=27。第二部分参数量是 1·1·3·4=14。总参数量只有39，但是却能实现普通卷积同样的输入输出尺寸变换。</p>
<p>分离卷积在Xception和MobileNets等对计算代价敏感的领域中得到了大量应用</p>
</blockquote>
<h3 id="12、深度残差网络">12、深度残差网络</h3>
<p>研究人员发现网络的层数越深，越有可能获得更好的泛化能力。但是当模型加深以后，网络变得越来越难训练，这主要是由于<strong>梯度弥散</strong>和<strong>梯度爆炸</strong>现象造成的。（在较深层数的神经网络中，梯度信息由网络的末层逐层传向网络的首层时，传递的过程中会出现梯度 接近于0或梯度值非常大的现象。网络层数越深，这种现象可能会越严重）</p>
<p>通过在输入和输出之间添加一条直接连接的Skip Connection可以让神经网络具有回退的能力。以VGG13深度神经网络为例，假设观察到VGG13模型出现梯度弥散现象，而 10 层的网络模型并没有观测到梯度弥散现象，那么可以考虑在最后的两个卷积层添加Skip Connection，如下所示。通过这种方式，网络模型可以自动选择是否经由这两个卷积层完成特征变换，还是直接跳过这两个卷积层而选择Skip Connection，亦或结合两个卷积层和Skip Connection的输出。</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201207171857461.png" alt="添加Skip Connection的VGG13"></p>
<p>2015年，微软亚洲研究院何凯明等人发表了基于Skip Connection的深度残差网络 (Residual Neural Network，简称ResNet)算法，并提出了18层、34层、50层、101 层、152层的ResNet-18、ResNet-34、ResNet-50、ResNet-101和ResNet-152等模型，甚至成功训练出层数达到1202层的极深层神经网络。</p>
<h4 id="1-ResNet原理">1.ResNet原理</h4>
<p>ResNet通过在卷积层的输入和输出之间添加Skip Connection实现层数回退机制：</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201207172326171.png" alt="残差模块"></p>
<p>输入x通过两个卷积层，得到特征变换后的输出F(x)，与输入x进行对应元素的相加运算，得到最终输出H(x)：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="script">H</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="bold-italic">x</mi><mo>+</mo><mi mathvariant="script">F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{H}(\boldsymbol{x})=\boldsymbol{x}+\mathcal{F}(\boldsymbol{x})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.00965em;">H</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.09931em;">F</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>叫作残差模块(Residual Block，简称ResBlock)。由于被 Skip Connection 包围的卷积神经网络需要学习映射 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="script">F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="script">H</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>−</mo><mi mathvariant="bold-italic">x</mi></mrow><annotation encoding="application/x-tex">\mathcal{F}(\boldsymbol{x})=\mathcal{H}(\boldsymbol{x})-\boldsymbol{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.09931em;">F</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.00965em;">H</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.44444em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">x</span></span></span></span></span></span> ，故称为残差网络。</p>
<p>为了能够满足输入x与卷积层的输出F(x)能够相加运算，需要输入x的shape与F(x)的 shape 完全一致。当出现shape不一致时，一般通过在 Skip Connection 上添加额外的卷积运算环节将输入x变换到与F(x)相同的shape，如上图中identity(x)函数所示，其中 identity(x)以1x1的卷积运算居多，主要用于调整输入的通道数</p>
<h4 id="2-ResBlock实现">2.ResBlock实现</h4>
<p>深度残差网络并没有增加新的网络层类型，只是通过在输入和输出之间添加一条Skip Connection，因此并没有针对ResNet的底层实现。</p>
<p>在TensorFlow中通过调用普通卷积层即可实现残差模块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicBlock</span>(<span class="params">layers.Layer</span>):</span>  <span class="comment"># 残差模块类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, filter_num, stride=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        <span class="comment"># f(x)包含了 2 个普通卷积层，创建卷积层 1</span></span><br><span class="line">        self.conv1 = layers.Conv2D(filter_num, (<span class="number">3</span>, <span class="number">3</span>), strides=stride, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.bn1 = layers.BatchNormalization()</span><br><span class="line">        self.relu = layers.Activation(<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        <span class="comment"># 创建卷积层 2</span></span><br><span class="line">        self.conv2 = layers.Conv2D(filter_num, (<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">1</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.bn2 = layers.BatchNormalization()</span><br><span class="line">	    <span class="comment"># 当ℱ(𝒙)的形状与𝒙不同时，无法直接相加，需要新建identity(𝒙)卷积层，来完成𝒙的形状转换</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span>:  <span class="comment"># 插入 identity 层</span></span><br><span class="line">            self.downsample = Sequential()</span><br><span class="line">            self.downsample.add(layers.Conv2D(filter_num, (<span class="number">1</span>, <span class="number">1</span>), strides=stride))</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 否则，直接连接</span></span><br><span class="line">            self.downsample = <span class="keyword">lambda</span> x:x</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span>  <span class="comment"># 前向传播函数</span></span><br><span class="line">        out = self.conv1(inputs) <span class="comment"># 通过第一个卷积层</span></span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        out = self.conv2(out) <span class="comment"># 通过第二个卷积层</span></span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        <span class="comment"># 输入通过 identity()转换</span></span><br><span class="line">        identity = self.downsample(inputs)</span><br><span class="line">        <span class="comment"># f(x)+x 运算</span></span><br><span class="line">        output = layers.add([out, identity])</span><br><span class="line">        <span class="comment"># 再通过激活函数并返回</span></span><br><span class="line">        output = tf.nn.relu(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="13、DenseNet">13、DenseNet</h3>
<p>DenseNet将前面所有层的特征图信息通过Skip Connection与当前层输出进行聚合，与ResNet的对应位置相加方式不同，DenseNet采用在通道轴c维度进行拼接操作，聚合特征信息。</p>
<p>如下所示，输入 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">X_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 通过 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">H_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 卷积层得到输出 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">X_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">X_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 与<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">X_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>在通道轴上进行拼接，得到聚合后的特征张量，送入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">H_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>卷积层，得到输出<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">X_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，同样的方法，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">X_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>与前面所有层的特征信息<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">X_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>与<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">X_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>进行聚合，再送入下一层。如此循环，直至最后一层的输出<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mn>4</mn></msub></mrow><annotation encoding="application/x-tex">X_4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和前面所有层的特征信息：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">{</mo><msub><mi>X</mi><mi>i</mi></msub><msub><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mn>3</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\{X_i\}_{i=0,1,2,3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mpunct mtight">,</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>进行聚合得到模块的最终输出。这样一种基于Skip Connection 稠密连接的模块叫做Dense Block</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201208012457182.png" alt="Dense Block结构"></p>
<p>DenseNet 通过堆叠多个 Dense Block 构成复杂的深层神经网络：</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201208012916926.png" alt="典型DenseNet结构"></p>
<h3 id="14、CIFAR10与ResNet18实战">14、CIFAR10与ResNet18实战</h3>
<p>标准的 ResNet18 接受输入为 22 × 22 大小的图片数据，这里将 ResNet18 进行适量调整，使得它输入大小为32 × 32，输出维度为 10。调整后的 ResNet18 网络结构如下所示</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201208013233665.png" alt="调整后的ResNet18"></p>
<p>网络实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> layers, Sequential</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicBlock</span>(<span class="params">layers.Layer</span>):</span>  <span class="comment"># 残差模块</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, filter_num, stride=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        <span class="comment"># 第一个卷积单元</span></span><br><span class="line">        self.conv1 = layers.Conv2D(filter_num, (<span class="number">3</span>, <span class="number">3</span>), strides=stride, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.bn1 = layers.BatchNormalization()</span><br><span class="line">        self.relu = layers.Activation(<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        <span class="comment"># 第二个卷积单元</span></span><br><span class="line">        self.conv2 = layers.Conv2D(filter_num, (<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">1</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">        self.bn2 = layers.BatchNormalization()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span>:<span class="comment"># 通过1x1卷积完成shape匹配</span></span><br><span class="line">            self.downsample = Sequential()</span><br><span class="line">            self.downsample.add(layers.Conv2D(filter_num, (<span class="number">1</span>, <span class="number">1</span>), strides=stride))</span><br><span class="line">        <span class="keyword">else</span>:<span class="comment"># shape匹配，直接短接</span></span><br><span class="line">            self.downsample = <span class="keyword">lambda</span> x:x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span>  <span class="comment"># 前向计算函数</span></span><br><span class="line">        <span class="comment"># [b, h, w, c]，通过第一个卷积单元</span></span><br><span class="line">        out = self.conv1(inputs)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        <span class="comment"># 通过第二个卷积单元</span></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        <span class="comment"># 通过identity模块</span></span><br><span class="line">        identity = self.downsample(inputs)</span><br><span class="line">        <span class="comment"># 2条路径输出直接相加</span></span><br><span class="line">        output = layers.add([out, identity])</span><br><span class="line">        output = tf.nn.relu(output) <span class="comment"># 激活函数</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="comment"># 通用的ResNet实现类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer_dims, num_classes=<span class="number">10</span></span>):</span> <span class="comment"># [2, 2, 2, 2]</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line">        <span class="comment"># 根网络，预处理</span></span><br><span class="line">        self.stem = Sequential([layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">                                layers.BatchNormalization(),</span><br><span class="line">                                layers.Activation(<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                                layers.MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">                                ])</span><br><span class="line">        <span class="comment"># 堆叠4个Block，每个block包含了多个BasicBlock,设置步长不一样</span></span><br><span class="line">        self.layer1 = self.build_resblock(<span class="number">64</span>,  layer_dims[<span class="number">0</span>])</span><br><span class="line">        self.layer2 = self.build_resblock(<span class="number">128</span>, layer_dims[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self.build_resblock(<span class="number">256</span>, layer_dims[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self.build_resblock(<span class="number">512</span>, layer_dims[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过Pooling层将高宽降低为1x1</span></span><br><span class="line">        self.avgpool = layers.GlobalAveragePooling2D()</span><br><span class="line">        <span class="comment"># 最后连接一个全连接层分类</span></span><br><span class="line">        self.fc = layers.Dense(num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># 通过根网络</span></span><br><span class="line">        x = self.stem(inputs)</span><br><span class="line">        <span class="comment"># 一次通过4个模块</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过池化层</span></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        <span class="comment"># 通过全连接层</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_resblock</span>(<span class="params">self, filter_num, blocks, stride=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="comment"># 辅助函数，堆叠filter_num个BasicBlock</span></span><br><span class="line">        res_blocks = Sequential()</span><br><span class="line">        <span class="comment"># 只有第一个BasicBlock的步长可能不为1，实现下采样</span></span><br><span class="line">        res_blocks.add(BasicBlock(filter_num, stride))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, blocks):<span class="comment">#其他BasicBlock步长都为1</span></span><br><span class="line">            res_blocks.add(BasicBlock(filter_num, stride=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res_blocks</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet18</span>():</span></span><br><span class="line">    <span class="comment"># 通过调整模块内部BasicBlock的数量和配置实现不同的ResNet</span></span><br><span class="line">    <span class="keyword">return</span> ResNet([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet34</span>():</span></span><br><span class="line">     <span class="comment"># 通过调整模块内部BasicBlock的数量和配置实现不同的ResNet</span></span><br><span class="line">    <span class="keyword">return</span> ResNet([<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>在设计深度卷积神经网络时，一般按照特征图高宽ℎ/𝑤逐渐减少，通道数𝑐逐渐增大的经验法则</li>
</ul>
<p>训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> layers, optimizers, datasets, Sequential</span><br><span class="line"><span class="keyword">import</span>  os</span><br><span class="line"><span class="keyword">from</span>    resnet <span class="keyword">import</span> resnet18</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>]=<span class="string">&#x27;2&#x27;</span></span><br><span class="line">tf.random.set_seed(<span class="number">2345</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    <span class="comment"># 将数据映射到-1~1</span></span><br><span class="line">    x = <span class="number">2</span>*tf.cast(x, dtype=tf.float32) / <span class="number">255.</span> - <span class="number">1</span></span><br><span class="line">    y = tf.cast(y, dtype=tf.int32) <span class="comment"># 类型转换</span></span><br><span class="line">    <span class="keyword">return</span> x,y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(x,y), (x_test, y_test) = datasets.cifar10.load_data() <span class="comment"># 加载数据集</span></span><br><span class="line">y = tf.squeeze(y, axis=<span class="number">1</span>) <span class="comment"># 删除不必要的维度</span></span><br><span class="line">y_test = tf.squeeze(y_test, axis=<span class="number">1</span>) <span class="comment"># 删除不必要的维度</span></span><br><span class="line">print(x.shape, y.shape, x_test.shape, y_test.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x,y)) <span class="comment"># 构建训练集</span></span><br><span class="line"><span class="comment"># 随机打散，预处理，批量化</span></span><br><span class="line">train_db = train_db.shuffle(<span class="number">1000</span>).<span class="built_in">map</span>(preprocess).batch(<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test)) <span class="comment">#构建测试集</span></span><br><span class="line"><span class="comment"># 随机打散，预处理，批量化</span></span><br><span class="line">test_db = test_db.<span class="built_in">map</span>(preprocess).batch(<span class="number">512</span>)</span><br><span class="line"><span class="comment"># 采样一个样本</span></span><br><span class="line">sample = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_db))</span><br><span class="line">print(<span class="string">&#x27;sample:&#x27;</span>, sample[<span class="number">0</span>].shape, sample[<span class="number">1</span>].shape,</span><br><span class="line">      tf.reduce_min(sample[<span class="number">0</span>]), tf.reduce_max(sample[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># [b, 32, 32, 3] =&gt; [b, 1, 1, 512]</span></span><br><span class="line">    model = resnet18() <span class="comment"># ResNet18网络</span></span><br><span class="line">    model.build(input_shape=(<span class="literal">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>))</span><br><span class="line">    model.summary() <span class="comment"># 统计网络参数</span></span><br><span class="line">    optimizer = optimizers.Adam(lr=<span class="number">1e-4</span>) <span class="comment"># 构建优化器</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>): <span class="comment"># 训练epoch</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> step, (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                <span class="comment"># [b, 32, 32, 3] =&gt; [b, 10],前向传播</span></span><br><span class="line">                logits = model(x)</span><br><span class="line">                <span class="comment"># [b] =&gt; [b, 10],one-hot编码</span></span><br><span class="line">                y_onehot = tf.one_hot(y, depth=<span class="number">10</span>)</span><br><span class="line">                <span class="comment"># 计算交叉熵</span></span><br><span class="line">                loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=<span class="literal">True</span>)</span><br><span class="line">                loss = tf.reduce_mean(loss)</span><br><span class="line">            <span class="comment"># 计算梯度信息</span></span><br><span class="line">            grads = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">            <span class="comment"># 更新网络参数</span></span><br><span class="line">            optimizer.apply_gradients(<span class="built_in">zip</span>(grads, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> step %<span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">                print(epoch, step, <span class="string">&#x27;loss:&#x27;</span>, <span class="built_in">float</span>(loss))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        total_num = <span class="number">0</span></span><br><span class="line">        total_correct = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x,y <span class="keyword">in</span> test_db:</span><br><span class="line"></span><br><span class="line">            logits = model(x)</span><br><span class="line">            prob = tf.nn.softmax(logits, axis=<span class="number">1</span>)</span><br><span class="line">            pred = tf.argmax(prob, axis=<span class="number">1</span>)</span><br><span class="line">            pred = tf.cast(pred, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)</span><br><span class="line">            correct = tf.reduce_sum(correct)</span><br><span class="line"></span><br><span class="line">            total_num += x.shape[<span class="number">0</span>]</span><br><span class="line">            total_correct += <span class="built_in">int</span>(correct)</span><br><span class="line"></span><br><span class="line">        acc = total_correct / total_num</span><br><span class="line">        print(epoch, <span class="string">&#x27;acc:&#x27;</span>, acc)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="七、循环神经网络">七、循环神经网络</h2>
<p>卷积神经网络利用数据的局部相关性和权值共享的思想大大减少了网络的参数量，非常适合于图片这种具有空间(Spatial)局部相关性的数据，已经被成功地应用到计算机视觉领域的一系列任务上。自然界的信号除了具有空间维度之外，还有一个时间（Temporal）维度。具有时间维度的信号非常常见，比如我们正在阅读的文本、说话时发出的语音信号、随着时间变化的股市参数等。这类数据并不一定具有局部相关性，同时数据在时间维度上的长度也是可变的，卷积神经网络并不擅长处理此类数据</p>
<h3 id="1、序列表示方法">1、序列表示方法</h3>
<p>能够直接用一个标量数值表示出来的可以通过一个shape：[b,s]来表示，其中b为序列数量，s为序列长度。但是很多信号并不能用一个标量数值来表示，如每个时间戳产生长度为n的特征向量，此时着需要shape为[b,s,n]的张量来表示</p>
<p>而更复杂的文本数据着不能通过某个标量来表示。对于一本含有n个单词的句子，可以通过前面介绍的One-hot编码来表示，如下所示为𝑛个地名的单词，可以将每个地名编码为长度为𝑛的 Onehot 向量：</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201208030032423.png" alt="地名One-hot编码方案"></p>
<p>把文字编码为数值的过程叫作Word Embedding。One-hot的编码方式实现Word Embedding简单直观，编码过程不需要学习和训练。但One-hot编码的向量是高维度而且极其稀疏的，大量的位置为0，计算效率较低，同时也不利于神经网络的训练。从语义角度来讲，One-hot编码还忽略了单词先天具有的语义相关性，不能很好地体现原有文字的语义相关度。</p>
<p>在自然语言处理领域，有专门的一个研究方向在探索如何学习到单词的表示向量（Word Vector)，使得语义层面的相关性能够很好地通过Word Vector体现出来。一个衡量词向量之间相关度的方法就是余弦相关度（Cosine similarity)：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">similarity</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">a</mi><mo separator="true">,</mo><mi mathvariant="bold-italic">b</mi><mo stretchy="false">)</mo><mo>≜</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">a</mi><mo>⋅</mo><mi mathvariant="bold-italic">b</mi></mrow><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="bold-italic">a</mi><mi mathvariant="normal">∣</mi><mo>⋅</mo><mi mathvariant="normal">∣</mi><mi mathvariant="bold-italic">b</mi><mi mathvariant="normal">∣</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\operatorname{similarity}(\boldsymbol{a}, \boldsymbol{b}) \triangleq \cos (\theta)=\frac{\boldsymbol{a} \cdot \boldsymbol{b}}{|\boldsymbol{a}| \cdot|\boldsymbol{b}|}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1666699999999999em;vertical-align:-0.25em;"></span><span class="mop"><span class="mord mathrm">s</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">i</span><span class="mord mathrm">l</span><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">i</span><span class="mord mathrm">t</span><span class="mord mathrm" style="margin-right:0.01389em;">y</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol">a</span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">b</span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel amsrm">≜</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.30744em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mord"><span class="mord"><span class="mord boldsymbol">a</span></span></span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">∣</span><span class="mord"><span class="mord"><span class="mord boldsymbol">b</span></span></span><span class="mord">∣</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">a</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">b</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>其中a和b代表了两个词向量。下图演示了单词“France”和“Italy”的相似度，以及单词“ball”和“crocodile”的相似度，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span>为两个词向量之间的夹角。可以看到余弦相关度较好地反映了语义相关性。</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20201208030338032.png" alt="余弦相似度示意图"></p>
<h4 id="1-Embedding层">1.Embedding层</h4>
<p>在神经网络中，单词的表示向量可以直接通过训练的方式得到，我们把单词的表示层叫作Embedding层。Embedding层负责把单词编码为某个词向量v，它接受的是采用数字编码的单词编号<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span>（如2表示“I”，3表示“me”等），系统总单词数量记为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi></mrow></msub></mrow><annotation encoding="application/x-tex">N_{vocab}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，输出长度为n的向量v：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="bold-italic">v</mi><mo>=</mo><msub><mi>f</mi><mi>θ</mi></msub><mrow><mo fence="true">(</mo><mi>i</mi><mo>∣</mo><msub><mi>N</mi><mrow><mi mathvariant="normal">v</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi></mrow></msub><mo separator="true">,</mo><mi>n</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\boldsymbol{v}=f_{\theta}\left(i \mid N_{\mathrm{vocab}}, n\right)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.44444em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight" style="margin-right:0.01389em;">v</span><span class="mord mathrm mtight">o</span><span class="mord mathrm mtight">c</span><span class="mord mathrm mtight">a</span><span class="mord mathrm mtight">b</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span></span></p>
<p>Embedding 层实现起来非常简单，构建一个 shape 为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><msub><mi>𝑁</mi><mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi></mrow></msub><mo separator="true">,</mo><mi>𝑛</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[𝑁_{vocab}, 𝑛]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mclose">]</span></span></span></span>的查询表对象 table，对于任意的单词编号𝑖，只需要查询到对应位置上的向量并返回即可：</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>𝒗</mi><mo>=</mo><mi>𝑡</mi><mi>𝑎</mi><mi>𝑏</mi><mi>𝑙</mi><mi>𝑒</mi><mo stretchy="false">[</mo><mi>𝑖</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">𝒗 = 𝑡𝑎𝑏𝑙𝑒[𝑖]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.44444em;vertical-align:0em;"></span><span class="mord boldsymbol" style="margin-right:0.11111em;">v</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault">b</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">e</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span></span></span></span></p>
<p>Embedding 层是可训练的，它可放置在神经网络之前，完成单词到向量的转换，得到的表示向量可以继续通过神经网络完成后续任务，并计算误差ℒ，采用梯度下降算法来实现端到端(end-to-end)的训练。</p>
<p>在 TensorFlow 中，可以通过 layers.Embedding(𝑁vocab,𝑛)来定义一个 Word Embedding 层：</p>
<ul>
<li>𝑁vocab：词汇数量</li>
<li>𝑛：单词向量的长度</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = tf.<span class="built_in">range</span>(<span class="number">10</span>) <span class="comment"># 生成 10 个单词的数字编码</span></span><br><span class="line">x = tf.random.shuffle(x) <span class="comment"># 打散</span></span><br><span class="line"><span class="comment"># 创建共 10 个单词，每个单词用长度为 4 的向量表示的层</span></span><br><span class="line">net = layers.Embedding(<span class="number">10</span>, <span class="number">4</span>)</span><br><span class="line">out = net(x) <span class="comment"># 获取词向量</span></span><br><span class="line"><span class="comment"># 些词向量随机初始化的，尚未经过网络训练</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">96</span>, shape=(<span class="number">10</span>, <span class="number">4</span>), dtype=float32, numpy=</span><br><span class="line">array([[-<span class="number">0.00998075</span>, -<span class="number">0.04006485</span>, <span class="number">0.03493755</span>, <span class="number">0.03328368</span>],</span><br><span class="line"> [-<span class="number">0.04139598</span>, -<span class="number">0.02630153</span>, -<span class="number">0.01353856</span>, <span class="number">0.02804044</span>],…</span><br><span class="line"></span><br><span class="line">net.embeddings <span class="comment"># 直接查看 Embedding 层内部的查询表 table</span></span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;embedding_4/embeddings:0&#x27;</span> shape=(<span class="number">10</span>, <span class="number">4</span>) dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">0.04112223</span>, <span class="number">0.01824595</span>, -<span class="number">0.01841902</span>, <span class="number">0.00482471</span>],</span><br><span class="line"> [-<span class="number">0.00428962</span>, -<span class="number">0.03172196</span>, -<span class="number">0.04929272</span>, <span class="number">0.04603403</span>],…</span><br><span class="line"></span><br><span class="line">net.embeddings.trainable <span class="comment"># 查看 net.embeddings 张量的可优化属性</span></span><br><span class="line"><span class="literal">True</span><span class="comment"># 可以通过梯度下降算法优化</span></span><br></pre></td></tr></table></figure>
<h4 id="2-预训练的词向量">2.预训练的词向量</h4>
<p>Embedding 层的查询表是随机初始化的，需要从零开始训练。实际上可以使用预训练的 Word Embedding 模型来得到单词的表示方法，基于预训练模型的词向量相当于 迁移了整个语义空间的知识，往往能得到更好的性能。</p>
<p>目前应用的比较广泛的预训练模型有 Word2Vec 和 GloVe 等。它们已经在海量语料库训练得到了较好的词向量表示方法，并可以直接导出学习到的词向量表，方便迁移到其它任务。如 GloVe 模型 GloVe.6B.50d，词汇量为 40 万，每个单词使用长度为 50 的向量表 示，用户只需要下载对应的模型文件即可，“glove6b50dtxt.zip”模型文件约 69MB。</p>
<p>对于 Embedding 层，不再采用随机初始化的方式，而是利用已经预训练好的模型参数去初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从预训练模型中加载词向量表</span></span><br><span class="line">embed_glove = load_embed(<span class="string">&#x27;glove.6B.50d.txt&#x27;</span>)</span><br><span class="line"><span class="comment"># 直接利用预训练的词向量表初始化 Embedding 层</span></span><br><span class="line">net.set_weights([embed_glove])</span><br></pre></td></tr></table></figure>
<p>经过预训练的词向量模型初始化的 Embedding 层可以设置为不参与训练：<code>net.trainable  = False</code>，那么预训练的词向量就直接应用到此特定任务上。</p>
<p>如果希望能够学到区别于预训练词向量模型不同的表示方法，那么可以把 Embedding 层包含进反向传播算法中去，利用梯度下降来微调单词表示方法。</p>
<h3 id="2、循环卷积网络">2、循环卷积网络</h3>
<p>现在来考虑处理一个序列信号，以文本序列为例，考虑一个句子：</p>
<p>“I hate this boring movie”</p>
<p>通过 Embedding 层，可以将它转换为 shape 为[𝑏, 𝑠, 𝑛]的张量（上述句子可以表示为 shape 为[1,5,10]的张量）：</p>
<ul>
<li>𝑏：句子数量</li>
<li>𝑠：句子长 度</li>
<li>𝑛：词向量长度</li>
</ul>
<p>接下来逐步探索能够处理序列信号的网络模型，为了便于表达，我们以情感分类任务为例，如下图所示。</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20210408115031465.png" alt="感情分类任务"></p>
<p>情感分类任务通过分析给出的文本序列，提炼出文本数据表达的整体语义特征，从而预测输入文本的情感类型：正面评价或者负面评价。但是由于输入是文本序列（与图片分类不一样），传统的卷积神经网络并不能取得很好的效果（没有考虑时间的连续）</p>
<h4 id="1-全连接层？">1.全连接层？</h4>
<p>如果对于每个词向量，分别使用一个全连接层网络 𝒐 = 𝜎(𝑾_𝑡𝒙_𝑡 + 𝒃_𝑡) 提取语义特征，各个单词的词向量通过𝑠个全连接层分类网络 1 提取每个单词的特征，所有单词的特征最后合并，并通过分类网络 2 输出序列的类别概率分布。但是：</p>
<ol>
<li>对长度为𝑠的句子来说，至少需要𝑠个全网络层，参数量较大，且每个序列长度s并不相同，网络是动态变化的</li>
<li>每个全连接层子网络<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑾</mi><mi>𝑖</mi></msub></mrow><annotation encoding="application/x-tex">𝑾_𝑖</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83611em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.18625em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.18625em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝒃</mi><mi>𝑖</mi></msub></mrow><annotation encoding="application/x-tex">𝒃_𝑖</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.07861em;">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07861em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>只能感受当前词向量的输入，并不能感知之前和之后的语境信息，导致句子整体语义的缺失，每个子网络只能根据自己的输入来提取高层特征</li>
</ol>
<h4 id="2-共享权值">2.共享权值</h4>
<p>在处理序列信号的问题上，可以借助卷积神经网络中共享权值的思想来减少网络的参数量。</p>
<p>𝑠个全连接层的网络并没有实现权值同享。这里将这𝑠个网络层参数共享，这样其实相当于使用一个全连接网络来提取所有单词的特征信息：</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20210408120103292.png" alt="网络模型"></p>
<p>但是这并没有考虑序列之间的先后顺序，将词向量打乱次序仍然能获得相同的输出，无法获取有效的全局语义信息</p>
<h4 id="3-全局语义">3.全局语义</h4>
<p>可以利用内存(Memory)机制解决该问题：网络提供一个单独的内存变量，每次提取词向量的特征并刷新内存变量，直至最后一个输入完成，此时的内存变量即存储了所有序列的语义特征，并且由于输入序列之间的先后顺序，使得内存变量内容与序列顺序紧密关联。</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20210408120335596.png" alt="循环神经网络（未添加偏置）"></p>
<p>将上述 Memory 机制实现为一个状态张量 ，如上所示，除了原来的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑾</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub></mrow><annotation encoding="application/x-tex">𝑾_{xh}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83611em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.18625em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.18625em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>参数共享外，还额外增加了一个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑾</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub></mrow><annotation encoding="application/x-tex">𝑾_{hh}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83611em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.18625em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.18625em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>参数，每个时间戳𝑡上状态张量刷新机制为：</p>
<p>h_{𝑡} = 𝜎(𝑾_{xh}x_t + 𝑾_{hh}h_{𝑡−1} + 𝒃)</p>
<p>其中状态张量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">h_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>为初始的内存状态，可以初始化为全0，经过𝑠个词向量的输入后得到网络最终的状态张量 𝑠， 𝑠较好地代表了句子的全局语义信息，基于 𝑠通过某个全连接层分类器即可完成情感分类任务。</p>
<h4 id="4-循环神经网络">4.循环神经网络</h4>
<p>进一步抽象，如下所示，在每个时间戳𝑡，网络层接受当前时间戳的输入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>𝑡</mi></msub></mrow><annotation encoding="application/x-tex">x_𝑡</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和上一个时间戳的网络状态向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mrow><mi>𝑡</mi><mi mathvariant="normal">−</mi><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{𝑡−1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.902771em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>，经过</p>
<p>h_𝑡 = 𝑓_𝜃(h_{𝑡−1},x_𝑡)</p>
<p>变换后得到当前时间戳的新状态向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>𝑡</mi></msub></mrow><annotation encoding="application/x-tex">h_𝑡</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，并写入内存状态中，其中：</p>
<ul>
<li>𝑓_𝜃：网络的运算逻辑</li>
<li>𝜃：网络参数集</li>
</ul>
<p>在每个时间戳上，网络层均有输出产生<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝒐</mi><mi>𝑡</mi></msub></mrow><annotation encoding="application/x-tex">𝒐_𝑡</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.59444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.07861em;">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07861em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，𝒐_𝑡 = 𝑔_𝜙(h_𝑡)，即将网络的状态向量变换后输出</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20210408120831135.png" alt="展开的RNN模型"></p>
<p>上述网络结构在时间戳上折叠，网络循环接受序列的每个特征向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">𝒙_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.59444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.12583em;">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.12583em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，并刷新内部状态向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>𝑡</mi></msub></mrow><annotation encoding="application/x-tex">h_𝑡</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，同时形成输出<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝒐</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">𝒐_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.59444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.07861em;">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07861em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。对于这种网络结构，称为循环网络结构(Recurrent Neural Network，简称 RNN)：</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20210408121449167.png" alt="折叠的RNN模型"></p>
<p>更特别地，如果使用张量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑾</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub></mrow><annotation encoding="application/x-tex">𝑾_{xh}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83611em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.18625em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.18625em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑾</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub></mrow><annotation encoding="application/x-tex">𝑾_{hh}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83611em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.18625em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.18625em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和偏置𝒃来参数化𝑓_𝜃网络，并按照</p>
<p>h_𝑡 = 𝜎(𝑾_{xh}x_𝑡 + 𝑾_{hh}h_{𝑡−1} + 𝒃)</p>
<p>方式更新内存状态，我们把这种网络叫做<strong>基本的循环神经网络</strong>，如无特别说明，一般说的循环神经网络即指这种实现。</p>
<p>在循环神经网络中，激活函数更多地采用 <code>tanh</code> 函数，并且可以选择不使用偏执𝒃来进一步减少参数量。状态向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>𝑡</mi></msub></mrow><annotation encoding="application/x-tex">h_𝑡</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>可以直接用作输出，即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝒐</mi><mi>𝑡</mi></msub><mo>=</mo><msub><mi>h</mi><mi>𝑡</mi></msub></mrow><annotation encoding="application/x-tex">𝒐_𝑡 = h_𝑡</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.59444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.07861em;">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07861em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，也可以对 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>𝑡</mi></msub></mrow><annotation encoding="application/x-tex">h_𝑡</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>做一个简单的线性变换<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝒐</mi><mi>𝑡</mi></msub><mo>=</mo><msub><mi>𝑊</mi><mrow><mi>h</mi><mi>o</mi></mrow></msub><msub><mi>h</mi><mi>𝑡</mi></msub></mrow><annotation encoding="application/x-tex">𝒐_𝑡 = 𝑊_{ho}h_𝑡</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.59444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.07861em;">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07861em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">o</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>后得到每个时间戳上的网络输出<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝒐</mi><mi>𝑡</mi></msub></mrow><annotation encoding="application/x-tex">𝒐_𝑡</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.59444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.07861em;">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07861em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<h3 id="3、梯度传播">3、梯度传播</h3>
<p>通过循环神经网络的更新表达式可以看出输出对张量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑾</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub></mrow><annotation encoding="application/x-tex">𝑾_{xh}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83611em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.18625em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.18625em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑾</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub></mrow><annotation encoding="application/x-tex">𝑾_{hh}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83611em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.18625em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.18625em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和偏置𝒃均是可导的， 可以利用自动梯度算法来求解网络的梯度。</p>
<p>需要注意的是在推导 \frac{𝜕ℒ}{𝜕𝑾_{ℎℎ}}的过程中发现，\frac{𝜕h_𝑡}{𝜕h_𝑖} 的梯度包含了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑾</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub></mrow><annotation encoding="application/x-tex">𝑾_{hh}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83611em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.18625em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.18625em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的连乘运算，这是导致循环神经网络训练困难的根本原因</p>
<h3 id="4、RNN层使用方法">4、RNN层使用方法</h3>
<p>在 TensorFlow 中，可以通过 <code>layers.SimpleRNNCell</code> 来完成𝜎(𝑾_{xh}x_𝑡 + 𝑾_{hh}h_{𝑡−1} + 𝒃)计算。</p>
<p>需要注意的是，在 TensorFlow 中，RNN 表示通用意义上的循环神经网络，对于我们目前介绍的基础循环神经网络，它一般叫做 SimpleRNN。</p>
<p>SimpleRNN 与 SimpleRNNCell 的区别：</p>
<ul>
<li>SimpleRNNCell层：仅仅完成了一个时间戳的前向运算</li>
<li>SimpleRNN层：一般是基于 Cell 层实现的，在内部已经完成了多个时间戳的循环运算</li>
</ul>
<h4 id="1-SimpleRNNCell">1.SimpleRNNCell</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cell = layers.SimpleRNNCell(<span class="number">3</span>) <span class="comment"># 创建 RNN Cell，内存向量h长度为 3</span></span><br><span class="line">cell.build(input_shape=(<span class="literal">None</span>,<span class="number">4</span>)) <span class="comment"># 输出特征长度 n=4</span></span><br><span class="line">cell.trainable_variables <span class="comment"># 打印 wxh, whh, b 张量</span></span><br><span class="line"></span><br><span class="line">[&lt;tf.Variable <span class="string">&#x27;kernel:0&#x27;</span> shape=(<span class="number">4</span>, <span class="number">3</span>) dtype=float32, numpy=…&gt;,</span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;recurrent_kernel:0&#x27;</span> shape=(<span class="number">3</span>, <span class="number">3</span>) dtype=float32, numpy=…&gt;,</span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;bias:0&#x27;</span> shape=(<span class="number">3</span>,) dtype=float32, numpy=array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>], </span><br><span class="line">dtype=float32)&gt;]</span><br></pre></td></tr></table></figure>
<p>SimpleRNNCell 内部维护了 3 个张量：</p>
<ul>
<li>kernel：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑾</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub></mrow><annotation encoding="application/x-tex">𝑾_{xh}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83611em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.18625em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.18625em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>张量</li>
<li>recurrent_kernel：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑾</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub></mrow><annotation encoding="application/x-tex">𝑾_{hh}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83611em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.18625em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.18625em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>张量</li>
<li>bias：偏置𝒃向量</li>
</ul>
<p>但是 RNN 的 Memory 向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">h</span></span></span></span>需要用户自行初始化并记录每个时间戳上的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<p>通过调用 Cell 实例即可完成前向运算：</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝒐</mi><mi>𝑡</mi></msub><mo separator="true">,</mo><mo stretchy="false">[</mo><msub><mi>h</mi><mi>𝑡</mi></msub><mo stretchy="false">]</mo><mo>=</mo><mi>C</mi><mi>e</mi><mi>l</mi><mi>l</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>𝑡</mi></msub><mo separator="true">,</mo><mo stretchy="false">[</mo><msub><mi>h</mi><mrow><mi>𝑡</mi><mi mathvariant="normal">−</mi><mn>1</mn></mrow></msub><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">𝒐_𝑡,[h_𝑡] = Cell(x_𝑡,[h_{𝑡−1}])</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.07861em;">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07861em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">]</span><span class="mclose">)</span></span></span></span></p>
<p>对于 SimpleRNNCell 来说：</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝒐</mi><mi>𝑡</mi></msub><mo>=</mo><msub><mi>h</mi><mi>𝑡</mi></msub></mrow><annotation encoding="application/x-tex">𝒐_𝑡 = h_𝑡</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.59444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.07861em;">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07861em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，并没有经过额外的线性层转换，是<strong>同一个对象</strong></li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>通过一个 List 包裹起来，是为了与 LSTM、GRU 等 RNN 变种格式统一。</li>
<li>在循环神经网络的初始化阶段，状态向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">h_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>一般初始化为全 0 向量</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化状态向量，用列表包裹，统一格式</span></span><br><span class="line">h0 = [tf.zeros([<span class="number">4</span>, <span class="number">64</span>])]</span><br><span class="line">x = tf.random.normal([<span class="number">4</span>, <span class="number">80</span>, <span class="number">100</span>]) <span class="comment"># 生成输入张量[b,s,n]，4 个 80 单词的句子</span></span><br><span class="line">xt = x[:,<span class="number">0</span>,:] <span class="comment"># 所有句子的第 1 个单词</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建输入特征 n=100,序列长度 s=80,状态长度=64 的 Cell</span></span><br><span class="line">cell = layers.SimpleRNNCell(<span class="number">64</span>)</span><br><span class="line">out, h1 = cell(xt, h0) <span class="comment"># 前向计算</span></span><br><span class="line">print(out.shape, h1[<span class="number">0</span>].shape)</span><br><span class="line">(<span class="number">4</span>, <span class="number">64</span>) (<span class="number">4</span>, <span class="number">64</span>) <span class="comment"># 两者shape一致</span></span><br><span class="line">print(<span class="built_in">id</span>(out), <span class="built_in">id</span>(h1[<span class="number">0</span>])) </span><br><span class="line"><span class="number">2154936585256</span> <span class="number">2154936585256</span> <span class="comment"># 两者 id 一致，即状态向量直接作为输出向量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于长度为𝑠的序列来说，需要循环通过 Cell 类𝑠次才算完成一次网络层的前向运算</span></span><br><span class="line">h = h0 <span class="comment"># h 保存每个时间戳上的状态向量列表</span></span><br><span class="line"><span class="comment"># 在序列长度的维度解开输入，得到 xt:[b,n]</span></span><br><span class="line"><span class="keyword">for</span> xt <span class="keyword">in</span> tf.unstack(x, axis=<span class="number">1</span>):</span><br><span class="line"> out, h = cell(xt, h) <span class="comment"># 前向计算,out 和 h 均被覆盖</span></span><br><span class="line"><span class="comment"># 最终输出可以聚合每个时间戳上的输出，也可以只取最后时间戳的输出</span></span><br><span class="line">out = out</span><br></pre></td></tr></table></figure>
<h4 id="2-多层SimpleRNNCell网络">2.多层SimpleRNNCell网络</h4>
<p>和卷积神经网络动辄几十、上百的深度层数来比，循环神经网络很容易出现梯度弥散和梯度爆炸到现象，深层的循环神经网络训练起来非常困难，目前常见的循环神经网络模型层数一般控制在十层以内。</p>
<p>2层SimpleRNNCell网络例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">80</span>,<span class="number">100</span>])</span><br><span class="line">xt = x[:,<span class="number">0</span>,:] <span class="comment"># 取第一个时间戳的输入 x0</span></span><br><span class="line"><span class="comment"># 构建 2 个 Cell,先 cell0,后 cell1，内存状态向量长度都为 64</span></span><br><span class="line">cell0 = layers.SimpleRNNCell(<span class="number">64</span>)</span><br><span class="line">cell1 = layers.SimpleRNNCell(<span class="number">64</span>)</span><br><span class="line">h0 = [tf.zeros([<span class="number">4</span>,<span class="number">64</span>])] <span class="comment"># cell0 的初始状态向量</span></span><br><span class="line">h1 = [tf.zeros([<span class="number">4</span>,<span class="number">64</span>])] <span class="comment"># cell1 的初始状态向量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式1：先完成一个时间戳上的输入在所有层上的传播，再循环计算完所有时间戳上的输入</span></span><br><span class="line"><span class="keyword">for</span> xt <span class="keyword">in</span> tf.unstack(x, axis=<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># xt 作为输入，输出为 out0</span></span><br><span class="line">    out0, h0 = cell0(xt, h0)</span><br><span class="line">    <span class="comment"># 上一个 cell 的输出 out0 作为本 cell 的输入</span></span><br><span class="line">    out1, h1 = cell1(out0, h1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2：先完成输入在第一层上所有时间戳的计算，并保存第一层在所有时间戳上的输出列表，再计算第二层、第三层等的传播</span></span><br><span class="line"><span class="comment"># 保存上一层的所有时间戳上面的输出</span></span><br><span class="line">middle_sequences = []</span><br><span class="line"><span class="comment"># 计算第一层的所有时间戳上的输出，并保存</span></span><br><span class="line"><span class="keyword">for</span> xt <span class="keyword">in</span> tf.unstack(x, axis=<span class="number">1</span>):</span><br><span class="line">    out0, h0 = cell0(xt, h0)</span><br><span class="line">    middle_sequences.append(out0)</span><br><span class="line"><span class="comment"># 计算第二层的所有时间戳上的输出</span></span><br><span class="line"><span class="comment"># 如果不是末层，需要保存所有时间戳上面的输出</span></span><br><span class="line"><span class="keyword">for</span> xt <span class="keyword">in</span> middle_sequences:</span><br><span class="line">    out1, h1 = cell1(xt, h1)</span><br></pre></td></tr></table></figure>
<p>一般来说，最末层 Cell 的状态有可能保存了高层的全局语义特征，因此一般使用最末层的输出作为后续任务网络的输入。</p>
<p>更特别地，每层最后一个时间戳上的状态输出包含了整个序列的全局信息，如果只希望选用一个状态变量来完成后续任务，比如情感分类问题，一般选用最末层、最末时间戳的状态输出最为合适。</p>
<h4 id="3-SimpleRNN层">3.SimpleRNN层</h4>
<p>如单层循环神经网络的前向运算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer = layers.SimpleRNN(<span class="number">64</span>) <span class="comment"># 创建状态向量长度为 64 的 SimpleRNN 层</span></span><br><span class="line">x = tf.random.normal([<span class="number">4</span>, <span class="number">80</span>, <span class="number">100</span>])</span><br><span class="line">out = layer(x) <span class="comment"># 和普通卷积网络一样，一行代码即可获得输出（默认返回最后一个时间戳上的输出）</span></span><br><span class="line">out.shape</span><br><span class="line"></span><br><span class="line">TensorShape([<span class="number">4</span>, <span class="number">64</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>如果希望返回所有时间戳上的输出列表，可以设置 <code>return_sequences=True</code> 参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 RNN 层时，设置返回所有时间戳上的输出</span></span><br><span class="line">layer = layers.SimpleRNN(<span class="number">64</span>,return_sequences=<span class="literal">True</span>)</span><br><span class="line">out = layer(x) <span class="comment"># 前向计算</span></span><br><span class="line">out <span class="comment"># 输出，自动进行了 concat 操作</span></span><br><span class="line"></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">12654</span>, shape=(<span class="number">4</span>, <span class="number">80</span>, <span class="number">64</span>), dtype=float32, numpy=</span><br><span class="line">array([[[ <span class="number">0.31804922</span>, <span class="number">0.7904409</span> , <span class="number">0.13204293</span>, ..., <span class="number">0.02601025</span>,</span><br><span class="line"> -<span class="number">0.7833339</span> , <span class="number">0.65577114</span>],…&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>中间维度的 80 即为时间戳维度</li>
</ul>
</li>
</ul>
<p>于多层循环神经网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = keras.Sequential([ <span class="comment"># 构建 2 层 RNN 网络</span></span><br><span class="line">    <span class="comment"># 除最末层外，都需要返回所有时间戳的输出，用作下一层的输入</span></span><br><span class="line">    layers.SimpleRNN(<span class="number">64</span>, return_sequences=<span class="literal">True</span>),</span><br><span class="line">    layers.SimpleRNN(<span class="number">64</span>),</span><br><span class="line">])</span><br><span class="line">out = net(x) <span class="comment"># 前向计算</span></span><br></pre></td></tr></table></figure>
<h3 id="5、RNN感情分类问题实战">5、RNN感情分类问题实战</h3>
<p>待补充</p>
<h3 id="6、梯度弥漫与梯度爆炸">6、梯度弥漫与梯度爆炸</h3>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">spaceman</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://nu-ll.github.io/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/">http://nu-ll.github.io/2020/01/03/深度学习与TensorFlow2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://NU-LL.github.io" target="_blank">spaceman</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/TensorFlow2/">TensorFlow2</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/01/10/python/"><img class="prev-cover" src="https://gitee.com/NU-LL/image-host/raw/master/12.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Python</div></div></a></div><div class="next-post pull-right"><a href="/2019/11/12/%E5%9F%BA%E4%BA%8ESTM32L476%E7%9A%84IAP%E5%8D%87%E7%BA%A7/"><img class="next-cover" src="https://gitee.com/NU-LL/image-host/raw/master/12.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">基于STM32L476的IAP升级</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/05/03/TensorFlow2/" title="TensorFlow2入门与实践"><img class="cover" src="https://gitee.com/NU-LL/image-host/raw/master/139-150515124111.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2020-05-03</div><div class="title">TensorFlow2入门与实践</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/spaceman.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">spaceman</div><div class="author-info__description">CtrlC CtrlV大师</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">79</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">85</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/NU-LL"><i class="fab fa-github"></i><span>Github</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">白嫖一时爽，一直白嫖一直爽</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">深度学习与TensorFlow2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81TensorFlow2%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C"><span class="toc-text">一、TensorFlow2基础操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-text">1、数据类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%95%B0%E5%80%BC%E7%B1%BB%E5%9E%8B"><span class="toc-text">1.数值类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B"><span class="toc-text">2.字符串类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%B8%83%E5%B0%94%E7%B1%BB%E5%9E%8B"><span class="toc-text">3.布尔类型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E6%95%B0%E5%80%BC%E7%B2%BE%E5%BA%A6"><span class="toc-text">2、数值精度</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%AF%BB%E5%8F%96%E7%B2%BE%E5%BA%A6"><span class="toc-text">1.读取精度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="toc-text">2.类型转换</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E5%BE%85%E4%BC%98%E5%8C%96%E5%BC%A0%E9%87%8F"><span class="toc-text">3、待优化张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F"><span class="toc-text">4、创建张量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BB%8E%E6%95%B0%E7%BB%84%E3%80%81%E5%88%97%E8%A1%A8%E4%B8%AD%E5%88%9B%E5%BB%BA"><span class="toc-text">1.从数组、列表中创建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%88%9B%E5%BB%BA%E5%85%A80%E5%85%A81%E5%BC%A0%E9%87%8F"><span class="toc-text">2.创建全0全1张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%88%9B%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E5%80%BC%E5%BC%A0%E9%87%8F"><span class="toc-text">3.创建自定义数值张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%88%9B%E5%BB%BA%E5%B7%B2%E7%9F%A5%E5%88%86%E5%B8%83%E7%9A%84%E5%BC%A0%E9%87%8F"><span class="toc-text">4.创建已知分布的张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E5%88%9B%E5%BB%BA%E5%BA%8F%E5%88%97"><span class="toc-text">5.创建序列</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E5%BC%A0%E9%87%8F%E7%9A%84%E5%85%B8%E5%9E%8B%E7%94%A8%E9%80%94"><span class="toc-text">5、张量的典型用途</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%E3%80%81%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%88%87%E7%89%87"><span class="toc-text">6、索引与切片</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%B4%A2%E5%BC%95"><span class="toc-text">1.索引</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%88%87%E7%89%87"><span class="toc-text">2.切片</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7%E3%80%81%E7%BB%B4%E5%BA%A6%E5%8F%98%E6%8D%A2"><span class="toc-text">7、维度变换</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%94%B9%E5%8F%98%E8%A7%86%E5%9B%BE-reshape"><span class="toc-text">1.改变视图 reshape</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%A2%9E%E5%88%A0%E7%BB%B4%E5%BA%A6"><span class="toc-text">2.增删维度</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A2%9E%E5%8A%A0%E7%BB%B4%E5%BA%A6"><span class="toc-text">增加维度</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%A0%E9%99%A4%E7%BB%B4%E5%BA%A6"><span class="toc-text">删除维度</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E4%BA%A4%E6%8D%A2%E7%BB%B4%E5%BA%A6"><span class="toc-text">3.交换维度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%A4%8D%E5%88%B6%E6%95%B0%E6%8D%AE"><span class="toc-text">4.复制数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8%E3%80%81Broadcasting"><span class="toc-text">8、Broadcasting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9%E3%80%81%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="toc-text">9、数学运算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%8A%A0%E5%87%8F%E4%B9%98%E9%99%A4"><span class="toc-text">1.加减乘除</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%B9%98%E6%96%B9"><span class="toc-text">2.乘方</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%8C%87%E6%95%B0%E5%92%8C%E5%AF%B9%E6%95%B0"><span class="toc-text">3.指数和对数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="toc-text">4.矩阵乘法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81TensorFlow2%E8%BF%9B%E9%98%B6%E6%93%8D%E4%BD%9C"><span class="toc-text">二、TensorFlow2进阶操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2"><span class="toc-text">1、合并与分割</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%90%88%E5%B9%B6"><span class="toc-text">1.合并</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8B%BC%E6%8E%A5"><span class="toc-text">拼接</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A0%86%E5%8F%A0"><span class="toc-text">堆叠</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%88%86%E5%89%B2"><span class="toc-text">2.分割</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1"><span class="toc-text">2、数据统计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%90%91%E9%87%8F%E8%8C%83%E6%95%B0"><span class="toc-text">1.向量范数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%9C%80%E5%80%BC%E3%80%81%E5%9D%87%E5%80%BC%E3%80%81%E5%92%8C"><span class="toc-text">2.最值、均值、和</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E5%BC%A0%E9%87%8F%E6%AF%94%E8%BE%83"><span class="toc-text">3、张量比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E5%A4%8D%E5%88%B6%E5%92%8C%E5%A1%AB%E5%85%85"><span class="toc-text">4、复制和填充</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%A1%AB%E5%85%85"><span class="toc-text">1.填充</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%A4%8D%E5%88%B6"><span class="toc-text">2.复制</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E6%95%B0%E6%8D%AE%E9%99%90%E5%B9%85"><span class="toc-text">5、数据限幅</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%E3%80%81%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C"><span class="toc-text">6、高级操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-tf-gather"><span class="toc-text">1.tf.gather</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-tf-gather-nd"><span class="toc-text">2.tf.gather_nd</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-tf-boolean-mask"><span class="toc-text">3.tf.boolean_mask</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-tf-where"><span class="toc-text">4.tf.where</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-text">例子</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-scatter-nd"><span class="toc-text">5.scatter_nd</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-meshgrid"><span class="toc-text">6.meshgrid</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7%E3%80%81%E7%BB%8F%E5%85%B8%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8A%A0%E8%BD%BD"><span class="toc-text">7、经典数据集加载</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%9A%8F%E6%9C%BA%E6%89%93%E6%95%A3"><span class="toc-text">1.随机打散</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%89%B9%E8%AE%AD%E7%BB%83"><span class="toc-text">2.批训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">3.预处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%BE%AA%E7%8E%AF%E8%AE%AD%E7%BB%83"><span class="toc-text">4.循环训练</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8%E3%80%81MNIST%E5%AE%9E%E6%88%98"><span class="toc-text">8、MNIST实战</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">三、神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-text">1、感知机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="toc-text">2、全连接层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%BC%A0%E9%87%8F%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0"><span class="toc-text">1.张量方式实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%B1%82%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0"><span class="toc-text">2.层方式实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">3、神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%BC%A0%E9%87%8F%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0-2"><span class="toc-text">1.张量方式实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%B1%82%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0-2"><span class="toc-text">2.层方式实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="toc-text">3.优化目标</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">4、激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Sigmoid"><span class="toc-text">1.Sigmoid</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-ReLU"><span class="toc-text">2.ReLU</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-LeakyReLU"><span class="toc-text">3.LeakyReLU</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-Tanh"><span class="toc-text">4.Tanh</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E8%BE%93%E5%87%BA%E5%B1%82%E8%AE%BE%E8%AE%A1"><span class="toc-text">5、输出层设计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%99%AE%E9%80%9A%E5%AE%9E%E6%95%B0%E7%A9%BA%E9%97%B4"><span class="toc-text">1.普通实数空间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-0-1-%E5%8C%BA%E9%97%B4"><span class="toc-text">2.[0,1]区间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-0-1-%E5%8C%BA%E9%97%B4%EF%BC%8C%E5%92%8C%E4%B8%BA1"><span class="toc-text">3.[0,1]区间，和为1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1-%E5%8C%BA%E9%97%B4"><span class="toc-text">4.[-1,1]区间</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%E3%80%81%E8%AF%AF%E5%B7%AE%E8%AE%A1%E7%AE%97"><span class="toc-text">6、误差计算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%9D%87%E6%96%B9%E5%B7%AE%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0"><span class="toc-text">1.均方差误差函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BA%A4%E5%8F%89%E7%86%B5%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0"><span class="toc-text">2.交叉熵误差函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%B1%BB%E5%9E%8B"><span class="toc-text">7、神经网络类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">1.卷积神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">2.循环神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88%E6%9C%BA%E5%88%B6%EF%BC%89%E7%BD%91%E7%BB%9C"><span class="toc-text">3.注意力（机制）网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">4.图卷积神经网络</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81Keras%E9%AB%98%E5%B1%82%E6%8E%A5%E5%8F%A3"><span class="toc-text">四、Keras高层接口</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%B8%B8%E8%A7%81%E5%8A%9F%E8%83%BD%E6%A8%A1%E5%9D%97"><span class="toc-text">1、常见功能模块</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E5%B1%82%E7%B1%BB"><span class="toc-text">1.常见网络层类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%BD%91%E7%BB%9C%E5%AE%B9%E5%99%A8"><span class="toc-text">2.网络容器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E6%A8%A1%E5%9E%8B%E8%A3%85%E9%85%8D%E3%80%81%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%B5%8B%E8%AF%95"><span class="toc-text">2、模型装配、训练与测试</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%A8%A1%E5%9E%8B%E8%A3%85%E9%85%8D"><span class="toc-text">1.模型装配</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-text">2.模型训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%A8%A1%E5%9E%8B%E6%B5%8B%E8%AF%95"><span class="toc-text">3.模型测试</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="toc-text">3、模型保存与加载</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%BC%A0%E9%87%8F%E6%96%B9%E5%BC%8F"><span class="toc-text">1.张量方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%BD%91%E7%BB%9C%E6%96%B9%E5%BC%8F"><span class="toc-text">2.网络方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-SavedModel%E6%96%B9%E5%BC%8F"><span class="toc-text">3.SavedModel方式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C"><span class="toc-text">4、自定义网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C%E5%B1%82"><span class="toc-text">1.自定义网络层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C"><span class="toc-text">2.自定义网络</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E6%A8%A1%E5%9E%8B%E4%B9%90%E5%9B%AD"><span class="toc-text">5、模型乐园</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-text">1.加载模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%E3%80%81%E6%B5%8B%E9%87%8F%E5%B7%A5%E5%85%B7"><span class="toc-text">6、测量工具</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%96%B0%E5%BB%BA%E6%B5%8B%E9%87%8F%E5%99%A8"><span class="toc-text">1.新建测量器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-text">2.写入数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E8%AF%BB%E5%8F%96%E7%BB%9F%E8%AE%A1%E4%BF%A1%E6%81%AF"><span class="toc-text">3.读取统计信息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E6%B8%85%E9%99%A4%E7%8A%B6%E6%80%81"><span class="toc-text">4.清除状态</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E5%87%86%E7%A1%AE%E7%8E%87%E7%BB%9F%E8%AE%A1%E5%AE%9E%E6%88%98"><span class="toc-text">5.准确率统计实战</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7%E3%80%81%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-text">7、可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%A8%A1%E5%9E%8B%E7%AB%AF"><span class="toc-text">1.模型端</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%B5%8F%E8%A7%88%E5%99%A8%E7%AB%AF"><span class="toc-text">2.浏览器端</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-text">五、过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%B9%E9%87%8F"><span class="toc-text">1、模型的容量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-text">2、过拟合与欠拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-text">1.欠拟合</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-text">2.过拟合</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86"><span class="toc-text">3、数据集划分</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%AA%8C%E8%AF%81%E9%9B%86%E4%B8%8E%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-text">1.验证集与超参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%8F%90%E5%89%8D%E5%81%9C%E6%AD%A2"><span class="toc-text">2.提前停止</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1"><span class="toc-text">4、模型设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">5、正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#L0%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">L0正则化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#L1%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">L1正则化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#L2%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">L2正则化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%E3%80%81Dropout"><span class="toc-text">6、Dropout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7%E3%80%81%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-text">7、数据增强</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%97%8B%E8%BD%AC"><span class="toc-text">1.旋转</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%BF%BB%E8%BD%AC"><span class="toc-text">2.翻转</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E8%A3%81%E5%89%AA"><span class="toc-text">3.裁剪</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE"><span class="toc-text">4.生成数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E5%85%B6%E4%BB%96%E6%96%B9%E5%BC%8F"><span class="toc-text">5.其他方式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8%E3%80%81%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98"><span class="toc-text">8、过拟合问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA"><span class="toc-text">1.数据集构建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%BD%91%E7%BB%9C%E5%B1%82%E6%95%B0%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text">2.网络层数的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Dropout%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text">3.Dropout的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text">4.正则化的影响</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">六、卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">1、全连接层的问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%B1%80%E9%83%A8%E7%9B%B8%E5%85%B3%E6%80%A7"><span class="toc-text">1.局部相关性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%9D%83%E5%80%BC%E5%85%B1%E4%BA%AB"><span class="toc-text">2.权值共享</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97"><span class="toc-text">3.卷积运算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">2、卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%8D%95%E9%80%9A%E9%81%93%E8%BE%93%E5%85%A5%E5%92%8C%E5%8D%95%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="toc-text">1.单通道输入和单卷积核</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%A4%9A%E9%80%9A%E9%81%93%E8%BE%93%E5%85%A5%E5%92%8C%E5%8D%95%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="toc-text">2.多通道输入和单卷积核</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%A4%9A%E9%80%9A%E9%81%93%E8%BE%93%E5%85%A5%E3%80%81%E5%A4%9A%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="toc-text">3.多通道输入、多卷积核</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E6%AD%A5%E9%95%BF"><span class="toc-text">4.步长</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E5%A1%AB%E5%85%85"><span class="toc-text">5.填充</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%AE%9E%E7%8E%B0"><span class="toc-text">3、卷积层实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9D%83%E5%80%BC"><span class="toc-text">1.自定义权值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%B1%BB"><span class="toc-text">2.卷积层类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81LeNet-5%E5%AE%9E%E6%88%98"><span class="toc-text">4、LeNet-5实战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0"><span class="toc-text">5、表示学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%E3%80%81%E6%A2%AF%E5%BA%A6%E4%BC%A0%E6%92%AD"><span class="toc-text">6、梯度传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7%E3%80%81%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-text">7、池化层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8%E3%80%81BatchNorm%E5%B1%82"><span class="toc-text">8、BatchNorm层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9%E3%80%81%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="toc-text">9、经典卷积网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-AlexNet"><span class="toc-text">1.AlexNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-VGG%E7%B3%BB%E5%88%97"><span class="toc-text">2.VGG系列</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-GoogLeNet"><span class="toc-text">3.GoogLeNet</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10%E3%80%81CIFAR10%E4%B8%8EVGG13%E5%AE%9E%E6%88%98"><span class="toc-text">10、CIFAR10与VGG13实战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11%E3%80%81%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%98%E7%A7%8D"><span class="toc-text">11、卷积层变种</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF"><span class="toc-text">1.空洞卷积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF"><span class="toc-text">2.转置卷积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF"><span class="toc-text">3.分离卷积</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12%E3%80%81%E6%B7%B1%E5%BA%A6%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C"><span class="toc-text">12、深度残差网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-ResNet%E5%8E%9F%E7%90%86"><span class="toc-text">1.ResNet原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-ResBlock%E5%AE%9E%E7%8E%B0"><span class="toc-text">2.ResBlock实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13%E3%80%81DenseNet"><span class="toc-text">13、DenseNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14%E3%80%81CIFAR10%E4%B8%8EResNet18%E5%AE%9E%E6%88%98"><span class="toc-text">14、CIFAR10与ResNet18实战</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">七、循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%BA%8F%E5%88%97%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95"><span class="toc-text">1、序列表示方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Embedding%E5%B1%82"><span class="toc-text">1.Embedding层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E8%AF%8D%E5%90%91%E9%87%8F"><span class="toc-text">2.预训练的词向量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E5%BE%AA%E7%8E%AF%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="toc-text">2、循环卷积网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%9F"><span class="toc-text">1.全连接层？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%85%B1%E4%BA%AB%E6%9D%83%E5%80%BC"><span class="toc-text">2.共享权值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%85%A8%E5%B1%80%E8%AF%AD%E4%B9%89"><span class="toc-text">3.全局语义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">4.循环神经网络</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E6%A2%AF%E5%BA%A6%E4%BC%A0%E6%92%AD"><span class="toc-text">3、梯度传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81RNN%E5%B1%82%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-text">4、RNN层使用方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-SimpleRNNCell"><span class="toc-text">1.SimpleRNNCell</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%A4%9A%E5%B1%82SimpleRNNCell%E7%BD%91%E7%BB%9C"><span class="toc-text">2.多层SimpleRNNCell网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-SimpleRNN%E5%B1%82"><span class="toc-text">3.SimpleRNN层</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81RNN%E6%84%9F%E6%83%85%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E5%AE%9E%E6%88%98"><span class="toc-text">5、RNN感情分类问题实战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%E3%80%81%E6%A2%AF%E5%BA%A6%E5%BC%A5%E6%BC%AB%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="toc-text">6、梯度弥漫与梯度爆炸</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/05/11/%E8%9E%8D%E5%90%88%E6%80%BB%E7%BB%93/" title="融合总结">融合总结</a><time datetime="2021-05-11T05:58:15.000Z" title="发表于 2021-05-11 13:58:15">2021-05-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/04/12/conda%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" title="conda常用命令">conda常用命令</a><time datetime="2021-04-12T08:47:06.096Z" title="发表于 2021-04-12 16:47:06">2021-04-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/04/02/lidar%E6%91%84%E5%83%8F%E5%A4%B4%E8%9E%8D%E5%90%88/" title="lidar摄像头融合">lidar摄像头融合</a><time datetime="2021-04-02T02:50:34.000Z" title="发表于 2021-04-02 10:50:34">2021-04-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/04/01/C++%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" title="C++设计模式">C++设计模式</a><time datetime="2021-04-01T13:36:25.000Z" title="发表于 2021-04-01 21:36:25">2021-04-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2021/03/31/%E4%B9%90%E8%A7%86%E6%B7%B1%E5%BA%A6%E7%9B%B8%E6%9C%BA%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/" title="乐视深度相机使用指南">乐视深度相机使用指南</a><time datetime="2021-03-31T04:19:14.000Z" title="发表于 2021-03-31 12:19:14">2021-03-31</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2021 By spaceman</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><script type="text/javascript" src="https://api.uixsj.cn/hitokoto/w.php?code=js"></script><div id="xsjhitokoto"><script>xsjhitokoto()</script></div> <iframe scrolling="no" src="https://tianqiapi.com/api.php?style=tx&color=eee" frameborder="0" allowtransparency="false" align="middle" height="20"></iframe></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="false" data-text="富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>