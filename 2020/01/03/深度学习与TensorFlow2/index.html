<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>深度学习与TensorFlow2 | spaceman</title><meta name="keywords" content="TensorFlow2,深度学习"><meta name="author" content="spaceman"><meta name="copyright" content="spaceman"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="深度学习与TensorFlow2 环境（版本需要匹配）：  TensorFlow：TensorFlow2.0 GPU版本 Anaconda：4.8.1（conda -V，conda -list，Python 3.7 version），链接：https:&#x2F;&#x2F;www.anaconda.com&#x2F;distribution&#x2F;#download-section CUDA：V10.0.130（nvcc -V）">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习与TensorFlow2">
<meta property="og:url" content="http://nu-ll.github.io/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/index.html">
<meta property="og:site_name" content="spaceman">
<meta property="og:description" content="深度学习与TensorFlow2 环境（版本需要匹配）：  TensorFlow：TensorFlow2.0 GPU版本 Anaconda：4.8.1（conda -V，conda -list，Python 3.7 version），链接：https:&#x2F;&#x2F;www.anaconda.com&#x2F;distribution&#x2F;#download-section CUDA：V10.0.130（nvcc -V）">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2020-01-03T13:04:26.000Z">
<meta property="article:modified_time" content="2020-05-13T08:30:26.831Z">
<meta property="article:author" content="spaceman">
<meta property="article:tag" content="TensorFlow2">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/favicon.jpg"><link rel="canonical" href="http://nu-ll.github.io/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.0.2',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  ClickShowText: {"text":"富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善","fontSize":"15px"},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-05-13 16:30:26'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = '2'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><meta name="generator" content="Hexo 5.0.2"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" data-lazy-src="/img/spaceman.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">62</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">72</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/todo/"><i class="fa-fw fas fa-list"></i><span> 清单</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8Etensorflow2"><span class="toc-text"> 深度学习与TensorFlow2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#tensorflow2%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C"><span class="toc-text"> TensorFlow2基础操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-text"> 1、数据类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E6%95%B0%E5%80%BC%E7%B1%BB%E5%9E%8B"><span class="toc-text"> 1.数值类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B"><span class="toc-text"> 2.字符串类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E5%B8%83%E5%B0%94%E7%B1%BB%E5%9E%8B"><span class="toc-text"> 3.布尔类型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%95%B0%E5%80%BC%E7%B2%BE%E5%BA%A6"><span class="toc-text"> 2、数值精度</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E8%AF%BB%E5%8F%96%E7%B2%BE%E5%BA%A6"><span class="toc-text"> 1.读取精度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="toc-text"> 2.类型转换</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%BE%85%E4%BC%98%E5%8C%96%E5%BC%A0%E9%87%8F"><span class="toc-text"> 3、待优化张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F"><span class="toc-text"> 4、创建张量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E4%BB%8E%E6%95%B0%E7%BB%84-%E5%88%97%E8%A1%A8%E4%B8%AD%E5%88%9B%E5%BB%BA"><span class="toc-text"> 1.从数组、列表中创建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E5%88%9B%E5%BB%BA%E5%85%A80%E5%85%A81%E5%BC%A0%E9%87%8F"><span class="toc-text"> 2.创建全0全1张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E5%88%9B%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E5%80%BC%E5%BC%A0%E9%87%8F"><span class="toc-text"> 3.创建自定义数值张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E5%88%9B%E5%BB%BA%E5%B7%B2%E7%9F%A5%E5%88%86%E5%B8%83%E7%9A%84%E5%BC%A0%E9%87%8F"><span class="toc-text"> 4.创建已知分布的张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5%E5%88%9B%E5%BB%BA%E5%BA%8F%E5%88%97"><span class="toc-text"> 5.创建序列</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%BC%A0%E9%87%8F%E7%9A%84%E5%85%B8%E5%9E%8B%E7%94%A8%E9%80%94"><span class="toc-text"> 5、张量的典型用途</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%88%87%E7%89%87"><span class="toc-text"> 6、索引与切片</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E7%B4%A2%E5%BC%95"><span class="toc-text"> 1.索引</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E5%88%87%E7%89%87"><span class="toc-text"> 2.切片</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E7%BB%B4%E5%BA%A6%E5%8F%98%E6%8D%A2"><span class="toc-text"> 7、维度变换</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E6%94%B9%E5%8F%98%E8%A7%86%E5%9B%BE-reshape"><span class="toc-text"> 1.改变视图 reshape</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E5%A2%9E%E5%88%A0%E7%BB%B4%E5%BA%A6"><span class="toc-text"> 2.增删维度</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A2%9E%E5%8A%A0%E7%BB%B4%E5%BA%A6"><span class="toc-text"> 增加维度</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%A0%E9%99%A4%E7%BB%B4%E5%BA%A6"><span class="toc-text"> 删除维度</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E4%BA%A4%E6%8D%A2%E7%BB%B4%E5%BA%A6"><span class="toc-text"> 3.交换维度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E5%A4%8D%E5%88%B6%E6%95%B0%E6%8D%AE"><span class="toc-text"> 4.复制数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-broadcasting"><span class="toc-text"> 8、Broadcasting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="toc-text"> 9、数学运算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E5%8A%A0%E5%87%8F%E4%B9%98%E9%99%A4"><span class="toc-text"> 1.加减乘除</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E4%B9%98%E6%96%B9"><span class="toc-text"> 2.乘方</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E6%8C%87%E6%95%B0%E5%92%8C%E5%AF%B9%E6%95%B0"><span class="toc-text"> 3.指数和对数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="toc-text"> 4.矩阵乘法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tensorflow2%E8%BF%9B%E9%98%B6%E6%93%8D%E4%BD%9C"><span class="toc-text"> TensorFlow2进阶操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2"><span class="toc-text"> 1、合并与分割</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E5%90%88%E5%B9%B6"><span class="toc-text"> 1.合并</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8B%BC%E6%8E%A5"><span class="toc-text"> 拼接</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A0%86%E5%8F%A0"><span class="toc-text"> 堆叠</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E5%88%86%E5%89%B2"><span class="toc-text"> 2.分割</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1"><span class="toc-text"> 2、数据统计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E5%90%91%E9%87%8F%E8%8C%83%E6%95%B0"><span class="toc-text"> 1.向量范数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E6%9C%80%E5%80%BC-%E5%9D%87%E5%80%BC-%E5%92%8C"><span class="toc-text"> 2.最值、均值、和</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%BC%A0%E9%87%8F%E6%AF%94%E8%BE%83"><span class="toc-text"> 3、张量比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%A4%8D%E5%88%B6%E5%92%8C%E5%A1%AB%E5%85%85"><span class="toc-text"> 4、复制和填充</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E5%A1%AB%E5%85%85"><span class="toc-text"> 1.填充</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E5%A4%8D%E5%88%B6"><span class="toc-text"> 2.复制</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%95%B0%E6%8D%AE%E9%99%90%E5%B9%85"><span class="toc-text"> 5、数据限幅</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C"><span class="toc-text"> 6、高级操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1tfgather"><span class="toc-text"> 1.tf.gather</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2tfgather_nd"><span class="toc-text"> 2.tf.gather_nd</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3tfboolean_mask"><span class="toc-text"> 3.tf.boolean_mask</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4tfwhere"><span class="toc-text"> 4.tf.where</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-text"> 例子</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5scatter_nd"><span class="toc-text"> 5.scatter_nd</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6meshgrid"><span class="toc-text"> 6.meshgrid</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E7%BB%8F%E5%85%B8%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8A%A0%E8%BD%BD"><span class="toc-text"> 7、经典数据集加载</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E9%9A%8F%E6%9C%BA%E6%89%93%E6%95%A3"><span class="toc-text"> 1.随机打散</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E6%89%B9%E8%AE%AD%E7%BB%83"><span class="toc-text"> 2.批训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text"> 3.预处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E5%BE%AA%E7%8E%AF%E8%AE%AD%E7%BB%83"><span class="toc-text"> 4.循环训练</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-mnist%E5%AE%9E%E6%88%98"><span class="toc-text"> 8、MNIST实战</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text"> 神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-text"> 1、感知机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="toc-text"> 2、全连接层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E5%BC%A0%E9%87%8F%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0"><span class="toc-text"> 1.张量方式实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E5%B1%82%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0"><span class="toc-text"> 2.层方式实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text"> 3、神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E5%BC%A0%E9%87%8F%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0-2"><span class="toc-text"> 1.张量方式实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E5%B1%82%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0-2"><span class="toc-text"> 2.层方式实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="toc-text"> 3.优化目标</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text"> 4、激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1sigmoid"><span class="toc-text"> 1.Sigmoid</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2relu"><span class="toc-text"> 2.ReLU</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3leakyrelu"><span class="toc-text"> 3.LeakyReLU</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4tanh"><span class="toc-text"> 4.Tanh</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E8%BE%93%E5%87%BA%E5%B1%82%E8%AE%BE%E8%AE%A1"><span class="toc-text"> 5、输出层设计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E6%99%AE%E9%80%9A%E5%AE%9E%E6%95%B0%E7%A9%BA%E9%97%B4"><span class="toc-text"> 1.普通实数空间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#201%E5%8C%BA%E9%97%B4"><span class="toc-text"> 2.[0,1]区间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#301%E5%8C%BA%E9%97%B4%E5%92%8C%E4%B8%BA1"><span class="toc-text"> 3.[0,1]区间，和为1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-11%E5%8C%BA%E9%97%B4"><span class="toc-text"> 4.[-1,1]区间</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E8%AF%AF%E5%B7%AE%E8%AE%A1%E7%AE%97"><span class="toc-text"> 6、误差计算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E5%9D%87%E6%96%B9%E5%B7%AE%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0"><span class="toc-text"> 1.均方差误差函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E4%BA%A4%E5%8F%89%E7%86%B5%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0"><span class="toc-text"> 2.交叉熵误差函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%B1%BB%E5%9E%8B"><span class="toc-text"> 7、神经网络类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text"> 1.卷积神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text"> 2.循环神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%BD%91%E7%BB%9C"><span class="toc-text"> 3.注意力（机制）网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text"> 4.图卷积神经网络</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#keras%E9%AB%98%E5%B1%82%E6%8E%A5%E5%8F%A3"><span class="toc-text"> Keras高层接口</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%B8%B8%E8%A7%81%E5%8A%9F%E8%83%BD%E6%A8%A1%E5%9D%97"><span class="toc-text"> 1、常见功能模块</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E5%B1%82%E7%B1%BB"><span class="toc-text"> 1.常见网络层类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E7%BD%91%E7%BB%9C%E5%AE%B9%E5%99%A8"><span class="toc-text"> 2.网络容器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%A8%A1%E5%9E%8B%E8%A3%85%E9%85%8D-%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%B5%8B%E8%AF%95"><span class="toc-text"> 2、模型装配、训练与测试</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E6%A8%A1%E5%9E%8B%E8%A3%85%E9%85%8D"><span class="toc-text"> 1.模型装配</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-text"> 2.模型训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E6%A8%A1%E5%9E%8B%E6%B5%8B%E8%AF%95"><span class="toc-text"> 3.模型测试</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="toc-text"> 3、模型保存与加载</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E5%BC%A0%E9%87%8F%E6%96%B9%E5%BC%8F"><span class="toc-text"> 1.张量方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E7%BD%91%E7%BB%9C%E6%96%B9%E5%BC%8F"><span class="toc-text"> 2.网络方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3savedmodel%E6%96%B9%E5%BC%8F"><span class="toc-text"> 3.SavedModel方式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C"><span class="toc-text"> 4、自定义网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C%E5%B1%82"><span class="toc-text"> 1.自定义网络层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C"><span class="toc-text"> 2.自定义网络</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%A8%A1%E5%9E%8B%E4%B9%90%E5%9B%AD"><span class="toc-text"> 5、模型乐园</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-text"> 1.加载模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E6%B5%8B%E9%87%8F%E5%B7%A5%E5%85%B7"><span class="toc-text"> 6、测量工具</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E6%96%B0%E5%BB%BA%E6%B5%8B%E9%87%8F%E5%99%A8"><span class="toc-text"> 1.新建测量器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-text"> 2.写入数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E8%AF%BB%E5%8F%96%E7%BB%9F%E8%AE%A1%E4%BF%A1%E6%81%AF"><span class="toc-text"> 3.读取统计信息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E6%B8%85%E9%99%A4%E7%8A%B6%E6%80%81"><span class="toc-text"> 4.清除状态</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5%E5%87%86%E7%A1%AE%E7%8E%87%E7%BB%9F%E8%AE%A1%E5%AE%9E%E6%88%98"><span class="toc-text"> 5.准确率统计实战</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-text"> 7、可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E6%A8%A1%E5%9E%8B%E7%AB%AF"><span class="toc-text"> 1.模型端</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E6%B5%8F%E8%A7%88%E5%99%A8%E7%AB%AF"><span class="toc-text"> 2.浏览器端</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-text"> 过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%B9%E9%87%8F"><span class="toc-text"> 1、模型的容量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-text"> 2、过拟合与欠拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-text"> 1.欠拟合</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-text"> 2.过拟合</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86"><span class="toc-text"> 3、数据集划分</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E9%AA%8C%E8%AF%81%E9%9B%86%E4%B8%8E%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-text"> 1.验证集与超参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E6%8F%90%E5%89%8D%E5%81%9C%E6%AD%A2"><span class="toc-text"> 2.提前停止</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1"><span class="toc-text"> 4、模型设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text"> 5、正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-dropout"><span class="toc-text"> 6、Dropout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-text"> 7、数据增强</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E6%97%8B%E8%BD%AC"><span class="toc-text"> 1.旋转</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E7%BF%BB%E8%BD%AC"><span class="toc-text"> 2.翻转</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E8%A3%81%E5%89%AA"><span class="toc-text"> 3.裁剪</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE"><span class="toc-text"> 4.生成数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5%E5%85%B6%E4%BB%96%E6%96%B9%E5%BC%8F"><span class="toc-text"> 5.其他方式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98"><span class="toc-text"> 8、过拟合问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA"><span class="toc-text"> 1.数据集构建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E7%BD%91%E7%BB%9C%E5%B1%82%E6%95%B0%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text"> 2.网络层数的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3dropout%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text"> 3.Dropout的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text"> 4.正则化的影响</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text"> 卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text"> 1、全连接层的问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E5%B1%80%E9%83%A8%E7%9B%B8%E5%85%B3%E6%80%A7"><span class="toc-text"> 1.局部相关性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E5%8A%9D%E5%80%BC%E5%85%B1%E4%BA%AB"><span class="toc-text"> 2.劝值共享</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97"><span class="toc-text"> 3.卷积运算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text"> 2、卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E5%8D%95%E9%80%9A%E9%81%93%E8%BE%93%E5%85%A5%E5%92%8C%E5%8D%95%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="toc-text"> 1.单通道输入和单卷积核</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E5%A4%9A%E9%80%9A%E9%81%93%E8%BE%93%E5%85%A5%E5%92%8C%E5%8D%95%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="toc-text"> 2.多通道输入和单卷积核</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E5%A4%9A%E9%80%9A%E9%81%93%E8%BE%93%E5%85%A5-%E5%A4%9A%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="toc-text"> 3.多通道输入、多卷积核</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E6%AD%A5%E9%95%BF"><span class="toc-text"> 4.步长</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5%E5%A1%AB%E5%85%85"><span class="toc-text"> 5.填充</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%AE%9E%E7%8E%B0"><span class="toc-text"> 3、卷积层实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9D%83%E5%80%BC"><span class="toc-text"> 1.自定义权值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%B1%BB"><span class="toc-text"> 2.卷积层类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-lenet-5%E5%AE%9E%E6%88%98"><span class="toc-text"> 4、LeNet-5实战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0"><span class="toc-text"> 5、表示学习</span></a></li></ol></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">spaceman</a></span><span id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/todo/"><i class="fa-fw fas fa-list"></i><span> 清单</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">深度学习与TensorFlow2</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-01-03T13:04:26.000Z" title="发表于 2020-01-03 21:04:26">2020-01-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-05-13T08:30:26.831Z" title="更新于 2020-05-13 16:30:26">2020-05-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">33.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>128分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="深度学习与tensorflow2"><a class="markdownIt-Anchor" href="#深度学习与tensorflow2"></a> 深度学习与TensorFlow2</h1>
<p>环境（版本需要匹配）：</p>
<ul>
<li>TensorFlow：TensorFlow2.0 GPU版本</li>
<li>Anaconda：4.8.1（conda -V，conda -list，Python 3.7 version），链接：<a target="_blank" rel="noopener" href="https://www.anaconda.com/distribution/#download-section">https://www.anaconda.com/distribution/#download-section</a></li>
<li>CUDA：V10.0.130（nvcc -V），链接：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a>
<ul>
<li>cuDNN：，链接：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/rdp/cudnn-archive">https://developer.nvidia.com/rdp/cudnn-archive</a></li>
</ul>
</li>
</ul>
<h2 id="tensorflow2基础操作"><a class="markdownIt-Anchor" href="#tensorflow2基础操作"></a> TensorFlow2基础操作</h2>
<p>TensorFlow中的数据载体叫做张量（Tensor）对象，即<code>tf.Tensor</code>，对应不同的类型，能够存储大量的连续的数据。同时所有的运算操作(Operation，简称 OP)也都是基于张量对象进行的</p>
<p>什么是Tensor：</p>
<ul>
<li>Tensor是一个比较广泛的数据</li>
<li>标量（scalar）：1.1、2.2等准确的数据类型（维度dim=0）</li>
<li>向量（vector）：[1.1]、[1.1,2.2,…]（dim=1）</li>
<li>矩阵（matrix）：[[1.1,2.2],[2.2,2.2],[3.3,2.2]]</li>
<li>数学上tensor：一般指维度&gt;2时的数据，但是在TensorFlow中维度&gt;=1时的数据全称为tensor，甚至标量也可以看作是tensor，所以工程上讲tensor一般指所有的数据</li>
</ul>
<h3 id="1-数据类型"><a class="markdownIt-Anchor" href="#1-数据类型"></a> 1、数据类型</h3>
<h4 id="1数值类型"><a class="markdownIt-Anchor" href="#1数值类型"></a> 1.数值类型</h4>
<p>数值类型的张量是 TensorFlow 的主要数据载体， 根据维度数来区分，可分为：</p>
<ul>
<li>
<p>标量(Scalar)：单个的实数，如 1.2, 3.4 等，维度(Dimension)数为 0， shape 为[]</p>
</li>
<li>
<p>向量(Vector)：单个实数的有序集合，通过中括号包裹，如[1.2]， [1.2, 3.4]等，维度数为 1，长度不定， shape 为[n]</p>
</li>
<li>
<p>矩阵(Matrix)：n行m列实数的有序集合，如[[1,2], [3,4]]，也可以写成<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>3</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>4</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix}
  1 &amp; 2\\ 
  3 &amp; 4
  \end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span></span></span></span>，维度数为 2，每个维度上的长度不定， shape 为[n,m]</p>
</li>
<li>
<p>张量(Tensor)：所有维度数dim &gt; 2的数组统称为张量。 张量的每个维度也作轴(Axis)，一般来说，维度代表了具体的物理含义，张量的维度数以及每个维度所代表的具体物理含义需要由用户自行定义。</p>
<blockquote>
<p>比如 Shape 为[2,32,32,3]的张量共有 4 维，如果表示图片数据的话，每个维度/轴代表的含义分别是图片数量、 图片高度、 图片宽度、 图片通道数，其中 2 代表了 2 张图片， 32 代表了高、 宽均为 32， 3 代表了 RGB 共 3 个通道</p>
</blockquote>
</li>
</ul>
<p>在 TensorFlow 中，为了表达方便，一般把标量、向量、矩阵也统称为张量，不作区分，需要根据张量的维度数或形状自行判断</p>
<p>张量的创建：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>aa = tf.constant(<span class="number">1.2</span>) <span class="comment"># TF方式创建标量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.constant([<span class="number">1</span>,<span class="number">2.</span>,<span class="number">3.3</span>]) <span class="comment"># TF方式创建向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">0</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">1.</span> , <span class="number">2.</span> , <span class="number">3.3</span>],dtype=float32)&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]) <span class="comment"># TF方式创建2行2列的矩阵</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">1</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>]])&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]]])<span class="comment"># TF方式创建3维张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">2</span>, shape=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]],</span><br><span class="line"></span><br><span class="line">       [[<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]]])&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>id：TensorFlow 中内部索引对象的编号</li>
<li>shape：张量的形状</li>
<li>dtype：张量的数值精度</li>
</ul>
<blockquote>
<p>张量可以通过 numpy()方法可以返回 Numpy.array 类型的数据，方便导出数据到系统的其他模块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.numpy() <span class="comment"># 将 TF 张量的数据导出为 numpy 数组格式</span></span><br><span class="line">array([<span class="number">1.</span> , <span class="number">2.</span> , <span class="number">3.3</span>], dtype=float32)</span><br></pre></td></tr></table></figure>
</blockquote>
<h4 id="2字符串类型"><a class="markdownIt-Anchor" href="#2字符串类型"></a> 2.字符串类型</h4>
<p>使用频率较低。通过传入字符串对象即可创建字符串类型的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant(<span class="string">&#x27;Hello, Deep Learning.&#x27;</span>) <span class="comment"># 创建字符串</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">3</span>, shape=(), dtype=string, numpy=<span class="string">b&#x27;Hello, Deep Learning.&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>
<p>在 <code>tf.strings</code> 模块中，提供了常见的字符串类型的工具函数，如小写化 lower()、 拼接join()、 长度 length()、 切分 split()等</p>
<blockquote>
<p>将字符串全部小写化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.strings.lower(a) <span class="comment"># 小写化字符串</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">19</span>, shape=(), dtype=string, numpy=<span class="string">b&#x27;hello, deep learning.&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<h4 id="3布尔类型"><a class="markdownIt-Anchor" href="#3布尔类型"></a> 3.布尔类型</h4>
<p>布尔类型的张量需要传入 Python 语言的布尔类型数据，转换成 TensorFlow 内部布尔型即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant(<span class="literal">True</span>) <span class="comment"># 创建布尔类型标量</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">22</span>, shape=(), dtype=bool, numpy=<span class="literal">True</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([<span class="literal">True</span>, <span class="literal">False</span>]) <span class="comment"># 创建布尔类型向量</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">25</span>, shape=(<span class="number">2</span>,), dtype=bool, numpy=array([ <span class="literal">True</span>, <span class="literal">False</span>])&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>TensorFlow 的布尔类型和 Python 语言的布尔类型并不等价，不能通用</li>
</ul>
<h3 id="2-数值精度"><a class="markdownIt-Anchor" href="#2-数值精度"></a> 2、数值精度</h3>
<p>保存的数据位越长，精度越高，同时占用的内存空间也就越大。常用的精度类型有<code>tf.int16</code>、<code>tf.int32</code>、<code>tf.int64</code>、<code>tf.float16</code>、<code>tf.float32</code>、<code>tf.float64</code>等，其中<code>tf.float64</code>即为<code>tf.double</code></p>
<p>在创建张量时，可以指定张量的保存精度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.constant(<span class="number">123456789</span>, dtype=tf.int16)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.constant(<span class="number">123456789</span>, dtype=tf.int32)</span><br></pre></td></tr></table></figure>
<p>对于大部分深度学习算法，一般使用 tf.int32 和 tf.float32 可满足大部分场合的运算精<br>
度要求，部分对精度要求较高的算法，如强化学习某些算法，可以选择使用 tf.int64 和<br>
tf.float64 精度保存张量</p>
<h4 id="1读取精度"><a class="markdownIt-Anchor" href="#1读取精度"></a> 1.读取精度</h4>
<p>通过访问张量的<code>dtype</code>成员属性可以判断张量的保存精度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">&#x27;before:&#x27;</span>,a.dtype) <span class="comment"># 读取原有张量的数值精度</span></span><br><span class="line"><span class="keyword">if</span> a.dtype != tf.float32: <span class="comment"># 如果精度不符合要求，则进行转换</span></span><br><span class="line">    a = tf.cast(a,tf.float32) <span class="comment"># tf.cast 函数可以完成精度转换</span></span><br><span class="line">print(<span class="string">&#x27;after :&#x27;</span>,a.dtype) <span class="comment"># 打印转换后的精度</span></span><br></pre></td></tr></table></figure>
<h4 id="2类型转换"><a class="markdownIt-Anchor" href="#2类型转换"></a> 2.类型转换</h4>
<p>通常通过<code>tf.cast</code>函数进行转换：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant(np.pi, dtype=tf.float16) <span class="comment"># 创建 tf.float16 低精度张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.cast(a, tf.double) <span class="comment"># 转换为高精度张量</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">44</span>, shape=(), dtype=float64, numpy=<span class="number">3.140625</span>&gt;</span><br></pre></td></tr></table></figure>
<p>布尔类型与整型之间相互转换也是合法的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([<span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.cast(a, tf.int32) <span class="comment"># 布尔类型转整型</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">48</span>, shape=(<span class="number">2</span>,), dtype=int32, numpy=array([<span class="number">1</span>, <span class="number">0</span>])&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>一般默认 0 表示 False， 1 表示 True，在 TensorFlow 中，将<strong>非 0 数字都视为 True</strong></li>
</ul>
<h3 id="3-待优化张量"><a class="markdownIt-Anchor" href="#3-待优化张量"></a> 3、待优化张量</h3>
<p>为了区分需要计算梯度信息的张量与不需要计算梯度信息的张量， TensorFlow 增加了<br>
一种专门的数据类型来<u>支持梯度信息的记录</u>：<code>tf.Variable</code></p>
<p><code>tf.Variable</code>类型在普通的张量类型基础上添加了 <code>name</code>， <code>trainable</code> 等属性来支持计算图的构建。由于梯度运算会消耗大量的计算资源，而且会自动更新相关参数，对于不需要的优化的张量，如神经网络的输入<strong>X</strong>，不需要通过 tf.Variable 封装；相反，对于<u>需要计算梯度并优化</u>的张量， 如神经网络层的<strong>W</strong>和<strong>b</strong>，需要通过 tf.Variable 包裹以便TensorFlow 跟踪相关梯度信息。</p>
<p>通过<code>tf.Variable()</code>函数可以将普通张量转换为待优化张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]) <span class="comment"># 创建 TF 张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>aa = tf.Variable(a) <span class="comment"># 转换为 Variable 类型</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>aa.name, aa.trainable <span class="comment"># Variable 类型张量的属性</span></span><br><span class="line">(<span class="string">&#x27;Variable:0&#x27;</span>, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>name 属性：命名计算图中的变量，这套命名体系是 TensorFlow 内部维护的， 一般不需要用户关注 name 属性</li>
<li>trainable属性：当前张量是否需要被优化（创建 Variable 对象时默认启用优化标志，可以设置trainable=False来设置张量不需要优化）</li>
</ul>
<p>直接创建待优化张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]) <span class="comment"># 直接创建 Variable 张量</span></span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=(<span class="number">2</span>, <span class="number">2</span>) dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>]])&gt;</span><br></pre></td></tr></table></figure>
<p>待优化张量可视为普通张量的特殊类型， 普通张量其实也可以通过<code>GradientTape.watch()</code>方法临时加入跟踪梯度信息的列表，从而支持自动求导功能</p>
<h3 id="4-创建张量"><a class="markdownIt-Anchor" href="#4-创建张量"></a> 4、创建张量</h3>
<h4 id="1从数组-列表中创建"><a class="markdownIt-Anchor" href="#1从数组-列表中创建"></a> 1.从数组、列表中创建</h4>
<p>通过<code>tf.convert_to_tensor</code>函数可以创建新 Tensor，并将保存在 Python List 对象或者Numpy Array 对象中的数据导入到新 Tensor 中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.convert_to_tensor([<span class="number">1</span>,<span class="number">2.</span>]) <span class="comment"># 从列表创建张量</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">86</span>, shape=(<span class="number">2</span>,), dtype=float32, numpy=array([<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">dtype=float32)&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.convert_to_tensor(np.array([[<span class="number">1</span>,<span class="number">2.</span>],[<span class="number">3</span>,<span class="number">4</span>]])) <span class="comment"># 从数组中创建张量</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">88</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float64, numpy=</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">       [<span class="number">3.</span>, <span class="number">4.</span>]])&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>Numpy 浮点数数组默认使用 64 位精度保存数据，可以在需要的时候将其转换为 tf.float32 类型</li>
<li>实际上， tf.constant()和 tf.convert_to_tensor()都能够自动的把 Numpy 数组或者 Python列表数据类型转化为 Tensor 类型，这两个 API 命名来自 TensorFlow 1.x 的命名习惯，在TensorFlow 2 中函数的名字并不是很贴切，使用其一即可</li>
</ul>
<h4 id="2创建全0全1张量"><a class="markdownIt-Anchor" href="#2创建全0全1张量"></a> 2.创建全0全1张量</h4>
<p>通过<code>tf.zeros()</code>和<code>tf.ones()</code>可创建任意形状，且内容全 0 或全 1 的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.zeros([]),tf.ones([]) <span class="comment"># 创建全 0，全 1 的标量</span></span><br><span class="line">(&lt;tf.Tensor: id=<span class="number">90</span>, shape=(), dtype=float32, numpy=<span class="number">0.0</span>&gt;,</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">91</span>, shape=(), dtype=float32, numpy=<span class="number">1.0</span>&gt;)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.zeros([<span class="number">1</span>]),tf.ones([<span class="number">1</span>]) <span class="comment"># 创建全 0，全 1 的向量</span></span><br><span class="line">(&lt;tf.Tensor: id=<span class="number">96</span>, shape=(<span class="number">1</span>,), dtype=float32, numpy=array([<span class="number">0.</span>],</span><br><span class="line">dtype=float32)&gt;,</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">99</span>, shape=(<span class="number">1</span>,), dtype=float32, numpy=array([<span class="number">1.</span>],</span><br><span class="line">dtype=float32)&gt;)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.ones([<span class="number">3</span>,<span class="number">2</span>]) <span class="comment"># 创建全 1 矩阵，指定 shape 为 3 行 2 列</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">108</span>, shape=(<span class="number">3</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<p>通过<code>tf.zeros_like</code>、<code>tf.ones_like</code>可以方便地新建与某个张量 shape 一致， 且内容为全 0 或全 1 的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.ones([<span class="number">2</span>,<span class="number">3</span>]) <span class="comment"># 创建一个矩阵</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.zeros_like(a) <span class="comment"># 创建一个与 a 形状相同，但是全 0 的新矩阵</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">113</span>, shape=(<span class="number">2</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>tf.*_like</code>是一系列的便捷函数，可以通过<code>tf.zeros(a.shape)</code>等方式实现</li>
</ul>
<h4 id="3创建自定义数值张量"><a class="markdownIt-Anchor" href="#3创建自定义数值张量"></a> 3.创建自定义数值张量</h4>
<p>通过<code>tf.fill(shape, value)</code>可以创建全为自定义数值 value 的张量，形状由 shape 参数指定：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.fill([<span class="number">2</span>,<span class="number">2</span>], <span class="number">99</span>) <span class="comment"># 创建 2 行 2 列，元素全为 99 的矩阵</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">136</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">99</span>, <span class="number">99</span>],</span><br><span class="line">       [<span class="number">99</span>, <span class="number">99</span>]])&gt;</span><br></pre></td></tr></table></figure>
<h4 id="4创建已知分布的张量"><a class="markdownIt-Anchor" href="#4创建已知分布的张量"></a> 4.创建已知分布的张量</h4>
<p>通过<code>tf.random.normal(shape, mean=0.0, stddev=1.0)</code>可以创建形状为 shape，均值为mean，标准差为 stddev 的<strong>正态分布</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.random.normal([<span class="number">2</span>,<span class="number">2</span>]) <span class="comment"># 创建标准正态分布的张量</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">143</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">-0.4307344</span> , <span class="number">0.44147003</span>],</span><br><span class="line">       [<span class="number">-0.6563149</span> , <span class="number">-0.30100572</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<p>通过<code>tf.random.uniform(shape, minval=0, maxval=None, dtype=tf.float32)</code>可以创建采样自[minval, maxval)区间的<strong>均匀分布</strong>的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.random.uniform([<span class="number">2</span>,<span class="number">2</span>]) <span class="comment"># 创建采样自[0,1)均匀分布的矩阵</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">158</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">0.65483284</span>, <span class="number">0.63064325</span>],</span><br><span class="line">       [<span class="number">0.008816</span> , <span class="number">0.81437767</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h4 id="5创建序列"><a class="markdownIt-Anchor" href="#5创建序列"></a> 5.创建序列</h4>
<p><code>tf.range(limit, delta=1)</code>可以创建[0, limit)之间，步长为 delta 的整型序列，不包含 limit 本身：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.range(<span class="number">10</span>) <span class="comment"># 0~10，不包含 10</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">180</span>, shape=(<span class="number">10</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])&gt;</span><br></pre></td></tr></table></figure>
<p>通过<code>tf.range(start, limit, delta=1)</code>可以创建[start, limit)，步长为 delta 的序列，不包含limit 本身</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.range(<span class="number">1</span>,<span class="number">10</span>,delta=<span class="number">2</span>) <span class="comment"># 1~10</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">190</span>, shape=(<span class="number">5</span>,), dtype=int32, numpy=array([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>])&gt;</span><br></pre></td></tr></table></figure>
<h3 id="5-张量的典型用途"><a class="markdownIt-Anchor" href="#5-张量的典型用途"></a> 5、张量的典型用途</h3>
<p>标量：误差值的表示、 各种测量指标的表示，比如准确度(Accuracy，简称 acc)，精度(Precision)和召回率(Recall)等</p>
<p>向量：如在全连接层和卷积神经网络层中，偏置张量b就是使用向量来表示</p>
<p>矩阵：比如全连接层的批量输入张量X的形状为[b,din]，其中b表示输入样本的个数，即Batch Size，din表示输入特征的长度</p>
<p>三维张量：表示序列信号，它的格式是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mo>=</mo><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><mi>s</mi><mi>e</mi><mi>q</mi><mi>u</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mo separator="true">,</mo><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mtext> </mtext><mi>l</mi><mi>e</mi><mi>n</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">X=[b,sequence,feature\ len]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mord mathdefault">u</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">c</span><span class="mord mathdefault">e</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">u</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">e</span><span class="mspace"> </span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mclose">]</span></span></span></span>，其中b表示序列信号的数量， sequence len 表示序列信号在时间维度上的采样点数或步数，feature len 表示每个点的特征长度，自然语言处理(Natural Language Processing，简称 NLP)中会使用到</p>
<p>四维张量：卷积神经网络中应用广泛，用于保存特征图数据，格式一般为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi><mo separator="true">,</mo><mi>c</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[b,h,w,c]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">c</span><span class="mclose">]</span></span></span></span>，其中b表示输入样本的数量， h/w 分别表示特征图的高/宽，c表示特征图的通道数。部分深度学习框架（如PyTorch）会使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><mi>c</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[b,c,h,w]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">]</span></span></span></span>格式</p>
<p>大于四维的张量一般应用的比较少，如在元学习(Meta Learning)中会采用五维的张量表示方法，理解方法与三、四维张量类似</p>
<h3 id="6-索引与切片"><a class="markdownIt-Anchor" href="#6-索引与切片"></a> 6、索引与切片</h3>
<h4 id="1索引"><a class="markdownIt-Anchor" href="#1索引"></a> 1.索引</h4>
<p>在 TensorFlow 中， 支持基本的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">[i][j]...</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mclose">]</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span></span></span></span>标准索引方式，也支持通过逗号分隔索引号的索引方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">4</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>]) <span class="comment"># 创建 4D 张量（4张32*32大小的彩色图片）</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x[<span class="number">0</span>] <span class="comment"># 程序中的第一的索引号应为 0，容易混淆，不过不影响理解</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">379</span>, shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[[ <span class="number">1.3005302</span> , <span class="number">1.5301839</span> , <span class="number">-0.32005513</span>],</span><br><span class="line">        [<span class="number">-1.3020388</span> , <span class="number">1.7837263</span> , <span class="number">-1.0747638</span> ], ...</span><br><span class="line">        [<span class="number">-1.1092019</span> , <span class="number">-1.045254</span> , <span class="number">-0.4980363</span> ],</span><br><span class="line">        [<span class="number">-0.9099222</span> , <span class="number">0.3947732</span> , <span class="number">-0.10433522</span>]]], dtype=float32)&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x[<span class="number">2</span>][<span class="number">1</span>][<span class="number">0</span>][<span class="number">1</span>] <span class="comment"># 取第 3 张图片，第 2 行，第 1 列的像素， B 通道(第 2 个通道)颜色强度值</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">418</span>, shape=(), dtype=float32, numpy=<span class="number">-0.84922135</span>&gt;</span><br></pre></td></tr></table></figure>
<p>当张量的维度数较高时， 使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[i][j]...[k]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mclose">]</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mclose">]</span></span></span></span>的方式书写不方便，可以采用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[i,j,...,k]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mclose">]</span></span></span></span>方式索引，他们是等价的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x[<span class="number">1</span>,<span class="number">9</span>,<span class="number">2</span>] <span class="comment"># 取第 2 张图片，第 10 行，第 3 列的数据，实现如下</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">436</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([ <span class="number">1.7487534</span> , <span class="number">-0.41491988</span>, <span class="number">-0.2944692</span> ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h4 id="2切片"><a class="markdownIt-Anchor" href="#2切片"></a> 2.切片</h4>
<p>通过<code>start:end:step</code>切片方式可以方便地提取一段数据，其中 start 为开始读取位置的索引， end 为结束读取位置的索引(不包含 end 位)， step 为采样步长</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x[<span class="number">1</span>:<span class="number">3</span>] <span class="comment"># 读取第 2,3 张图片</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">441</span>, shape=(<span class="number">2</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[[[ <span class="number">0.6920027</span> , <span class="number">0.18658352</span>, <span class="number">0.0568333</span> ],</span><br><span class="line">         [ <span class="number">0.31422952</span>, <span class="number">0.75933754</span>, <span class="number">0.26853144</span>],</span><br><span class="line">         [ <span class="number">2.7898</span> , <span class="number">-0.4284912</span> , <span class="number">-0.26247284</span>],...</span><br></pre></td></tr></table></figure>
<p><code>start:end:step</code>切片方式有很多简写方式</p>
<ul>
<li>全部省略时即为<code>::</code>， 表示从最开始读取到最末尾，步长为 1（不跳过任何元素）。为了更加简洁， <code>::</code>可以简写为单个冒号<code>:</code></li>
<li>从第一个元素读取时<code>start</code>可以省略（即start=0可以省略）</li>
<li>取到最后一个元素时<code>end</code>可以省略</li>
<li>步长为 1 时<code>step</code>可以省略</li>
</ul>
<p>特别地，<code>step</code>可以为负数，当step = -1时，<code>start:end:-1</code>表示从 start 开始， 逆序读取至 end 结束(不包含 end)，索引号<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>e</mi><mi>n</mi><mi>d</mi><mo>⩽</mo><mi>s</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">end\leqslant start</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83111em;vertical-align:-0.13667em;"></span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel amsrm">⩽</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">t</span></span></span></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.range(<span class="number">9</span>) <span class="comment"># 创建 0~9 向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x[<span class="number">8</span>:<span class="number">0</span>:<span class="number">-1</span>] <span class="comment"># 从 8 取到 0，逆序，不包含 0</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">466</span>, shape=(<span class="number">8</span>,), dtype=int32, numpy=array([<span class="number">8</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>])&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">4</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x[<span class="number">0</span>,::<span class="number">-2</span>,::<span class="number">-2</span>] <span class="comment"># 行、列逆序间隔采样</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">487</span>, shape=(<span class="number">16</span>, <span class="number">16</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[[ <span class="number">0.63320625</span>, <span class="number">0.0655185</span> , <span class="number">0.19056146</span>],</span><br><span class="line">        [<span class="number">-1.0078577</span> , <span class="number">-0.61400175</span>, <span class="number">0.61183935</span>],</span><br><span class="line">        [ <span class="number">0.9230892</span> , <span class="number">-0.6860094</span> , <span class="number">-0.01580668</span>],</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>为了避免出现像 [: , : , : ,1]这样过多冒号的情况，可以使用<code>...</code>符号表示取多个维度上所有的数据， 其中维度的数量需根据规则自动推断：</p>
<ul>
<li>当切片方式出现<code>...</code>符号时，<code>...</code>符号左边的维度将自动对齐到最左边</li>
<li><code>...</code>符号右边的维度将自动对齐到最右边，此时系统再自动推断<code>...</code>符号代表的维度数量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">4</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x[<span class="number">0</span>:<span class="number">2</span>,...,<span class="number">1</span>:] <span class="comment"># 高宽维度全部采集</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">497</span>, shape=(<span class="number">2</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[[[ <span class="number">0.575703</span> , <span class="number">0.8872789</span> ],</span><br><span class="line">         [ <span class="number">0.11028383</span>, <span class="number">-0.27128693</span>],</span><br><span class="line">         [<span class="number">-0.9950867</span> , <span class="number">-1.7737272</span> ],</span><br><span class="line">         ...</span><br></pre></td></tr></table></figure>
<h3 id="7-维度变换"><a class="markdownIt-Anchor" href="#7-维度变换"></a> 7、维度变换</h3>
<p>算法的每个模块对于数据张量的格式有不同的逻辑要求，当现有的数据格式不满足算<br>
法要求时，需要通过维度变换将数据调整为正确的格式。这就是维度变换的功能。</p>
<p>基本的维度变换操作函数包含了改变视图 <code>reshape</code>、 插入新维度 <code>expand_dims</code>，删除维度 <code>squeeze</code>、 交换维度 <code>transpose</code>、 复制数据 <code>tile</code> 等函数</p>
<p>Batch 维度：为了实现维度变换，我们需要将原始数据插入一个新的维度，并把它定义为 Batch 维度，然后在 Batch 维度对数据进行相关操作，得到变换后的新的数据。这一系列的操作就是维度变换操作。</p>
<h4 id="1改变视图-reshape"><a class="markdownIt-Anchor" href="#1改变视图-reshape"></a> 1.改变视图 reshape</h4>
<p>张量的视图（View）：就是我们理解张量的方式，比如 shape 为[2,4,4,3]的张量A，从逻辑上可以理解为 2 张图片，每张图片 4 行 4 列，每个位置有 RGB 3 个通道的数据</p>
<p>张量的存储（Storage）：体现在张量在内存上保存为一段连续的内存区域，对于同样的存储，我们可以有不同的理解方式，比如上述张量A，我们可以在不改变张量的存储下，将张量A理解为 2个样本，每个样本的特征为长度 48 的向量</p>
<p>同一个存储，从不同的角度观察数据，可以产生不同的视图， 这就是存储与视图的关系。 视图的产生是非常灵活的，但需要保证是合理。</p>
<p>通过 <code>tf.range()</code>模拟生成一个向量数据，并通过<code>tf.reshape</code>视图改变函数产生不同的视图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x=tf.range(<span class="number">96</span>) <span class="comment"># 生成向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x=tf.reshape(x,[<span class="number">2</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">3</span>]) <span class="comment"># 改变 x 的视图，获得 4D 张量，存储并未改变</span></span><br><span class="line"><span class="comment"># 可以观察到数据仍然是 0~95 的顺序，可见数据并未改变，改变的是数据的结构</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">11</span>, shape=(<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>), dtype=int32, numpy=</span><br><span class="line">array([[[[ <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">         [ <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">         [ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]],...</span><br></pre></td></tr></table></figure>
<p>在存储数据时，内存只能以平铺方式按序写入内存，因此视图的层级关系需要人为管理。为了方便表达，一般把张量 shape 列表中相对靠左侧的维度叫作大维度， shape 列表中相对靠右侧的维度叫作小维度（如[2,4,4,3]的张量中，图片数量维度与通道数量相比，图片数量叫作大维度，通道数叫作小维度）</p>
<p>改变视图操作在提供便捷性的同时，也会带来很多逻辑隐患，主要的原因是改变视图操作的默认前提是<u>存储不需要改变</u>，否则改变视图操作就是非法的</p>
<blockquote>
<p>张量A按着初始视图[b,h,w,c]写入的内存布局，改变A的理解方式，它可以有多种合法的理解方式：</p>
<ul>
<li>[b,h∙w,c]张量理解为b张图片，h∙w个像素点，c个通道</li>
<li>[b,h,w∙c]张量理解为b张图片，h行，每行的特征长度为w∙c</li>
<li>[b,h∙w∙c]张量理解为b张图片，每张图片的特征长度为h∙w∙c</li>
</ul>
<p>从语法上来说， 视图变换只需要满足新视图的元素总量与存储区域大小相等即可。正是由于视图的设计的语法约束很少，使得在改变视图时容易出现逻辑隐患。</p>
<p>不合法的视图变换：</p>
<p>例如，如果定义新视图为[b,w,h,c]，[b,c,h*w]或者[b,c,h,w]等时，张量的存储顺序需要改变， 如果不同步更新张量的存储顺序，那么恢复出的数据将与新视图不一致，从而导致数据错乱。</p>
<p>这需要用户理解数据，才能判断操作是否合法。我们会在“交换维度”一节介绍如何改变张量的存储</p>
</blockquote>
<p>在通过 reshape 改变视图时，必须始终记住张量的存储顺序，新视图的维度顺序不能与存储顺序相悖，否则需要通过<strong>交换维度</strong>操作将存储顺序同步过来</p>
<p>在 TensorFlow 中，可以通过张量的 <code>ndim</code> 和 <code>shape</code> 成员属性获得张量的维度数和形<br>
状：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.ndim,x.shape <span class="comment"># 获取张量的维度数和形状列表</span></span><br><span class="line">(<span class="number">4</span>, TensorShape([<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>]))</span><br></pre></td></tr></table></figure>
<hr>
<p>通过 <code>tf.reshape(x, new_shape)</code>，可以将张量的视图任意地合法改变：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reshape(x,[<span class="number">2</span>,<span class="number">-1</span>])</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">520</span>, shape=(<span class="number">2</span>, <span class="number">48</span>), dtype=int32, numpy=</span><br><span class="line">array([[ <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>,</span><br><span class="line">        <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>, <span class="number">31</span>,…</span><br><span class="line">        <span class="number">80</span>, <span class="number">81</span>, <span class="number">82</span>, <span class="number">83</span>, <span class="number">84</span>, <span class="number">85</span>, <span class="number">86</span>, <span class="number">87</span>, <span class="number">88</span>, <span class="number">89</span>, <span class="number">90</span>, <span class="number">91</span>, <span class="number">92</span>, <span class="number">93</span>, <span class="number">94</span>, <span class="number">95</span>]])&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>参数-1：当前轴上长度需要根据张量总元素不变的法则自动推导（该处推导成(2*4*4*3)/2=48）</li>
</ul>
<p>再次改变数据的视图为[2,16,3] ，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reshape(x,[<span class="number">2</span>,<span class="number">-1</span>,<span class="number">3</span>])</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">526</span>, shape=(<span class="number">2</span>, <span class="number">16</span>, <span class="number">3</span>), dtype=int32, numpy=</span><br><span class="line">array([[[ <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], …</span><br><span class="line">       [<span class="number">45</span>, <span class="number">46</span>, <span class="number">47</span>]],</span><br><span class="line">      [[<span class="number">48</span>, <span class="number">49</span>, <span class="number">50</span>],…</span><br><span class="line">       [<span class="number">93</span>, <span class="number">94</span>, <span class="number">95</span>]]])&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>上述一系列连续变换视图操作中，张量的存储顺序始终没有改变，数据在内存中仍然是按着初始写入的顺序0,1,2, ⋯ ,95保存</li>
</ul>
<h4 id="2增删维度"><a class="markdownIt-Anchor" href="#2增删维度"></a> 2.增删维度</h4>
<h5 id="增加维度"><a class="markdownIt-Anchor" href="#增加维度"></a> 增加维度</h5>
<p>通过 <code>tf.expand_dims(x, axis)</code>可在指定的 <strong>axis 轴前</strong>可以插入一个新的维度（长度为1）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.uniform([<span class="number">28</span>,<span class="number">28</span>],maxval=<span class="number">10</span>,dtype=tf.int32) <span class="comment"># 产生矩阵</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">11</span>, shape=(<span class="number">28</span>, <span class="number">28</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">6</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">7</span>],...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.expand_dims(x,axis=<span class="number">2</span>) <span class="comment"># axis=2 表示宽维度后面的一个维度</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">13</span>, shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>), dtype=int32, numpy=</span><br><span class="line">array([[[<span class="number">6</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>],</span><br><span class="line">        [<span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>],</span><br><span class="line">        [<span class="number">3</span>],...</span><br></pre></td></tr></table></figure>
<ul>
<li>增加一个长度为 1 的维度相当于给原有的数据添加一个新维度的概念，数据并不发生改变，仅仅是改变数据的理解方式，因此它其实可以理解为改变视图的一种特殊方式</li>
</ul>
<p>需要注意的是， <code>tf.expand_dims</code> 的 axis 为<strong>正</strong>时，表示在当前维度<strong>之前</strong>插入一个新维度； 为<strong>负</strong>时，表示当前维度<strong>之后</strong>插入一个新的维度。以[b,h,w,c]张量为例，不同 axis 参数的实际插入位置如下所示：</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200503172458500.png" alt="增加维度 axis 参数位置示意图"></p>
<h5 id="删除维度"><a class="markdownIt-Anchor" href="#删除维度"></a> 删除维度</h5>
<p>与增加维度一样，删除维度只能删除<strong>长度为1</strong>的维度，也不会改变张量的存储。</p>
<p>通过 <code>tf.squeeze(x, axis)</code>函数， axis 参数为待删除的维度的索引号：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据同上一个例子</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.squeeze(x, axis=<span class="number">2</span>) <span class="comment"># 删除图片通道数维度</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">588</span>, shape=(<span class="number">28</span>, <span class="number">28</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">8</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">9</span>, <span class="number">7</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">0</span>],...</span><br></pre></td></tr></table></figure>
<p>如果不指定维度参数 axis，即 <code>tf.squeeze(x)</code>， 那么它会默认删除所有长度为 1 的维度</p>
<h4 id="3交换维度"><a class="markdownIt-Anchor" href="#3交换维度"></a> 3.交换维度</h4>
<p>在保持维度顺序不变的条件下， 仅仅改变张量的理解方式是不够的，有时需要直接调整的存储顺序，即<strong>交换维度</strong>(Transpose)。通过交换维度操作，改变了张量的存储顺序，同时也改变了张量的视图</p>
<p>我们以图片格式[b,h,w,c]转换到图片格式[b,c,h,w]为例，介绍使用 <code>tf.transpose(x, perm)</code>函数完成维度交换操作，其中参数 perm表示新维度的顺序 List。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">2</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.transpose(x,perm=[<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>]) <span class="comment"># 交换维度</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">603</span>, shape=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>), dtype=float32, numpy=</span><br><span class="line">array([[[[<span class="number">-1.93072677e+00</span>, <span class="number">-4.80163872e-01</span>, <span class="number">-8.85614634e-01</span>, ...,</span><br><span class="line">          <span class="number">1.49124235e-01</span>, <span class="number">1.16427064e+00</span>, <span class="number">-1.47740364e+00</span>],</span><br><span class="line">         [<span class="number">-1.94761145e+00</span>, <span class="number">7.26879001e-01</span>, <span class="number">-4.41877693e-01</span>, ...</span><br></pre></td></tr></table></figure>
<ul>
<li>图片张量 shape 为[2,32,32,3]，“图片数量、行、列、通道数” 的维度索引分别为 0、 1、 2、 3。交换为[b,c,h,w]格式，新维度的排序为“图片数量、通道数、行、列”，对应的索引号为[0,3,1,2]</li>
<li>通过 <code>tf.transpose</code> 维度交换后，张量的存储顺序已经改变， 视图也随之改变， 后续的所有操作必须基于新的存续顺序和视图进行。 相对于改变视图操作，维度交换操作的<strong>计算代价更高</strong></li>
</ul>
<h4 id="4复制数据"><a class="markdownIt-Anchor" href="#4复制数据"></a> 4.复制数据</h4>
<p>可以通过<code>tf.tile(x, multiples)</code>函数完成数据在指定维度上的复制操作， multiples 分别指定了每个维度上面的复制倍数，对应位置为 1 表明不复制，为 2 表明新长度为原来长度的2 倍，即数据复制一份，以此类推</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.constant([<span class="number">1</span>,<span class="number">2</span>]) <span class="comment"># 创建向量 b</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.expand_dims(b, axis=<span class="number">0</span>) <span class="comment"># 插入新维度，变成矩阵</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">645</span>, shape=(<span class="number">1</span>, <span class="number">2</span>), dtype=int32, numpy=array([[<span class="number">1</span>, <span class="number">2</span>]])&gt;</span><br><span class="line"><span class="comment"># 在 Batch 维度上复制数据 1 份，实现如下：</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.tile(b, multiples=[<span class="number">2</span>,<span class="number">1</span>]) <span class="comment"># 样本维度上复制一份</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">648</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>]])&gt;</span><br></pre></td></tr></table></figure>
<p><code>tf.tile</code> 会创建一个<strong>新的张量</strong>来保存复制后的张量，会涉及大量数据的读写 IO 运算，计算代价相对较高</p>
<h3 id="8-broadcasting"><a class="markdownIt-Anchor" href="#8-broadcasting"></a> 8、Broadcasting</h3>
<p>Broadcasting 称为广播机制(或自动扩展机制)，它是一种轻量级的张量复制手段，在逻辑上扩展张量数据的形状， 但是只会在需要时才会执行实际存储复制操作。</p>
<p>对于用户来说， Broadcasting 和 tf.tile 复制的最终效果是一样的，操作对用户透明，但是 Broadcasting 机制节省了大量计算资源，建议在运算过程中尽可能地利用 Broadcasting 机制提高计算效率</p>
<p>考虑上述的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mi mathvariant="normal">@</mi><mi>W</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">Y=X@W+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord">@</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>的例子， <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mi mathvariant="normal">@</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">X@W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord">@</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>的 shape 为[2,3]，b的 shape 为[3]，可以通过结合 tf.expand_dims 和 tf.tile 手动完成复制数据操作，将b变换为[2,3]，然后与 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mi mathvariant="normal">@</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">X@W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord">@</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>完成相加运算。但实际上，直接将 shape 为[2,3]与[3]的b相加也是合法的，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">2</span>,<span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>w = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.random.normal([<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x@w+b <span class="comment"># 不同 shape 的张量直接相加</span></span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>会自动调用 Broadcasting函数 <code>tf.broadcast_to(x, new_shape)</code>， 将两者 shape 扩张为相同的[2,3]， 即上式可以等效为</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x@w + tf.broadcast_to(b,[<span class="number">2</span>,<span class="number">3</span>]) <span class="comment"># 手动扩展，并相加</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>操作符+在遇到 shape 不一致的 2 个张量时，会自动考虑将 2 个张量自动扩展到一致的 shape，然后再调用 tf.add 完成张量相加运算</p>
</li>
</ul>
<p>所有的运算都需要在正确逻辑（满足Broadcasting 设计的核心思想）下进行， Broadcasting 机制并不会扰乱正常的计算逻辑， 它只会针对于最常见的场景自动完成增加维度并复制数据的功能， 提高开发效率和运行效率。</p>
<blockquote>
<p>Broadcasting 机制的核心思想是普适性，即同一份数据能普遍适合于其他位置。 在验证普适性之前，需要先将张量 shape 靠右对齐， 然后进行普适性判断：</p>
<ul>
<li>对于<strong>长度为1</strong>的维度，默认这个数据普遍适合于当前维度的其他位置</li>
<li>对于<strong>不存在</strong>的维度， 则在增加新维度后默认当前数据也是普适于新维度的， 从而可以扩展为更多维度数、 任意长度的张量形状</li>
</ul>
</blockquote>
<p>在进行张量运算时，有些运算在处理不同 shape 的张量时，会隐式地自动调用Broadcasting 机制，如+， -， *， /等运算，将参与运算的张量 Broadcasting 成一个公共shape，再进行相应的计算。</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200503181730592.png" alt="加法运算时自动Broadcasting示意图"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.random.normal([<span class="number">2</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.random.normal([<span class="number">32</span>,<span class="number">32</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a+b,a-b,a*b,a/b <span class="comment"># 测试加减乘除运算的 Broadcasting 机制</span></span><br></pre></td></tr></table></figure>
<ul>
<li>这些运算都能 Broadcasting 成[2,32,32,32]的公共 shape，再进行运算</li>
</ul>
<h3 id="9-数学运算"><a class="markdownIt-Anchor" href="#9-数学运算"></a> 9、数学运算</h3>
<h4 id="1加减乘除"><a class="markdownIt-Anchor" href="#1加减乘除"></a> 1.加减乘除</h4>
<p>加、 减、 乘、 除是最基本的数学运算，分别通过 <code>tf.add</code>, <code>tf.subtract</code>, <code>tf.multiply</code>, <code>tf.divide</code>函数实现， TensorFlow 已经重载了+、 - 、 ∗ 、 /运算符，推荐直接使用运算符来完成加、 减、 乘、 除运算</p>
<p>整除和余除也是常见的运算之一，分别通过<code>//</code>和<code>%</code>运算符实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.range(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.constant(<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a//b <span class="comment"># 整除运算</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">115</span>, shape=(<span class="number">5</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>])&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a%b <span class="comment"># 余除运算</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">117</span>, shape=(<span class="number">5</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])&gt;</span><br></pre></td></tr></table></figure>
<h4 id="2乘方"><a class="markdownIt-Anchor" href="#2乘方"></a> 2.乘方</h4>
<p>通过 <code>tf.pow(x, a)</code>可以方便地完成<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><msup><mi>x</mi><mi>a</mi></msup></mrow><annotation encoding="application/x-tex">y=x^a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.664392em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">a</span></span></span></span></span></span></span></span></span></span></span>的乘方运算，也可以通过运算符<code>**</code>实现<code>x∗∗a</code>运算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.range(<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.pow(x,<span class="number">3</span>) <span class="comment"># 乘方运算</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">124</span>, shape=(<span class="number">4</span>,), dtype=int32, numpy=array([ <span class="number">0</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">27</span>])&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x**<span class="number">2</span> <span class="comment"># 乘方运算符</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">127</span>, shape=(<span class="number">4</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>])&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x=tf.constant([<span class="number">1.</span>,<span class="number">4.</span>,<span class="number">9.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x**(<span class="number">0.5</span>) <span class="comment"># 平方根</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">139</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>设置指数为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mi>a</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{a}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>形式， 即可实现<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mroot><mi>x</mi><mi>a</mi></mroot></mrow><annotation encoding="application/x-tex">\sqrt[a]{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.23972em;"></span><span class="mord sqrt"><span class="root"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.5516160000000001em;"><span style="top:-2.836336em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size6 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span></span></span></span></span></span></span></span><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8002800000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathdefault">x</span></span></span><span style="top:-2.76028em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"><path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.23972em;"><span></span></span></span></span></span></span></span></span>根号运算</li>
<li>对于常见的平方和平方根运算，可以使用 <code>tf.square(x)</code>和 <code>tf.sqrt(x)</code>实现</li>
</ul>
<h4 id="3指数和对数"><a class="markdownIt-Anchor" href="#3指数和对数"></a> 3.指数和对数</h4>
<p>通过 <code>tf.pow(a, x)</code>或者<code>**</code>运算符也可以方便地实现指数运算<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>a</mi><mi>x</mi></msup></mrow><annotation encoding="application/x-tex">a^x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.664392em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.constant([<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">2</span>**x <span class="comment"># 指数运算</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">179</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">2.</span>, <span class="number">4.</span>, <span class="number">8.</span>], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.exp(<span class="number">1.</span>) <span class="comment"># 自然指数运算</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">182</span>, shape=(), dtype=float32, numpy=<span class="number">2.7182817</span>&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>对于自然指数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>e</mi><mi>x</mi></msup></mrow><annotation encoding="application/x-tex">e^x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.664392em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span>， 可以通过 <code>tf.exp(x)</code>实现</li>
</ul>
<p>自然对数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mi>e</mi></msub><mi>x</mi></mrow><annotation encoding="application/x-tex">log_ex</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">x</span></span></span></span>可以通过 <code>tf.math.log(x)</code>实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x=tf.exp(<span class="number">3.</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.math.log(x) <span class="comment"># 对数运算</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">186</span>, shape=(), dtype=float32, numpy=<span class="number">3.0</span>&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>如果希望计算其它底数的对数，可以根据对数的换底公式<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mi>a</mi></msub><mi>x</mi><mo>=</mo><mfrac><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mi>e</mi></msub><mi>x</mi></mrow><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mi>e</mi></msub><mi>a</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">log_ax=\frac{log_ex}{log_ea}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.4133239999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">o</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mathdefault mtight">a</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">o</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mathdefault mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>间接实现</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.constant([<span class="number">1.</span>,<span class="number">2.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = <span class="number">10</span>**x</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.math.log(x)/tf.math.log(<span class="number">10.</span>) <span class="comment"># 换底公式</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">6</span>, shape=(<span class="number">2</span>,), dtype=float32, numpy=array([<span class="number">1.</span>, <span class="number">2.</span>], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="4矩阵乘法"><a class="markdownIt-Anchor" href="#4矩阵乘法"></a> 4.矩阵乘法</h4>
<p>通过<code>@</code>运算符可以方便的实现矩阵相乘，还可以通过 <code>tf.matmul(a, b)</code>函数实现</p>
<p>需要注意的是， TensorFlow 中的矩阵相乘可以使用批量方式，也就是张量A和B的维度数可以大于 2。当张量A和B维度数大于 2 时， TensorFlow 会选择A和B的最后两个维度进行矩阵相乘，前面所有的维度都视作Batch 维度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">32</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a@b <span class="comment"># 批量形式的矩阵相乘</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">236</span>, shape=(<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line">array([[[[<span class="number">-1.66706240e+00</span>, <span class="number">-8.32602978e+00</span>],</span><br><span class="line">         [ <span class="number">9.83304405e+00</span>, <span class="number">8.15909767e+00</span>],</span><br><span class="line">         [ <span class="number">6.31014729e+00</span>, <span class="number">9.26124632e-01</span>],...</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>矩阵相乘函数同样支持自动 Broadcasting 机制</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>,<span class="number">32</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.random.normal([<span class="number">32</span>,<span class="number">16</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.matmul(a,b) <span class="comment"># 先自动扩展，再矩阵相乘</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">264</span>, shape=(<span class="number">4</span>, <span class="number">28</span>, <span class="number">16</span>), dtype=float32, numpy=</span><br><span class="line">array([[[<span class="number">-1.11323869e+00</span>, <span class="number">-9.48194981e+00</span>, <span class="number">6.48123884e+00</span>, ...,</span><br><span class="line">         <span class="number">6.53280640e+00</span>, <span class="number">-3.10894990e+00</span>, <span class="number">1.53050375e+00</span>],</span><br><span class="line">        [ <span class="number">4.35898495e+00</span>, <span class="number">-1.03704405e+01</span>, <span class="number">8.90656471e+00</span>, ...,</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="tensorflow2进阶操作"><a class="markdownIt-Anchor" href="#tensorflow2进阶操作"></a> TensorFlow2进阶操作</h2>
<h3 id="1-合并与分割"><a class="markdownIt-Anchor" href="#1-合并与分割"></a> 1、合并与分割</h3>
<h4 id="1合并"><a class="markdownIt-Anchor" href="#1合并"></a> 1.合并</h4>
<p>合并：将多个张量在某个维度上合并为一个张量</p>
<p>张量的合并可以使用拼接(Concatenate)和堆叠(Stack)操作实现：</p>
<ul>
<li>拼接：不会产生新的维度， 仅在现有的维度上合并</li>
<li>堆叠：会创建新维度</li>
</ul>
<p>选择使用拼接还是堆叠操作来合并张量，取决于具体的场景是否需要创建新维度</p>
<h5 id="拼接"><a class="markdownIt-Anchor" href="#拼接"></a> 拼接</h5>
<p>通过<code>tf.concat(tensors, axis)</code>函数拼接张量，其中参数tensors 保存了所有需要合并的张量 List， axis 参数指定需要合并的维度索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.random.normal([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>]) <span class="comment"># 模拟成绩册 A</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.random.normal([<span class="number">6</span>,<span class="number">35</span>,<span class="number">8</span>]) <span class="comment"># 模拟成绩册 B</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.concat([a,b],axis=<span class="number">0</span>) <span class="comment"># 拼接合并成绩册</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">13</span>, shape=(<span class="number">10</span>, <span class="number">35</span>, <span class="number">8</span>), dtype=float32, numpy=</span><br><span class="line">array([[[ <span class="number">1.95299834e-01</span>, <span class="number">6.87859178e-01</span>, <span class="number">-5.80048323e-01</span>, ...,</span><br><span class="line">          <span class="number">1.29430830e+00</span>, <span class="number">2.56610274e-01</span>, <span class="number">-1.27798581e+00</span>],</span><br><span class="line">        [ <span class="number">4.29753691e-01</span>, <span class="number">9.11329567e-01</span>, <span class="number">-4.47975427e-01</span>, ...,</span><br></pre></td></tr></table></figure>
<ul>
<li>从语法上来说，拼接合并操作可以在任意的维度上进行，唯一的约束是非合并维度的长度<strong>必须一致</strong></li>
</ul>
<h5 id="堆叠"><a class="markdownIt-Anchor" href="#堆叠"></a> 堆叠</h5>
<p>使用<code>tf.stack(tensors, axis)</code>可以堆叠方式合并多个张量，通过 tensors 列表表示， 参数axis 指定新维度插入的位置（用法与 <code>tf.expand_dims</code> 一致，当axis ≥ 0时，在 axis之前插入； 当axis &lt; 0时，在 axis 之后插入新维度）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.random.normal([<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.random.normal([<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.stack([a,b],axis=<span class="number">0</span>) <span class="comment"># 堆叠合并为 2 个班级，班级维度插入在最前</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">55</span>, shape=(<span class="number">2</span>, <span class="number">35</span>, <span class="number">8</span>), dtype=float32, numpy=</span><br><span class="line">array([[[ <span class="number">3.68728966e-01</span>, <span class="number">-8.54765773e-01</span>, <span class="number">-4.77824420e-01</span>,</span><br><span class="line">         <span class="number">-3.83714020e-01</span>, <span class="number">-1.73216307e+00</span>, <span class="number">2.03872994e-02</span>,</span><br><span class="line">          <span class="number">2.63810277e+00</span>, <span class="number">-1.12998331e+00</span>],...</span><br></pre></td></tr></table></figure>
<ul>
<li>需要所有待合并的张量 shape <strong>完全一致</strong>才可合并</li>
</ul>
<h4 id="2分割"><a class="markdownIt-Anchor" href="#2分割"></a> 2.分割</h4>
<p>合并操作的逆过程就是分割，将一个张量分拆为多个张量</p>
<p>通过<code>tf.split(x, num_or_size_splits, axis)</code>可以完成张量的分割操作</p>
<ul>
<li>x：待分割张量</li>
<li>num_or_size_splits：切割方案。当 num_or_size_splits 为单个数值时，如 10，表<br>
示等长切割为 10 份；当 num_or_size_splits 为 List 时， List 的每个元素表示每份的长度，如[2,4,2,2]表示切割为 4 份，每份的长度依次是 2、 4、 2、 2</li>
<li>axis：指定分割的维度索引号</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">10</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line"><span class="comment"># 等长切割为 10 份</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result = tf.split(x, num_or_size_splits=<span class="number">10</span>, axis=<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(result) <span class="comment"># 返回的列表为 10 个张量的列表</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result[<span class="number">0</span>] <span class="comment"># 查看第一个班级的成绩册张量</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">136</span>, shape=(<span class="number">1</span>, <span class="number">35</span>, <span class="number">8</span>), dtype=float32, numpy=</span><br><span class="line">array([[[<span class="number">-1.7786729</span> , <span class="number">0.2970506</span> , <span class="number">0.02983334</span>, <span class="number">1.3970423</span> ,</span><br><span class="line">          <span class="number">1.315918</span> , <span class="number">-0.79110134</span>, <span class="number">-0.8501629</span> , <span class="number">-1.5549672</span> ],</span><br><span class="line">        [ <span class="number">0.5398711</span> , <span class="number">0.21478991</span>, <span class="number">-0.08685189</span>, <span class="number">0.7730989</span> ,...</span><br></pre></td></tr></table></figure>
<ul>
<li>仍保留了第一个维度</li>
</ul>
<p>如果希望在某个维度上全部按长度为 1 的方式分割，还可以使用<code>tf.unstack(x, axis)</code>函数。这种方式是<code>tf.split</code>的一种特殊情况，切割长度固定为 1，只需要指定切割维度的索引号即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">10</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result = tf.unstack(x,axis=<span class="number">0</span>) <span class="comment"># Unstack 为长度为 1 的张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(result) <span class="comment"># 返回 10 个张量的列表</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result[<span class="number">0</span>] <span class="comment"># 第一个班级</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">166</span>, shape=(<span class="number">35</span>, <span class="number">8</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">-0.2034383</span> , <span class="number">1.1851563</span> , <span class="number">0.25327438</span>, <span class="number">-0.10160723</span>, <span class="number">2.094969</span> , <span class="number">-0.8571669</span> , <span class="number">-0.48985648</span>, <span class="number">0.55798006</span>],...</span><br></pre></td></tr></table></figure>
<ul>
<li>通过<code>tf.unstack</code>切割后，shape 变为[35,8]，即第一个维度消失了，这是与tf.split区别之处</li>
</ul>
<h3 id="2-数据统计"><a class="markdownIt-Anchor" href="#2-数据统计"></a> 2、数据统计</h3>
<h4 id="1向量范数"><a class="markdownIt-Anchor" href="#1向量范数"></a> 1.向量范数</h4>
<p>向量范数(Vector Norm)是表征向量“长度”的一种度量方法， 它可以推广到张量上。在神经网络中，常用来表示张量的权值大小，梯度大小等。常用的向量范数有：</p>
<ul>
<li>L1 范数，定义为向量x的所有元素绝对值之和：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mo fence="true">∥</mo><mi>x</mi><mo fence="true">∥</mo></mrow><mn>1</mn></msub><mo>=</mo><msub><mo>∑</mo><mi>i</mi></msub><mrow><mo fence="true">∣</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence="true">∣</mo></mrow></mrow><annotation encoding="application/x-tex">\left \| x \right \|_1=\sum_{i}\left | x_i \right |</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.29969999999999997em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∥</span><span class="mord mathdefault">x</span><span class="mclose delimcenter" style="top:0em;">∥</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151408em;"><span style="top:-2.4003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29969999999999997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0497100000000001em;vertical-align:-0.29971000000000003em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span></span></span></span></li>
<li>L2 范数，定义为向量x的所有元素的平方和，再开根号：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mo fence="true">∥</mo><mi>x</mi><mo fence="true">∥</mo></mrow><mn>2</mn></msub><mo>=</mo><msqrt><mrow><msub><mo>∑</mo><mi>i</mi></msub><msup><mrow><mo fence="true">∣</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence="true">∣</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\left \| x \right \|_2=\sqrt{\sum_{i}\left | x_i \right |^2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.29969999999999997em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∥</span><span class="mord mathdefault">x</span><span class="mclose delimcenter" style="top:0em;">∥</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151408em;"><span style="top:-2.4003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29969999999999997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.8400000000000003em;vertical-align:-0.527851em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3121490000000002em;"><span class="svg-align" style="top:-3.8em;"><span class="pstrut" style="height:3.8em;"></span><span class="mord" style="padding-left:1em;"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.954008em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-3.2721489999999998em;"><span class="pstrut" style="height:3.8em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.8800000000000001em;"><svg width="400em" height="1.8800000000000001em" viewbox="0 0 400000 1944" preserveaspectratio="xMinYMin slice"><path d="M1001,80H400000v40H1013.1s-83.4,268,-264.1,840c-180.7,
572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,
-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744c-10,12,-21,25,-33,39s-32,39,-32,39
c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30c26.7,-32.7,52,-63,76,-91s52,-60,52,-60
s208,722,208,722c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,
-658.5c53.7,-170.3,84.5,-266.8,92.5,-289.5c4,-6.7,10,-10,18,-10z
M1001 80H400000v40H1013z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.527851em;"><span></span></span></span></span></span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord">∞</span></span></span></span>-范数，定义为向量x的所有元素绝对值的最大值：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mo fence="true">∥</mo><mi>x</mi><mo fence="true">∥</mo></mrow><mi mathvariant="normal">∞</mi></msub><mo>=</mo><mi>m</mi><mi>a</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mrow><mo fence="true">∣</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence="true">∣</mo></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\left \| x \right \|_\infty=max_i(\left | x_i \right |)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.29969999999999997em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∥</span><span class="mord mathdefault">x</span><span class="mclose delimcenter" style="top:0em;">∥</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.0016920000000000268em;"><span style="top:-2.4003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">∞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29969999999999997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span><span class="mclose">)</span></span></span></span></li>
</ul>
<p>对于矩阵和张量，同样可以利用向量范数的计算公式，等价于将矩阵和张量打平成向量后计算</p>
<p>通过<code>tf.norm(x, ord)</code>求解张量的 L1、 L2、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord">∞</span></span></span></span>等范数，其中参数ord 指定为 1、 2时计算 L1、 L2 范数，指定为<code>np.inf</code>时计算<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord">∞</span></span></span></span>-范数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.ones([<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.norm(x,ord=<span class="number">1</span>) <span class="comment"># 计算 L1 范数</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">183</span>, shape=(), dtype=float32, numpy=<span class="number">4.0</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.norm(x,ord=<span class="number">2</span>) <span class="comment"># 计算 L2 范数</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">189</span>, shape=(), dtype=float32, numpy=<span class="number">2.0</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.norm(x,ord=np.inf) <span class="comment"># 计算∞范数</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">194</span>, shape=(), dtype=float32, numpy=<span class="number">1.0</span>&gt;</span><br></pre></td></tr></table></figure>
<h4 id="2最值-均值-和"><a class="markdownIt-Anchor" href="#2最值-均值-和"></a> 2.最值、均值、和</h4>
<p>通过 <code>tf.reduce_max</code>、 <code>tf.reduce_min</code>、 <code>tf.reduce_mean</code>、 <code>tf.reduce_sum</code> 函数可以求解张量在某个维度上的最大、最小、 均值、和，也可以求全局最大、最小、均值、和信息</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设第一个维度为样本数量，第二个维度为当前样本分别属于 10 个类别的概率</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">4</span>,<span class="number">10</span>]) <span class="comment"># 模型生成概率</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_max(x,axis=<span class="number">1</span>) <span class="comment"># 统计概率维度上的最大值</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">203</span>, shape=(<span class="number">4</span>,), dtype=float32, numpy=array([<span class="number">1.2410722</span> , <span class="number">0.88495886</span>, <span class="number">1.4170984</span> , <span class="number">0.9550192</span> ], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_min(x,axis=<span class="number">1</span>) <span class="comment"># 统计概率维度上的最小值</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">206</span>, shape=(<span class="number">4</span>,), dtype=float32, numpy=array([<span class="number">-0.27862206</span>, <span class="number">-2.4480672</span> , <span class="number">-1.9983795</span> , <span class="number">-1.5287997</span> ], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_mean(x,axis=<span class="number">1</span>) <span class="comment"># 统计概率维度上的均值</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">209</span>, shape=(<span class="number">4</span>,), dtype=float32, numpy=array([ <span class="number">0.39526337</span>, <span class="number">-0.17684573</span>, <span class="number">-0.148988</span> , <span class="number">-0.43544054</span>], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计全局的最大、最小、均值、和，返回的张量均为标量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">4</span>,<span class="number">10</span>]) <span class="comment"># 模型生成概率</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_max(x),tf.reduce_min(x),tf.reduce_mean(x)</span><br><span class="line">(&lt;tf.Tensor: id=<span class="number">218</span>, shape=(), dtype=float32, numpy=<span class="number">1.8653786</span>&gt;, &lt;tf.Tensor: id=<span class="number">220</span>, shape=(), dtype=float32, numpy=<span class="number">-1.9751656</span>&gt;, &lt;tf.Tensor: id=<span class="number">222</span>, shape=(), dtype=float32, numpy=<span class="number">0.014772797</span>&gt;)</span><br></pre></td></tr></table></figure>
<ul>
<li>不指定 axis 参数时， <code>tf.reduce_*</code>函数会求解出全局元素的最大、最小、 均值、和等数据</li>
</ul>
<p>通过 <code>tf.argmax(x, axis)</code>和 <code>tf.argmin(x, axis)</code>可以求解在 axis 轴上， x 的最大值、 最小值所在的索引号</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># out：</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">257</span>, shape=(<span class="number">2</span>, <span class="number">10</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">0.18773547</span>, <span class="number">0.1510464</span> , <span class="number">0.09431915</span>, <span class="number">0.13652141</span>, <span class="number">0.06579739</span>,</span><br><span class="line">        <span class="number">0.02033597</span>, <span class="number">0.06067333</span>, <span class="number">0.0666793</span> , <span class="number">0.14594753</span>, <span class="number">0.07094406</span>],</span><br><span class="line">       [<span class="number">0.5092072</span> , <span class="number">0.03887136</span>, <span class="number">0.0390687</span> , <span class="number">0.01911005</span>, <span class="number">0.03850609</span>,</span><br><span class="line">        <span class="number">0.03442522</span>, <span class="number">0.08060656</span>, <span class="number">0.10171875</span>, <span class="number">0.08244187</span>, <span class="number">0.05604421</span>]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pred = tf.argmax(out, axis=<span class="number">1</span>) <span class="comment"># 选取概率最大的位置</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">262</span>, shape=(<span class="number">2</span>,), dtype=int64, numpy=array([<span class="number">0</span>, <span class="number">0</span>], dtype=int64)&gt;</span><br></pre></td></tr></table></figure>
<h3 id="3-张量比较"><a class="markdownIt-Anchor" href="#3-张量比较"></a> 3、张量比较</h3>
<p>通过<code>tf.equal(a, b)</code>(或<code>tf.math.equal(a, b)</code>，两者等价)函数可以比较2个张量是否相等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pred：</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">272</span>, shape=(<span class="number">100</span>,), dtype=int64, numpy=</span><br><span class="line">array([<span class="number">0</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">4</span>,...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = tf.random.uniform([<span class="number">100</span>],dtype=tf.int64,maxval=<span class="number">10</span>)</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">281</span>, shape=(<span class="number">100</span>,), dtype=int64, numpy=</span><br><span class="line">array([<span class="number">0</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>,...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = tf.equal(pred,y) <span class="comment"># 预测值与真实值比较，返回布尔类型的张量</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">288</span>, shape=(<span class="number">100</span>,), dtype=bool, numpy=</span><br><span class="line">array([<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>,...</span><br></pre></td></tr></table></figure>
<p>相关函数汇总表：</p>
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">比较逻辑</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">tf.equal(a, b)</td>
<td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>=</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a=b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span></td>
</tr>
<tr>
<td style="text-align:center">tf.math.greater(a, b)</td>
<td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>&gt;</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a&gt;b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span></td>
</tr>
<tr>
<td style="text-align:center">tf.math.less(a, b)</td>
<td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>&lt;</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a&lt;b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span></td>
</tr>
<tr>
<td style="text-align:center">tf.math.greater_equal(a, b)</td>
<td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>⩾</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a\geqslant b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7733399999999999em;vertical-align:-0.13667em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel amsrm">⩾</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span></td>
</tr>
<tr>
<td style="text-align:center">tf.math.less_equal(a, b)</td>
<td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>⩽</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a\leqslant b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7733399999999999em;vertical-align:-0.13667em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel amsrm">⩽</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span></td>
</tr>
<tr>
<td style="text-align:center">tf.math.not_equal(a, b)</td>
<td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mi mathvariant="normal">≠</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">a\neq b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="rlap"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="inner"><span class="mrel"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span></td>
</tr>
<tr>
<td style="text-align:center">tf.math.is_nan(a, b)</td>
<td style="text-align:center"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>=</mo><mi>n</mi><mi>a</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">a=nan</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span></span></span></span></td>
</tr>
</tbody>
</table>
<h3 id="4-复制和填充"><a class="markdownIt-Anchor" href="#4-复制和填充"></a> 4、复制和填充</h3>
<h4 id="1填充"><a class="markdownIt-Anchor" href="#1填充"></a> 1.填充</h4>
<p>之前我们介绍了通过复制的方式可以增加数据的长度，但是重复复制数据会破坏原有的数据结构，并不适合于此处。通常的做法是，在需要补充长度的数据开始或结束处填充足够数量的特定数值， 这些特定数值一般代表了<strong>无效意义</strong>，例如 0，使得填充后的长度满足系统要求。那么这种操作就叫作填充(Padding)</p>
<p>填充操作可以通过<code>tf.pad(x, paddings)</code>函数实现， 参数 paddings 是包含了多个[Left Padding, Right Padding]的嵌套方案 List，如[[0,0], [2,1], [1,2]]表示第一个维度不填<br>
充， 第二个维度左边(起始处)填充两个单元， 右边(结束处)填充一个单元， 第三个维度左边填充一个单元， 右边填充两个单元</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]) <span class="comment"># 第一个句子</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.constant([<span class="number">7</span>,<span class="number">8</span>,<span class="number">1</span>,<span class="number">6</span>]) <span class="comment"># 第二个句子</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.pad(b, [[<span class="number">0</span>,<span class="number">2</span>]]) <span class="comment"># 句子末尾填充 2 个 0</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">3</span>, shape=(<span class="number">6</span>,), dtype=int32, numpy=array([<span class="number">7</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">0</span>])&gt;</span><br></pre></td></tr></table></figure>
<h4 id="2复制"><a class="markdownIt-Anchor" href="#2复制"></a> 2.复制</h4>
<p>即<code>tf.tile()</code>函数。参考<code>基础操作-&gt;维度变换-&gt;复制数据</code>章节</p>
<h3 id="5-数据限幅"><a class="markdownIt-Anchor" href="#5-数据限幅"></a> 5、数据限幅</h3>
<p>可以通过<code>tf.maximum(x, a)</code>实现数据的下限幅，即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>∈</mo><mo stretchy="false">[</mo><mi>a</mi><mo separator="true">,</mo><mo>+</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x\in [a,+\infty )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">+</span><span class="mord">∞</span><span class="mclose">)</span></span></span></span>。可以通过<code>tf.minimum(x, a)</code>实现数据的上限幅，即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>∈</mo><mo stretchy="false">(</mo><mo>−</mo><mi mathvariant="normal">∞</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">x\in (-\infty ,a]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">−</span><span class="mord">∞</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">a</span><span class="mclose">]</span></span></span></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.range(<span class="number">9</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.maximum(x,<span class="number">2</span>) <span class="comment"># 下限幅到 2</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">48</span>, shape=(<span class="number">9</span>,), dtype=int32, numpy=array([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.minimum(x,<span class="number">7</span>) <span class="comment"># 上限幅到 7</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">41</span>, shape=(<span class="number">9</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>])&gt;</span><br></pre></td></tr></table></figure>
<p>通过组合<code>tf.maximum(x, a)</code>和<code>tf.minimum(x, b)</code>可以实现同时对数据的上下边界限幅，即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>∈</mo><mo stretchy="false">[</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">x\in [a,b]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">b</span><span class="mclose">]</span></span></span></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.range(<span class="number">9</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.minimum(tf.maximum(x,<span class="number">2</span>),<span class="number">7</span>) <span class="comment"># 限幅为 2~7</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">57</span>, shape=(<span class="number">9</span>,), dtype=int32, numpy=array([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>])&gt;</span><br></pre></td></tr></table></figure>
<p>更方便地，我们可以使用<code>tf.clip_by_value</code>函数实现上下限幅：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.range(<span class="number">9</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.clip_by_value(x,<span class="number">2</span>,<span class="number">7</span>) <span class="comment"># 限幅为 2~7</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">66</span>, shape=(<span class="number">9</span>,), dtype=int32, numpy=array([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>])&gt;</span><br></pre></td></tr></table></figure>
<h3 id="6-高级操作"><a class="markdownIt-Anchor" href="#6-高级操作"></a> 6、高级操作</h3>
<h4 id="1tfgather"><a class="markdownIt-Anchor" href="#1tfgather"></a> 1.tf.gather</h4>
<p><code>tf.gather</code>可以实现根据索引号收集数据的目的（与切片类似，但是对于<strong>不规则</strong>的索引方式，切片实现起来非常麻烦， 而<code>tf.gather</code>则更加方便）</p>
<ul>
<li>参数1：带收集的张量</li>
<li>参数2：指定需要收集的索引号</li>
<li>参数3：指定收集的维度</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.uniform([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>],maxval=<span class="number">100</span>,dtype=tf.int32) <span class="comment"># 成绩册张量（4个班级 35个学生 8门科目）</span></span><br><span class="line"><span class="comment"># 收集第 1,4,9,12,13,27 号同学成绩</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.gather(x,[<span class="number">0</span>,<span class="number">3</span>,<span class="number">8</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">26</span>],axis=<span class="number">1</span>)</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">87</span>, shape=(<span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>), dtype=int32, numpy=</span><br><span class="line">array([[[<span class="number">43</span>, <span class="number">10</span>, <span class="number">93</span>, <span class="number">85</span>, <span class="number">75</span>, <span class="number">87</span>, <span class="number">28</span>, <span class="number">19</span>],</span><br><span class="line">        [<span class="number">74</span>, <span class="number">11</span>, <span class="number">25</span>, <span class="number">64</span>, <span class="number">84</span>, <span class="number">89</span>, <span class="number">79</span>, <span class="number">85</span>],...</span><br></pre></td></tr></table></figure>
<p>索引号可以<strong>乱序</strong>排列，此时收集的数据也是<strong>对应顺序</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=tf.range(<span class="number">8</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=tf.reshape(a,[<span class="number">4</span>,<span class="number">2</span>]) <span class="comment"># 生成张量 a</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">115</span>, shape=(<span class="number">4</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">       [<span class="number">6</span>, <span class="number">7</span>]])&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.gather(a,[<span class="number">3</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>],axis=<span class="number">0</span>) <span class="comment"># 收集第 4,2,1,3 号元素</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">119</span>, shape=(<span class="number">4</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>]])&gt;</span><br></pre></td></tr></table></figure>
<h4 id="2tfgather_nd"><a class="markdownIt-Anchor" href="#2tfgather_nd"></a> 2.tf.gather_nd</h4>
<p>通过 tf.gather_nd 函数，可以通过指定每次采样点的多维坐标来实现采样多个点的目的（利用手动一个一个提取然后stack合并的方式也可以达到同样效果，但是效率极低）</p>
<ul>
<li>参数1：带收集的张量</li>
<li>参数2：指定的采样点的索引坐标</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.uniform([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>],maxval=<span class="number">100</span>,dtype=tf.int32) <span class="comment"># 成绩册张量（4个班级 35个学生 8门科目）</span></span><br><span class="line"><span class="comment"># 根据多维坐标收集数据</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.gather_nd(x,[[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">3</span>]])<span class="comment">#抽查第 2 个班级的第 2 个同学的所有科目，第 3 个班级的第 3 个同学的所有科目，第 4 个班级的第 4 个同学的所有科目</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">256</span>, shape=(<span class="number">3</span>, <span class="number">8</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">45</span>, <span class="number">34</span>, <span class="number">99</span>, <span class="number">17</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">43</span>, <span class="number">86</span>],</span><br><span class="line">       [<span class="number">11</span>, <span class="number">25</span>, <span class="number">84</span>, <span class="number">95</span>, <span class="number">97</span>, <span class="number">95</span>, <span class="number">69</span>, <span class="number">69</span>],</span><br><span class="line">       [ <span class="number">0</span>, <span class="number">89</span>, <span class="number">52</span>, <span class="number">29</span>, <span class="number">76</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">98</span>]])&gt;</span><br></pre></td></tr></table></figure>
<p>一般地，在使用 <code>tf.gather_nd</code> 采样多个样本时， 例如希望采样i号班级，j个学生，k门科目的成绩，则可以表达为[. . . , [i,j,k], . . . ]， 外层的括号长度为采样样本的个数，内层列表包含了每个采样点的索引坐标</p>
<h4 id="3tfboolean_mask"><a class="markdownIt-Anchor" href="#3tfboolean_mask"></a> 3.tf.boolean_mask</h4>
<p>除了可以通过给定索引号的方式采样，还可以通过给定掩码(Mask)的方式进行采样。通过<code>tf.boolean_mask(x, mask, axis)</code>可以在 axis 轴上根据mask 方案进行采样</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.uniform([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>],maxval=<span class="number">100</span>,dtype=tf.int32) <span class="comment"># 成绩册张量（4个班级 35个学生 8门科目）</span></span><br><span class="line"><span class="comment"># 根据掩码方式采样班级，给出掩码和维度索引</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.boolean_mask(x,mask=[<span class="literal">True</span>, <span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>],axis=<span class="number">0</span>)</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">288</span>, shape=(<span class="number">2</span>, <span class="number">35</span>, <span class="number">8</span>), dtype=int32, numpy=</span><br><span class="line">array([[[<span class="number">43</span>, <span class="number">10</span>, <span class="number">93</span>, <span class="number">85</span>, <span class="number">75</span>, <span class="number">87</span>, <span class="number">28</span>, <span class="number">19</span>],...</span><br></pre></td></tr></table></figure>
<ul>
<li>掩码的长度必须与对应维度的<strong>长度一致</strong></li>
<li>掩码可以是List嵌套，此时效果与<code>tf.gather_nd</code>类似</li>
</ul>
<p><code>tf.boolean_mask</code>既可以实现了<code>tf.gather</code>方式的一维掩码采样， 又可以实现<code>tf.gather_nd</code>方式的多维掩码采样</p>
<h4 id="4tfwhere"><a class="markdownIt-Anchor" href="#4tfwhere"></a> 4.tf.where</h4>
<p>通过<code>tf.where(cond, a, b)</code>操作可以根据 cond 条件的真假从参数A或B中读取数据， 条件判定规则如下：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>o</mi><mi>i</mi></msub><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>a</mi><mi>i</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>c</mi><mi>o</mi><mi>n</mi><msub><mi>d</mi><mi>i</mi></msub><mi mathvariant="normal">为</mi><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>b</mi><mi>i</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>c</mi><mi>o</mi><mi>n</mi><msub><mi>d</mi><mi>i</mi></msub><mi mathvariant="normal">为</mi><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">o_i=\left\{\begin{matrix}
a_i &amp; cond_i为True\\ 
b_i &amp; cond_i为False
\end{matrix}\right.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">{</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">为</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">u</span><span class="mord mathdefault">e</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">为</span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>。其中i为张量的元素索引， 返回的张量大小与A和B一致， 当对应位置的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mi>o</mi><mi>n</mi><msub><mi>d</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">cond_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>为 True， <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>o</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">o_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>从<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">a_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>中复制数据；当对应位置的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mi>o</mi><mi>n</mi><msub><mi>d</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">cond_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>为 False， <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>o</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">o_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>从<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">a_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>中复制数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.ones([<span class="number">3</span>,<span class="number">3</span>]) <span class="comment"># 构造 a 为全 1 矩阵</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.zeros([<span class="number">3</span>,<span class="number">3</span>]) <span class="comment"># 构造 b 为全 0 矩阵</span></span><br><span class="line"><span class="comment"># 构造采样条件</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cond = tf.constant([[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>],[<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">False</span>],[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.where(cond,a,b) <span class="comment"># 根据条件从 a,b 中采样</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">384</span>, shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<p>当参数a=b=None时，tf.where 会返回 cond 张量中所有 True 的元素的索引坐标</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>cond = tf.constant([[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>],[<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">False</span>],[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.where(cond) <span class="comment"># 获取 cond 中为 True 的元素索引</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">387</span>, shape=(<span class="number">4</span>, <span class="number">2</span>), dtype=int64, numpy=</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">1</span>]], dtype=int64)&gt;</span><br></pre></td></tr></table></figure>
<h5 id="例子"><a class="markdownIt-Anchor" href="#例子"></a> 例子</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">3</span>,<span class="number">3</span>]) <span class="comment"># 构造 a</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">403</span>, shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">-2.2946844</span> , <span class="number">0.6708417</span> , <span class="number">-0.5222212</span> ],</span><br><span class="line">       [<span class="number">-0.6919401</span> , <span class="number">-1.9418817</span> , <span class="number">0.3559235</span> ],</span><br><span class="line">       [<span class="number">-0.8005251</span> , <span class="number">1.0603906</span> , <span class="number">-0.68819374</span>]], dtype=float32)&gt;</span><br><span class="line"><span class="comment"># 通过比较运算，得到所有正数的掩码：</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask=x&gt;<span class="number">0</span> <span class="comment"># 比较操作，等同于 tf.math.greater()</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">405</span>, shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=bool, numpy=</span><br><span class="line">array([[<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">       [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>],</span><br><span class="line">       [<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>]])&gt;</span><br><span class="line"><span class="comment"># 通过 tf.where 提取此掩码处 True 元素的索引坐标：</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>indices=tf.where(mask) <span class="comment"># 提取所有大于 0 的元素索引</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">407</span>, shape=(<span class="number">3</span>, <span class="number">2</span>), dtype=int64, numpy=</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">1</span>]], dtype=int64)&gt;</span><br><span class="line"><span class="comment"># 拿到索引后，通过 tf.gather_nd 即可恢复出所有正数的元素：</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.gather_nd(x,indices) <span class="comment"># 提取正数的元素值</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">410</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">0.6708417</span>, <span class="number">0.3559235</span>, <span class="number">1.0603906</span>], dtype=float32)&gt;</span><br><span class="line"><span class="comment"># 实际上，也可以直接通过 tf.boolean_mask 获取所有正数的元素向量:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.boolean_mask(x,mask) <span class="comment"># 通过掩码提取正数的元素值</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">439</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">0.6708417</span>, <span class="number">0.3559235</span>, <span class="number">1.0603906</span>], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h4 id="5scatter_nd"><a class="markdownIt-Anchor" href="#5scatter_nd"></a> 5.scatter_nd</h4>
<p>通过<code>tf.scatter_nd(indices, updates, shape)</code>函数可以高效地刷新张量的部分数据，但是这个函数只能在<strong>全0</strong>的白板张量上面执行刷新操作，因此可能需要结合其它操作来实现现有张量的数据刷新功能</p>
<p>白板的形状通过 shape 参数表示，需要刷新的数据索引号通过 indices 表示，新数据为 updates。 根据 indices 给出的索引位置将 updates 中新的数据依次写入白板中，并返回更新后的结果张量</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200504220326506.png" alt="scatter_nd更新数据示意图"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造需要刷新数据的位置参数，即为 4、 3、 1 和 7 号位置</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>indices = tf.constant([[<span class="number">4</span>], [<span class="number">3</span>], [<span class="number">1</span>], [<span class="number">7</span>]])</span><br><span class="line"><span class="comment"># 构造需要写入的数据， 4 号位写入 4.4,3 号位写入 3.3，以此类推</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>updates = tf.constant([<span class="number">4.4</span>, <span class="number">3.3</span>, <span class="number">1.1</span>, <span class="number">7.7</span>])</span><br><span class="line"><span class="comment"># 在长度为 8 的全 0 向量上根据 indices 写入 updates 数据</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.scatter_nd(indices, updates, [<span class="number">8</span>])</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">467</span>, shape=(<span class="number">8</span>,), dtype=float32, numpy=array([<span class="number">0.</span> , <span class="number">1.1</span>, <span class="number">0.</span> , <span class="number">3.3</span>, <span class="number">4.4</span>, <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">7.7</span>], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<p>3维张量刷新例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造写入位置，即 2 个位置</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>indices = tf.constant([[<span class="number">1</span>],[<span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>updates = tf.constant([<span class="comment"># 构造写入数据，即 2 个矩阵</span></span><br><span class="line">[[<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">7</span>,<span class="number">7</span>,<span class="number">7</span>],[<span class="number">8</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">8</span>]],</span><br><span class="line">[[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>]]</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 在 shape 为[4,4,4]白板上根据 indices 写入 updates</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.scatter_nd(indices,updates,[<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">477</span>, shape=(<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>), dtype=int32, numpy=</span><br><span class="line">array([[[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line">       [[<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>], <span class="comment"># 写入的新数据 1</span></span><br><span class="line">        [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">7</span>],</span><br><span class="line">        [<span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>]],</span><br><span class="line">       [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line">       [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], <span class="comment"># 写入的新数据 2</span></span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]]])&gt;</span><br></pre></td></tr></table></figure>
<h4 id="6meshgrid"><a class="markdownIt-Anchor" href="#6meshgrid"></a> 6.meshgrid</h4>
<p>通过<code>tf.meshgrid</code>函数可以方便地生成二维网格的采样点坐标，方便可视化等应用场合</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.linspace(<span class="number">-8.</span>,<span class="number">8</span>,<span class="number">100</span>) <span class="comment"># 设置 x 轴的采样点</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = tf.linspace(<span class="number">-8.</span>,<span class="number">8</span>,<span class="number">100</span>) <span class="comment"># 设置 y 轴的采样点</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x,y = tf.meshgrid(x,y) <span class="comment"># 生成网格点，并内部拆分后返回</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.shape,y.shape <span class="comment"># 打印拆分后的所有点的 x,y 坐标张量 shape</span></span><br><span class="line">(TensorShape([<span class="number">100</span>, <span class="number">100</span>]), TensorShape([<span class="number">100</span>, <span class="number">100</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = tf.sqrt(x**<span class="number">2</span>+y**<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = tf.sin(z)/z <span class="comment"># sinc 函数实现</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 导入 3D 坐标轴支持</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fig = plt.figure()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ax = Axes3D(fig) <span class="comment"># 设置 3D 坐标轴</span></span><br><span class="line"><span class="comment"># 根据网格点绘制 sinc 函数 3D 曲面</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ax.contour3D(x.numpy(), y.numpy(), z.numpy(), <span class="number">50</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>plt.show()</span><br></pre></td></tr></table></figure>
<ul>
<li>通过在 x 轴上进行采样 100 个数据点， y 轴上采样 100 个数据点，然后利用tf.meshgrid(x, y)即可返回这 10000 个数据点的张量数据， 保存在 shape 为[100,100,2]的张量中。为了方便计算， tf.meshgrid 会返回在 axis=2 维度切割后的 2 个张量A和B，其中张量A包含了所有点的 x 坐标， B包含了所有点的 y 坐标， shape 都为[100,100]</li>
</ul>
<h3 id="7-经典数据集加载"><a class="markdownIt-Anchor" href="#7-经典数据集加载"></a> 7、经典数据集加载</h3>
<p><code>keras.datasets</code> 模块提供了常用经典数据集的自动下载、 管理、 加载与转换功能，并且提供了 <code>tf.data.Dataset</code> 数据集对象， 方便实现多线程(Multi-threading)、 预处理(Preprocessing)、 随机打散(Shuffle)和批训练(Training on Batch)等常用数据集的功能</p>
<p>常用的经典数据集：</p>
<ul>
<li>Boston Housing， 波士顿房价趋势数据集，用于回归模型训练与测试</li>
<li>CIFAR10/100， 真实图片数据集，用于图片分类任务</li>
<li>MNIST/Fashion_MNIST， 手写数字图片数据集，用于图片分类任务</li>
<li>IMDB， 情感分类任务数据集，用于文本分类任务</li>
</ul>
<hr>
<p>通过<code>datasets.xxx.load_data()</code>函数即可实现经典数据集的<strong>自动加载</strong>，其中 xxx 代表具体的数据集名称，如“CIFAR10”、“MNIST”。TensorFlow会默认将数据缓存在用户目录下的<code>.keras/datasets</code> 文件夹，用户无需关心数据集是如何保存的。如果当前数据集不在缓存中，则会自动从网络下载、 解压和加载数据集；如果已经在缓存中， 则自动完成加载</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets <span class="comment"># 导入经典数据集加载模块</span></span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>(x, y), (x_test, y_test) = datasets.mnist.load_data()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">&#x27;x:&#x27;</span>, x.shape, <span class="string">&#x27;y:&#x27;</span>, y.shape, <span class="string">&#x27;x test:&#x27;</span>, x_test.shape, <span class="string">&#x27;y test:&#x27;</span>, y_test)</span><br><span class="line"><span class="comment"># 返回数组的形状</span></span><br><span class="line">x: (<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>) y: (<span class="number">60000</span>,) x test: (<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>) y test: [<span class="number">7</span> <span class="number">2</span> <span class="number">1</span> ... <span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>通过 load_data()函数会返回相应格式的数据，对于图片数据集 MNIST、 CIFAR10 等，会返回 2 个 tuple，第一个 tuple 保存了用于训练的数据 x 和 y 训练集对象；第 2 个 tuple 则保存了用于测试的数据 x_test 和 y_test 测试集对象，所有的数据都用 Numpy 数组容器保存</li>
</ul>
<p>数据加载进入内存后，需要转换成 <code>Dataset</code> 对象。通过<code>Dataset.from_tensor_slices</code>可以将数据<strong>转换成Dataset对象</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>train_db = tf.data.Dataset.from_tensor_slices((x, y)) <span class="comment"># 以将训练部分的数据图片x和标签y转换成Dataset对象</span></span><br></pre></td></tr></table></figure>
<h4 id="1随机打散"><a class="markdownIt-Anchor" href="#1随机打散"></a> 1.随机打散</h4>
<p>通过<code>Dataset.shuffle(buffer_size)</code>工具可以设置 Dataset 对象随机打散数据之间的顺序，防止每次训练时数据按固定顺序产生，从而使得模型尝试“记忆”住标签信息</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_db为Dataset对象</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>train_db = train_db.shuffle(<span class="number">10000</span>) <span class="comment"># 随机打散样本，不会打乱样本与标签映射关系</span></span><br></pre></td></tr></table></figure>
<p>buffer_size 参数指定缓冲池的大小，一般设置为一个较大的常数即可。 调用 Dataset提供的这些工具函数会<strong>返回新的Dataset对象</strong>，可以通过<code>db = db.step1().step2().step3()</code>方式按序完成所有的数据处理步骤</p>
<h4 id="2批训练"><a class="markdownIt-Anchor" href="#2批训练"></a> 2.批训练</h4>
<p>为了利用显卡的并行计算能力，一般在网络的计算过程中会同时计算多个样本，把这种训练方式叫做批训练，其中一个批中样本的数量叫做<code>Batch Size</code></p>
<p>为了一次能够从Dataset 中产生 Batch Size 数量的样本，需要设置 Dataset 为批训练方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_db为Dataset对象</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>train_db = train_db.batch(<span class="number">128</span>) <span class="comment"># 设置批训练， batch size 为 128</span></span><br></pre></td></tr></table></figure>
<ul>
<li>一次并行计算 128 个样本的数据</li>
</ul>
<p>Batch Size 一般根据用户的 GPU 显存资源来设置，当显存不足时，可以适量减少 Batch Size 来减少显存使用量</p>
<h4 id="3预处理"><a class="markdownIt-Anchor" href="#3预处理"></a> 3.预处理</h4>
<p>Dataset 对象通过提供<code>map(func)</code>工具函数， 可以非常方便地调用用户自定义的预处理逻辑， 它实现在func函数里</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params">x, y</span>):</span> <span class="comment"># 自定义的预处理函数</span></span><br><span class="line">    <span class="comment"># 调用此函数时会自动传入 x,y 对象， shape 为[b, 28, 28], [b]</span></span><br><span class="line">    x = tf.cast(x, dtype=tf.float32) / <span class="number">255.</span><span class="comment"># 标准化到 0~1</span></span><br><span class="line">    x = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>*<span class="number">28</span>]) <span class="comment"># 打平</span></span><br><span class="line">    y = tf.cast(y, dtype=tf.int32) <span class="comment"># 转成整型张量</span></span><br><span class="line">    y = tf.one_hot(y, depth=<span class="number">10</span>) <span class="comment"># one-hot 编码</span></span><br><span class="line">    <span class="comment"># 返回的 x,y 将替换传入的 x,y 参数，从而实现数据的预处理功能</span></span><br><span class="line">    <span class="keyword">return</span> x,y</span><br><span class="line"><span class="comment"># 预处理函数实现在 preprocess 函数中，传入函数名即可</span></span><br><span class="line">train_db = train_db.map(preprocess)</span><br></pre></td></tr></table></figure>
<h4 id="4循环训练"><a class="markdownIt-Anchor" href="#4循环训练"></a> 4.循环训练</h4>
<p>对于 Dataset 对象， 在使用时可以通过</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step, (x,y) <span class="keyword">in</span> enumerate(train_db): <span class="comment"># 迭代数据集对象，带 step 参数</span></span><br></pre></td></tr></table></figure>
<p>或</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> train_db: <span class="comment"># 迭代数据集对象</span></span><br></pre></td></tr></table></figure>
<p>方式进行迭代，每次返回的 x 和 y 对象即为批量样本和标签，当对 train_db 的所有样本完成一次迭代后， for 循环终止退出</p>
<p>这样完成一个 Batch 的数据训练（执行一次循环体），叫做一个<strong>Step</strong></p>
<p>通过多个 step 来完成整个训练集的一次迭代（执行一次整个循环），叫做一个<strong>Epoch</strong></p>
<p>在实际训练时，通常需要对数据集迭代多个 Epoch 才能取得较好地训练效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">20</span>): <span class="comment"># 训练 Epoch 数</span></span><br><span class="line">    <span class="keyword">for</span> step, (x,y) <span class="keyword">in</span> enumerate(train_db): <span class="comment"># 迭代 Step 数</span></span><br><span class="line">        <span class="comment"># training...</span></span><br><span class="line"><span class="comment"># 可以通过repeat设置epoch的迭代次数（上面与下面等价）</span></span><br><span class="line">train_db = train_db.repeat(<span class="number">20</span>) <span class="comment"># 数据集迭代 20 遍才终止</span></span><br><span class="line"><span class="keyword">for</span> step, (x,y) <span class="keyword">in</span> enumerate(train_db): <span class="comment"># 迭代 Step 数</span></span><br><span class="line">    <span class="comment"># training...</span></span><br></pre></td></tr></table></figure>
<ul>
<li>通过repeat，使得数据集对象内部遍历多次才会退出</li>
</ul>
<h3 id="8-mnist实战"><a class="markdownIt-Anchor" href="#8-mnist实战"></a> 8、MNIST实战</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="神经网络"><a class="markdownIt-Anchor" href="#神经网络"></a> 神经网络</h2>
<p>神经网络属于机器学习的一个研究分支，它特指利用多个神经元去参数化映射函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>f</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">f_\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的模型</p>
<h3 id="1-感知机"><a class="markdownIt-Anchor" href="#1-感知机"></a> 1、感知机</h3>
<p>感知机是线性模型，并不能处理线性不可分问题。通过在线性模型后添加激活函数后得到<br>
活性值(Activation) <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span></span></span></span>：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>W</mi><mi>T</mi></msup><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a=\sigma (z)=\sigma (W^Tx+b)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span></span></span></span>，其中激活函数可以是阶跃函数(Step function)，也可以是符号函数(Sign function)</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505012316465.png" alt="感知机模型"></p>
<ul>
<li>接受长度为𝑛的一维向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mo stretchy="false">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">x=[x_1,x_2,...,x_n]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></li>
<li>每个输入节点通过权值为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>i</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">w_i,i\in [1,n]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mclose">]</span></span></span></span>的连接汇集为变量𝑧，即：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi><mo>=</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo>+</mo><msub><mi>w</mi><mi>n</mi></msub><msub><mi>x</mi><mi>n</mi></msub><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">z=w_1x_1+w_2x_2+...+w_nx_n+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>
<ul>
<li>𝑏称为感知机的偏置(Bias)</li>
<li>一维向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi><mo>=</mo><mo stretchy="false">[</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>w</mi><mi>n</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">W=[w_1,w_2,..w_n]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>称为感知机的权值(Weight)</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span>称为感知机的净活性值(Net Activation)</li>
</ul>
</li>
</ul>
<p>但是阶跃函数和符号函数在0处是不连续的， 其他位置导数为 0，无法利用梯度下降算法进行参数优化</p>
<p>以感知机为代表的线性模型不能解决异或(XOR)等线性不可分问题</p>
<h3 id="2-全连接层"><a class="markdownIt-Anchor" href="#2-全连接层"></a> 2、全连接层</h3>
<p>现代深度学习的核心结构在感知机的基础上，将不连续的阶跃激活函数换成了其它平滑连续可导的激活函数， 并通过堆叠多个网络层来增强网络的表达能力</p>
<p>如下所示的整个网络层可以通过矩阵关系式表达<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>=</mo><mi>X</mi><mi mathvariant="normal">@</mi><mi>W</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">O=X@W+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord">@</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span></p>
<ul>
<li>输入矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span>的 shape 定义为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[b,d_{in}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>为样本数量，此处只有1个样本参与前向运算，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{in}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>为输入节点数（即<strong>输入特征长度</strong>）</li>
<li>权值矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>的 shape 定义为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[d_{in},d_{out}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>为输出节点数（即<strong>输出特征长度</strong>）</li>
<li>偏置向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>的 shape 定义为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[d_{out}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></li>
<li>输出矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi></mrow><annotation encoding="application/x-tex">O</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span></span></span></span>包含了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>个样本的输出特征， shape 为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[b,d_{out}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></li>
</ul>
<p>由于每个输出节点与全部的输入节点相连接，这种网络层称为<strong>全连接层</strong>(Fully-connected Layer)，或者稠密连接层(Dense Layer)，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>矩阵叫做全连接层的权值矩阵，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>向量叫做全连接层的偏置向量</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505013252398.png" alt="全连接层"></p>
<h4 id="1张量方式实现"><a class="markdownIt-Anchor" href="#1张量方式实现"></a> 1.张量方式实现</h4>
<p>要实现全连接层，只需要定义好权值张量𝑾和偏置张量𝒃，并利用批量矩阵相乘函数<code>tf.matmul()</code>即可完成网络层的计算</p>
<p>例如， 创建输入𝑿矩阵为𝑏 = 2个样本，每个样本的输入特征长度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{in}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> = 784，输出节点数为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> = 256，故定义权值矩阵𝑾的 shape 为[784,256]，并采用正态分布初始化𝑾；偏置向量𝒃的 shape 定义为[256]，在计算完𝑿@𝑾后相加即可，最终全连接层的输出𝑶的 shape 为[2,256]，即 2 个样本的特征，每个特征长度为 256，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 W,b 张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">2</span>,<span class="number">784</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>w1 = tf.Variable(tf.random.truncated_normal([<span class="number">784</span>, <span class="number">256</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b1 = tf.Variable(tf.zeros([<span class="number">256</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>o1 = tf.matmul(x,w1) + b1 <span class="comment"># 线性变换</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>o1 = tf.nn.relu(o1) <span class="comment"># 激活函数</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">31</span>, shape=(<span class="number">2</span>, <span class="number">256</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">1.51279330e+00</span>, <span class="number">2.36286330e+00</span>, <span class="number">8.16453278e-01</span>,</span><br><span class="line">         <span class="number">1.80338228e+00</span>, <span class="number">4.58602428e+00</span>, <span class="number">2.54454136e+00</span>,...</span><br></pre></td></tr></table></figure>
<h4 id="2层方式实现"><a class="markdownIt-Anchor" href="#2层方式实现"></a> 2.层方式实现</h4>
<p>作为最常用的网络层之一，TensorFlow中有更高层、使用更方便的层实现方式： <code>layers.Dense(units, activation)</code>。</p>
<p>通过<code>layer.Dense</code>类， 只需要指定输出节点数 Units 和激活函数类型 activation 即可</p>
<p>注意：</p>
<ul>
<li>输入节点数会根据第一次运算时的输入 shape 确定，同时根据输入、输出节点数<br>
自动创建并初始化权值张量𝑾和偏置张量𝒃（因此在新建类 Dense 实例时，并不会立即创建权值张量𝑾和偏置张量𝒃， 而是需要调用 build 函数或者直接进行一次前向计算，才能完成网络参数的创建）</li>
<li>activation参数指定当前层的激活函数，可以为常见的激活函数或自定义激活函数，也可以指定为 None，即无激活函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers <span class="comment"># 导入层模块</span></span><br><span class="line"><span class="comment"># 创建全连接层，指定输出节点数和激活函数</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fc = layers.Dense(<span class="number">512</span>, activation=tf.nn.relu)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h1 = fc(x) <span class="comment"># 通过 fc 类实例完成一次全连接层的计算，返回输出张量</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">72</span>, shape=(<span class="number">4</span>, <span class="number">512</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">0.63339347</span>, <span class="number">0.21663809</span>, <span class="number">0.</span> , ..., <span class="number">1.7361937</span> , <span class="number">0.39962345</span>, <span class="number">2.4346168</span> ],...</span><br></pre></td></tr></table></figure>
<p>通过类内部的成员名 kernel 和 bias 来获取权值张量W和偏置张量b对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>fc.kernel <span class="comment"># 获取 Dense 类的权值矩阵</span></span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;dense_1/kernel:0&#x27;</span> shape=(<span class="number">784</span>, <span class="number">512</span>) dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">-0.04067389</span>, <span class="number">0.05240148</span>, <span class="number">0.03931375</span>, ..., <span class="number">-0.01595572</span>, <span class="number">-0.01075954</span>, <span class="number">-0.06222073</span>],</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fc.bias <span class="comment"># 获取 Dense 类的偏置向量</span></span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;dense_1/bias:0&#x27;</span> shape=(<span class="number">512</span>,) dtype=float32, numpy=</span><br><span class="line">array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,</span><br></pre></td></tr></table></figure>
<p>在优化参数时，需要获得网络的所有待优化的张量参数列表，可以通过类的<code>trainable_variables</code>来返回<strong>待优化参数列表</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>fc.trainable_variables <span class="comment"># 返回待优化参数列表</span></span><br><span class="line">[&lt;tf.Variable <span class="string">&#x27;dense_1/kernel:0&#x27;</span> shape=(<span class="number">784</span>, <span class="number">512</span>) dtype=float32,...,</span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;dense_1/bias:0&#x27;</span> shape=(<span class="number">512</span>,) dtype=float32, numpy=...]</span><br></pre></td></tr></table></figure>
<p>网络层除了保存了待优化张量列表 trainable_variables，还有部分层包含了不参与梯度优化的张量，如后续介绍的 Batch Normalization 层， 可以通过<code>non_trainable_variables</code>成员返回所有<strong>不需要优化参数列表</strong>。如果希望获得<strong>所有参数列表</strong>， 可以通过类的<code>variables</code>返回</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>fc.variables <span class="comment"># 返回所有参数列表</span></span><br><span class="line">[&lt;tf.Variable <span class="string">&#x27;dense_1/kernel:0&#x27;</span> shape=(<span class="number">784</span>, <span class="number">512</span>) dtype=float32,...,</span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;dense_1/bias:0&#x27;</span> shape=(<span class="number">512</span>,) dtype=float32, numpy=...]</span><br></pre></td></tr></table></figure>
<ul>
<li>对于全连接层，内部张量都参与梯度优化</li>
</ul>
<p>利用网络层类对象进行前向计算时，只需要调用类的<code>__call__</code>方法即可，即写成<code>fc(x)</code>方式便可（会自动调用类的<code>__call__</code>方法，在<code>__call__</code>方法中会自动调用call方法，由 TensorFlow 框架自动完成）</p>
<h3 id="3-神经网络"><a class="markdownIt-Anchor" href="#3-神经网络"></a> 3、神经网络</h3>
<p>通过层层堆叠全连接层，保证前一层的输出节点数与当前层的输入节点数匹配，即可堆叠出任意层数的网络。把这种由神经元相互连接而成的网络叫做神经网络</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505021027758.png" alt="4层神经网络结构"></p>
<p>通过堆叠 4 个全连接层，可以获得层数为 4 的神经网络，由于每层均为全连接层， 称为<strong>全连接网络</strong>。其中第 1~3 个全连接层在网络中间，称之为<strong>隐藏层</strong>1、 2、3，最后一个全连接层的输出作为网络的输出，称为<strong>输出层</strong></p>
<p>在设计全连接网络时，网络的结构配置等超参数可以按着经验法则自由设置，只需要<br>
遵循少量的约束即可。例如：</p>
<ul>
<li>隐藏层 1 的输入节点数需和数据的实际特征长度匹配</li>
<li>每层的输入层节点数与上一层输出节点数匹配</li>
<li>输出层的激活函数和节点数需要根据任务的具体设定进行设计。</li>
</ul>
<p>总的来说，神经网络模型的结构设计自由度较大，至于与哪一组超参数是最优的，这需要很多的领域经验知识和大量的实验尝试</p>
<h4 id="1张量方式实现-2"><a class="markdownIt-Anchor" href="#1张量方式实现-2"></a> 1.张量方式实现</h4>
<p>对于多层神经网络，以上图4层网络结构为例， 需要分别定义各层的权值矩阵𝑾和偏置向量𝒃，且每层的参数只能用于对应的层，不能混淆使用。在计算时，只需要按照网络层的顺序，将上一层的输出作为当前层的输入即可。最后一层是否需要添加激活函数通常视具体的任务而定</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 隐藏层 1 张量</span></span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">784</span>, <span class="number">256</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">b1 = tf.Variable(tf.zeros([<span class="number">256</span>]))</span><br><span class="line"><span class="comment"># 隐藏层 2 张量</span></span><br><span class="line">w2 = tf.Variable(tf.random.truncated_normal([<span class="number">256</span>, <span class="number">128</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">128</span>]))</span><br><span class="line"><span class="comment"># 隐藏层 3 张量</span></span><br><span class="line">w3 = tf.Variable(tf.random.truncated_normal([<span class="number">128</span>, <span class="number">64</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">b3 = tf.Variable(tf.zeros([<span class="number">64</span>]))</span><br><span class="line"><span class="comment"># 输出层张量</span></span><br><span class="line">w4 = tf.Variable(tf.random.truncated_normal([<span class="number">64</span>, <span class="number">10</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">b4 = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape: <span class="comment"># 梯度记录器</span></span><br><span class="line">    <span class="comment"># x: [b, 28*28]</span></span><br><span class="line">    <span class="comment"># 隐藏层 1 前向计算， [b, 28*28] =&gt; [b, 256]</span></span><br><span class="line">    h1 = x@w1 + tf.broadcast_to(b1, [x.shape[<span class="number">0</span>], <span class="number">256</span>])</span><br><span class="line">    h1 = tf.nn.relu(h1)</span><br><span class="line">    <span class="comment"># 隐藏层 2 前向计算， [b, 256] =&gt; [b, 128]</span></span><br><span class="line">    h2 = h1@w2 + b2</span><br><span class="line">    h2 = tf.nn.relu(h2)</span><br><span class="line">    <span class="comment"># 隐藏层 3 前向计算， [b, 128] =&gt; [b, 64]</span></span><br><span class="line">    h3 = h2@w3 + b3</span><br><span class="line">    h3 = tf.nn.relu(h3)</span><br><span class="line">    <span class="comment"># 输出层前向计算， [b, 64] =&gt; [b, 10]</span></span><br><span class="line">    h4 = h3@w4 + b4</span><br></pre></td></tr></table></figure>
<ul>
<li>在使用 TensorFlow 自动求导功能计算梯度时，需要将前向计算过程放置在tf.GradientTape()环境中，从而利用 GradientTape 对象的 gradient()方法自动求解参数的梯度，并利用 optimizers 对象更新参数</li>
</ul>
<h4 id="2层方式实现-2"><a class="markdownIt-Anchor" href="#2层方式实现-2"></a> 2.层方式实现</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入常用网络层 layers</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,Sequential</span><br><span class="line">fc1 = layers.Dense(<span class="number">256</span>, activation=tf.nn.relu) <span class="comment"># 隐藏层 1</span></span><br><span class="line">fc2 = layers.Dense(<span class="number">128</span>, activation=tf.nn.relu) <span class="comment"># 隐藏层 2</span></span><br><span class="line">fc3 = layers.Dense(<span class="number">64</span>, activation=tf.nn.relu) <span class="comment"># 隐藏层 3</span></span><br><span class="line">fc4 = layers.Dense(<span class="number">10</span>, activation=<span class="literal">None</span>) <span class="comment"># 输出层</span></span><br><span class="line"><span class="comment"># 前向计算</span></span><br><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">h1 = fc1(x) <span class="comment"># 通过隐藏层 1 得到输出</span></span><br><span class="line">h2 = fc2(h1) <span class="comment"># 通过隐藏层 2 得到输出</span></span><br><span class="line">h3 = fc3(h2) <span class="comment"># 通过隐藏层 3 得到输出</span></span><br><span class="line">h4 = fc4(h3) <span class="comment"># 通过输出层得到网络输出</span></span><br></pre></td></tr></table></figure>
<p>对于这种数据<strong>依次向前传播</strong>的网络， 也可以通过<code>Sequential</code>容器封装成一个网络大类对象，调用大类的前向计算函数一次即可完成所有层的前向计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 Sequential 容器</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,Sequential</span><br><span class="line"><span class="comment"># 通过 Sequential 容器封装为一个网络类</span></span><br><span class="line">model = Sequential([</span><br><span class="line">    layers.Dense(<span class="number">256</span>, activation=tf.nn.relu) , <span class="comment"># 创建隐藏层 1</span></span><br><span class="line">    layers.Dense(<span class="number">128</span>, activation=tf.nn.relu) , <span class="comment"># 创建隐藏层 2</span></span><br><span class="line">    layers.Dense(<span class="number">64</span>, activation=tf.nn.relu) , <span class="comment"># 创建隐藏层 3</span></span><br><span class="line">    layers.Dense(<span class="number">10</span>, activation=<span class="literal">None</span>) , <span class="comment"># 创建输出层</span></span><br><span class="line">])</span><br><span class="line"><span class="comment"># 前向计算得到输出</span></span><br><span class="line">out = model(x)</span><br></pre></td></tr></table></figure>
<h4 id="3优化目标"><a class="markdownIt-Anchor" href="#3优化目标"></a> 3.优化目标</h4>
<p>我们把神经网络从输入到输出的计算过程叫做<strong>前向传播</strong>(Forward Propagation)或<strong>前向计</strong><br>
<strong>算</strong>。神经网络的前向传播过程，也是数据张量(Tensor)从第一层流动(Flow)至输出层的过程，即从输入数据开始，途径每个隐藏层，直至得到输出并计算误差，这也是 TensorFlow<br>
框架名字由来</p>
<p>前向传播的最后一步就是完成误差的计算<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="script">L</mi><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msub><mi>𝑓</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">ℒ=g(𝑓_\theta(x),y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7em;vertical-align:0em;"></span><span class="mord"><span class="mord mathscr" style="margin-right:0.19189em;">L</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></p>
<ul>
<li>其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑓</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">𝑓_\theta(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span>代表了利用𝜃参数化的神经网络模型</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span>称之为误差函数，用来描述当前网络的预测值<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>𝑓</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">𝑓_\theta(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>与真实标签𝒚之间的差距度量， 比如常用的均方差误差函数</li>
<li>ℒ称为网络的误差(Error，或损失 Loss)，一般为标量</li>
</ul>
<p>我们希望通过在训练集<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi mathvariant="double-struck">D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbb{D}^{train}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">D</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span></span></span></span></span>上面学习到一组参数𝜃使得训练的误差ℒ最小：</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mo>∗</mo></msup><mo>=</mo><munder><munder><mrow><mi>a</mi><mi>r</mi><mi>g</mi><mtext> </mtext><mi>m</mi><mi>i</mi><mi>n</mi></mrow><mo stretchy="true">⏟</mo></munder><mi>θ</mi></munder><mi>g</mi><mo stretchy="false">(</mo><msub><mi>𝑓</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>x</mi><mo>∈</mo><msup><mi mathvariant="double-struck">D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\theta^*=\underbrace{arg\ min}_{\theta}g(𝑓_\theta(x),y),x\in \mathbb{D}^{train}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.278548em;vertical-align:-1.528548em;"></span><span class="mord munder"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6595199999999999em;"><span style="top:-1.471452em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord munder"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.65952em;"><span class="svg-align" style="top:-2.15756em;"><span class="pstrut" style="height:3em;"></span><span class="stretchy" style="height:0.548em;min-width:1.6em;"><span class="brace-left" style="height:0.548em;"><svg width="400em" height="0.548em" viewbox="0 0 400000 548" preserveaspectratio="xMinYMin slice"><path d="M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13
 35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688
 0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7
-331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"/></svg></span><span class="brace-center" style="height:0.548em;"><svg width="400em" height="0.548em" viewbox="0 0 400000 548" preserveaspectratio="xMidYMin slice"><path d="M199572 214
c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14
 53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3
 11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0
-5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"/></svg></span><span class="brace-right" style="height:0.548em;"><svg width="400em" height="0.548em" viewbox="0 0 400000 548" preserveaspectratio="xMaxYMin slice"><path d="M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3
 28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237
-174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"/></svg></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mspace"> </span><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8424400000000001em;"><span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.528548em;"><span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">D</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>上述的最小化优化问题一般采用误差反向传播(Backward Propagation，简称 BP)算法来求解网络参数𝜃的梯度信息，并利用梯度下降(Gradient Descent，简称 GD)算法迭代更新参数：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mo mathvariant="normal">′</mo></msup><mo>=</mo><mi>θ</mi><mo>−</mo><mi>η</mi><mo>⋅</mo><msub><mo>▽</mo><mi>θ</mi></msub><mi mathvariant="script">L</mi></mrow><annotation encoding="application/x-tex">\theta&#x27;=\theta-\eta\cdot\bigtriangledown_\thetaℒ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.751892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.63889em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">η</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8944399999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mbin">▽</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathscr" style="margin-right:0.19189em;">L</span></span></span></span></span>，其中𝜂为学习率</p>
<p>网络的参数量是衡量网络规模的重要指标。计算全连接层的参数量方法：</p>
<p>考虑权值矩阵𝑾，偏置向量𝒃，输入特征长度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{in}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>， 输出特征长度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的网络层， 𝑾的参数量为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>⋅</mo><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{in}\cdot d_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，再加上偏置𝒃的参数， 总参数量为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>⋅</mo><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>+</mo><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{in}\cdot d_{out}+d_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。对于多层的全连接神经网络，总参数量的计算应累加所有全连接层的总参数量</p>
<h3 id="4-激活函数"><a class="markdownIt-Anchor" href="#4-激活函数"></a> 4、激活函数</h3>
<p>激活函数都是平滑可导的，适合于梯度下降算法</p>
<h4 id="1sigmoid"><a class="markdownIt-Anchor" href="#1sigmoid"></a> 1.Sigmoid</h4>
<p>Sigmoid 函数也叫 Logistic 函数，定义为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>≐</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">Sigmoid(x)\doteq\frac{1}{1+e^{-x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">m</span><span class="mord mathdefault">o</span><span class="mord mathdefault">i</span><span class="mord mathdefault">d</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≐</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2484389999999999em;vertical-align:-0.403331em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7026642857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.403331em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<p>它的一个优良特性就是能够把𝑥∈𝑅的输入“压缩” 到𝑥∈(0,1)区间，这个区间的数值在机器学习常用来表示以下意义：</p>
<ul>
<li>概率分布 (0,1)区间的输出和概率的分布范围[0,1]契合，可以通过 Sigmoid 函数将输出转译为<strong>概率输出</strong></li>
<li>信号强度：一般可以将 0~1 理解为某种信号的强度（如像素的颜色强度， 1 代表当前通道颜色最强， 0 代表当前通道无颜色；抑或代表门控值(Gate)的强度， 1 代表当前门控全部开放， 0 代表门控关闭）</li>
</ul>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505030303847.png" alt="Sigmoid函数曲线"></p>
<p>可以通过<code>tf.nn.sigmoid</code>实现 Sigmoid 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.linspace(<span class="number">-6.</span>,<span class="number">6.</span>,<span class="number">10</span>) <span class="comment"># 构造-6~6 的输入向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.nn.sigmoid(x) <span class="comment"># 通过 Sigmoid 函数</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">7</span>, shape=(<span class="number">10</span>,), dtype=float32, numpy=</span><br><span class="line">array([<span class="number">0.00247264</span>, <span class="number">0.00931597</span>, <span class="number">0.03444517</span>, <span class="number">0.11920291</span>, <span class="number">0.33924365</span>, <span class="number">0.6607564</span> , <span class="number">0.8807971</span> , <span class="number">0.96555483</span>, <span class="number">0.99068403</span>, <span class="number">0.9975274</span> ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h4 id="2relu"><a class="markdownIt-Anchor" href="#2relu"></a> 2.ReLU</h4>
<p>在 ReLU(REctified Linear Unit， 修正线性单元)激活函数提出之前， Sigmoid 函数通常<br>
是神经网络的激活函数首选。</p>
<p>Sigmoid 函数在输入值较大或较小时容易出现梯度值接近于 0 的现象，称为<strong>梯度弥散</strong>现象。出现梯度弥散现象时， 网络参数长时间得不到更新，导致训练不收敛或停滞不动的现象发生， 较深层次的网络模型中更容易出现梯度弥散现象</p>
<p>ReLU定义为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>≐</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">ReLU(x)\doteq max(0,x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault">e</span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≐</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>，函数曲线如下：</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505030816893.png" alt="ReLU函数曲线"></p>
<p>可以通过<code>tf.nn.relu</code>实现 ReLU 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.linspace(<span class="number">-6.</span>,<span class="number">6.</span>,<span class="number">10</span>) <span class="comment"># 构造-6~6 的输入向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.nn.relu(x) <span class="comment"># 通过 ReLU 激活函数</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">11</span>, shape=(<span class="number">10</span>,), dtype=float32, numpy=</span><br><span class="line">array([<span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.666667</span>, <span class="number">2.</span> , <span class="number">3.333334</span>, <span class="number">4.666667</span>, <span class="number">6.</span> ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<p>除了可以使用函数式接口 tf.nn.relu 实现 ReLU 函数外，还可以像 Dense 层一样将ReLU 函数作为一个网络层添加到网络中，对应的类为 <code>layers.ReLU()</code>类。一般来说，激活<br>
函数类并不是主要的网络运算层，不计入网络的层数</p>
<p>ReLU 函数有着优良的梯度特性，在大量的深度学习应用中被验证非常有效，是应用最广泛的激活函数之一</p>
<h4 id="3leakyrelu"><a class="markdownIt-Anchor" href="#3leakyrelu"></a> 3.LeakyReLU</h4>
<p>ReLU 函数在𝑥 &lt; 0时导数值恒为 0，也可能会造成梯度弥散现象，为了克服这个问<br>
题， LeakyReLU 函数被提出：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mi>e</mi><mi>a</mi><mi>k</mi><mi>y</mi><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo>≐</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>x</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo>⩾</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>p</mi><mi>x</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo>&lt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">LeakyReLU\doteq \left\{\begin{matrix}
x &amp; x\geqslant 0\\ 
px &amp; x&lt;0
\end{matrix}\right.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">L</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault">e</span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≐</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">{</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="mord mathdefault">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel amsrm">⩾</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<p>其中𝑝为用户自行设置的某较小数值的超参数，如 0.02 等。当𝑝 = 0时， LeayReLU 函数退化为 ReLU 函数；当𝑝 ≠ 0时， 𝑥 &lt; 0处能够获得较小的导数值𝑝，从而避免出现梯度弥散现象</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505031651370.png" alt="LeakyReLU函数曲线"></p>
<p>可以通过<code>tf.nn.leaky_relu</code>实现 LeakyReLU 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.linspace(<span class="number">-6.</span>,<span class="number">6.</span>,<span class="number">10</span>) <span class="comment"># 构造-6~6 的输入向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.nn.leaky_relu(x, alpha=<span class="number">0.1</span>) <span class="comment"># 通过 LeakyReLU 激活函数</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">13</span>, shape=(<span class="number">10</span>,), dtype=float32, numpy=</span><br><span class="line">array([<span class="number">-0.6</span> , <span class="number">-0.46666667</span>, <span class="number">-0.33333334</span>, <span class="number">-0.2</span> , <span class="number">-0.06666666</span>, <span class="number">0.666667</span> , <span class="number">2.</span> , <span class="number">3.333334</span> , <span class="number">4.666667</span> , <span class="number">6.</span> ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>alpha 参数代表𝑝</li>
</ul>
<p>tf.nn.leaky_relu 对应的类为 <code>layers.LeakyReLU</code>，可以通过LeakyReLU(alpha)创建LeakyReLU 网络层，并设置𝑝参数，像 Dense 层一样将 LeakyReLU层放置在网络的合适位置</p>
<h4 id="4tanh"><a class="markdownIt-Anchor" href="#4tanh"></a> 4.Tanh</h4>
<p>Tanh 函数能够将𝑥 ∈ 𝑅的输入“压缩” 到(-1,1)区间，定义为：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>≐</mo><mfrac><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac><mo>=</mo><mn>2</mn><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo stretchy="false">(</mo><mn>2</mn><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">tanh(x)\doteq\frac{e^x-e^{-x}}{e^x+e^{-x}}=2sigmoid(2x)-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≐</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.3906960000000002em;vertical-align:-0.403331em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.987365em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.5935428571428571em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7026642857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7385428571428572em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8476642857142858em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.403331em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">2</span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">m</span><span class="mord mathdefault">o</span><span class="mord mathdefault">i</span><span class="mord mathdefault">d</span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>（tanh 激活函数可通过 Sigmoid 函数缩放平移后实现）</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505032201454.png" alt="tanh函数曲线"></p>
<p>可以通过 tf.nn.tanh 实现 tanh 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.linspace(<span class="number">-6.</span>,<span class="number">6.</span>,<span class="number">10</span>) <span class="comment"># 构造-6~6 的输入向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.nn.tanh(x) <span class="comment"># 通过 tanh 激活函数</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">15</span>, shape=(<span class="number">10</span>,), dtype=float32, numpy=</span><br><span class="line">array([<span class="number">-0.9999877</span> , <span class="number">-0.99982315</span>, <span class="number">-0.997458</span> , <span class="number">-0.9640276</span> , <span class="number">-0.58278286</span>, <span class="number">0.5827831</span> , <span class="number">0.9640276</span> , <span class="number">0.997458</span> , <span class="number">0.99982315</span>, <span class="number">0.9999877</span> ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h3 id="5-输出层设计"><a class="markdownIt-Anchor" href="#5-输出层设计"></a> 5、输出层设计</h3>
<p>常见的几种输出类型包括：</p>
<ul>
<li>𝑜𝑖 ∈ <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>𝑅</mi><mi>𝑑</mi></msup></mrow><annotation encoding="application/x-tex">𝑅^𝑑</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span> 输出属于整个实数空间，或者某段普通的实数空间，比如函数值趋势的预<br>
测，年龄的预测问题等</li>
<li>𝑜𝑖 ∈ [0,1] 输出值特别地落在[0, 1]的区间， 如图片生成，图片像素值一般用[0, 1]区间<br>
的值表示；或者二分类问题的概率，如硬币正反面的概率预测问题</li>
<li>𝑜𝑖 ∈ [0,  1],   𝑖 𝑜𝑖 = 1 输出值落在[0, 1]的区间， 并且所有输出值之和为 1， 常见的如<br>
多分类问题，如 MNIST 手写数字图片识别，图片属于 10 个类别的概率之和应为 1</li>
<li>𝑜𝑖 ∈ [-1,  1] 输出值在[-1, 1]之间</li>
</ul>
<h4 id="1普通实数空间"><a class="markdownIt-Anchor" href="#1普通实数空间"></a> 1.普通实数空间</h4>
<p>该类问题较普遍，输出层可以不加激活函数。</p>
<p>误差的计算直接基于最后一层的输出𝒐和真实值𝒚进行计算， 如采用均方差误差函数度量输出值𝒐与真实值𝒚之间的距离：ℒ = 𝑔(𝒐, 𝒚)，其中𝑔代表了某个具体的误差计算函数，例如 MSE 等</p>
<h4 id="201区间"><a class="markdownIt-Anchor" href="#201区间"></a> 2.[0,1]区间</h4>
<p>为了让像素的值范围映射到[0,1]的有效实数空间，需要在输出层后添加某个合适的激活函数𝜎，其中 Sigmoid 函数刚好具有此功能</p>
<p>对于二分类问题，如硬币的正反面的预测， 输出层可以<strong>只设置一个节点</strong>，表示某个事件 A 发生的概率𝑃(A|𝒙)， 𝒙为网络输入。假设网络的输出标量𝑜表示正面事件出现的概率，则反面事件出现的概率即为1 - 𝑜，网络结构如下所示</p>
<ul>
<li>𝑃(正面|𝒙) = 𝑜</li>
<li>𝑃(反面|𝒙) = 1 - 𝑜</li>
</ul>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200505180332392.png" alt="单输出节点的二分类网络"></p>
<ul>
<li>输出层的净活性值𝑧后添加 Sigmoid 函数即可将输出转译为概率值</li>
</ul>
<h4 id="301区间和为1"><a class="markdownIt-Anchor" href="#301区间和为1"></a> 3.[0,1]区间，和为1</h4>
<p>输出值𝑜𝑖 ∈ [0,1]， 且所有输出值之和为 1，这种设定以多分类问题最为常见</p>
<p>可以通过在输出层添加 Softmax 函数实现：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>Z</mi><mi>i</mi></msub></msup><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></msubsup><msup><mi>e</mi><msub><mi>Z</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">Softmax(z_i)=\frac{e^{Z_i}}{\sum^{d_{out}}_{j=1}e^{Z_j}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.8939049999999997em;vertical-align:-0.85654em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0373649999999999em;"><span style="top:-2.4662800000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0338857142857143em;"><span style="top:-2.177714285714286em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.0378571428571433em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3447999999999998em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.61508em;"></span><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.27027999999999996em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.46117142857142857em;"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.19516666666666668em;"></span><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9595285714285715em;"><span style="top:-2.9714357142857146em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:-0.07153em;margin-right:0.1em;"><span class="pstrut" style="height:2.65952em;"></span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5091600000000001em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9190928571428572em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:-0.07153em;margin-right:0.1em;"><span class="pstrut" style="height:2.65952em;"></span><span class="mord mathdefault mtight">i</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.31472em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.85654em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<ul>
<li>Softmax 函数不仅可以将输出值映射到[0,1]区间，还满足所有的输出值之和为 1 的特性</li>
<li>通过 Softmax函数可以将输出层的输出转译为类别概率，在分类问题中使用的非常频繁</li>
</ul>
<p>可以通过 tf.nn.softmax 实现 Softmax 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = tf.constant([<span class="number">2.</span>,<span class="number">1.</span>,<span class="number">0.1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.nn.softmax(z) <span class="comment"># 通过 Softmax 函数</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">19</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">0.6590012</span>, <span class="number">0.242433</span> , <span class="number">0.0985659</span>], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<p>与 Dense 层类似， Softmax 函数也可以作为网络层类使用， 通过类<code>layers.Softmax(axis=-1)</code>可以方便添加 Softmax 层，其中 axis 参数指定需要进行计算的维度</p>
<p>在 Softmax 函数的数值计算过程中，容易因输入值偏大发生<strong>数值溢出</strong>现象；在计算交叉熵时，也会出现数值溢出的问题。为了数值计算的稳定性， TensorFlow 中提供了一个统一的接口，将 Softmax 与交叉熵损失函数同时实现，同时也处理了数值不稳定的异常，一般推荐使用这些接口函数，避免分开使用 Softmax 函数与交叉熵损失函数。函数式接口为<code>tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False)</code></p>
<ul>
<li>y_true：One-hot 编码后的真实标签</li>
<li>y_pred：网络的预测值
<ul>
<li>当 from_logits 设置为 True 时，y_pred 表示须为未经过 Softmax 函数的变量 z</li>
<li>当 from_logits 设置为 False 时， y_pred 表示为经过 Softmax 函数的输出</li>
</ul>
</li>
<li>为了数值计算稳定性，一般设置 from_logits 为 True（此时<code>tf.keras.losses.categorical_crossentropy</code>将在内部进行 Softmax 函数计算，所以不需要在模型中显式调用 Softmax 函数）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = tf.random.normal([<span class="number">2</span>,<span class="number">10</span>]) <span class="comment"># 构造输出层的输出</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_onehot = tf.constant([<span class="number">1</span>,<span class="number">3</span>]) <span class="comment"># 构造真实值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_onehot = tf.one_hot(y_onehot, depth=<span class="number">10</span>) <span class="comment"># one-hot 编码</span></span><br><span class="line"><span class="comment"># 输出层未使用 Softmax 函数，故 from_logits 设置为 True</span></span><br><span class="line"><span class="comment"># 这样 categorical_crossentropy 函数在计算损失函数前，会先内部调用 Softmax 函数</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = keras.losses.categorical_crossentropy(y_onehot,z,from_logits=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = tf.reduce_mean(loss) <span class="comment"># 计算平均交叉熵损失</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">210</span>, shape=(), dtype=float32, numpy= <span class="number">2.4201946</span>&gt;</span><br></pre></td></tr></table></figure>
<p>除了函数式接口， 也可以利用<code>losses.CategoricalCrossentropy(from_logits)</code>类方式同时实现 Softmax 与交叉熵损失函数的计算， from_logits 参数的设置方式相同</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 Softmax 与交叉熵计算类，输出层的输出 z 未使用 softmax</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>criteon = keras.losses.CategoricalCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = criteon(y_onehot,z) <span class="comment"># 计算损失</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">258</span>, shape=(), dtype=float32, numpy= <span class="number">2.4201946</span>&gt;</span><br></pre></td></tr></table></figure>
<h4 id="4-11区间"><a class="markdownIt-Anchor" href="#4-11区间"></a> 4.[-1,1]区间</h4>
<p>使用 tanh 激活函数即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.linspace(<span class="number">-6.</span>,<span class="number">6.</span>,<span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.tanh(x) <span class="comment"># tanh 激活函数</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">264</span>, shape=(<span class="number">10</span>,), dtype=float32, numpy=</span><br><span class="line">array([<span class="number">-0.9999877</span> , <span class="number">-0.99982315</span>, <span class="number">-0.997458</span> , <span class="number">-0.9640276</span> , <span class="number">-0.58278286</span>, <span class="number">0.5827831</span> , <span class="number">0.9640276</span> , <span class="number">0.997458</span> , <span class="number">0.99982315</span>, <span class="number">0.9999877</span> ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h3 id="6-误差计算"><a class="markdownIt-Anchor" href="#6-误差计算"></a> 6、误差计算</h3>
<p>常见的误差函数有均方差、 交叉熵、 KL 散度、 Hinge Loss 函数等，其中均方差函数和交叉熵函数在深度学习中比较常见，均方差函数主要用于回归问题，交叉熵函数主要用于分类问题</p>
<h4 id="1均方差误差函数"><a class="markdownIt-Anchor" href="#1均方差误差函数"></a> 1.均方差误差函数</h4>
<p>均方差(Mean Squared Error，简称 MSE)误差函数：把输出向量和真实向量映射到笛卡尔坐标系的两个点上，通过计算这两个点之间的欧式距离(准确地说是欧式距离的平方)来衡量两个向量之间的差距：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mi>S</mi><mi>E</mi><mo stretchy="false">(</mo><mi>y</mi><mo separator="true">,</mo><mi>o</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></msubsup><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>o</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">MSE(y,o)=\frac{1}{d_{out}}\sum^{d_{out}}_{i=1}(y_i-o_i)^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">o</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.434108em;vertical-align:-0.44509999999999994em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.989008em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></p>
<ul>
<li>MSE 误差函数的值总是大于等于 0</li>
<li>当 MSE 函数达到最小值 0 时， 输出等于真实标签，此时神经网络的参数达到最优状态</li>
</ul>
<p>可以通过函数方式或层方式实现 MSE 误差计算。通过函数式调用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>o = tf.random.normal([<span class="number">2</span>,<span class="number">10</span>]) <span class="comment"># 构造网络输出</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_onehot = tf.constant([<span class="number">1</span>,<span class="number">3</span>]) <span class="comment"># 构造真实值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_onehot = tf.one_hot(y_onehot, depth=<span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = keras.losses.MSE(y_onehot, o) <span class="comment"># 计算均方差</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">27</span>, shape=(<span class="number">2</span>,), dtype=float32, numpy=array([<span class="number">0.779179</span> ,</span><br><span class="line"><span class="number">1.6585705</span>], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>MSE 函数返回的是每个样本的均方差</p>
</li>
<li>
<p>可以在样本维度上再次平均来获得平均样本的均方差</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = tf.reduce_mean(loss) <span class="comment"># 计算 batch 均方差</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">30</span>, shape=(), dtype=float32, numpy=<span class="number">1.2188747</span>&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>通过层方式实现，对应的类为<code>keras.losses.MeanSquaredError()</code>，和其他层的类一样，调用<code>__call__</code>函数即可完成前向计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 MSE 类</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>criteon = keras.losses.MeanSquaredError()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = criteon(y_onehot,o) <span class="comment"># 计算 batch 均方差</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">54</span>, shape=(), dtype=float32, numpy=<span class="number">1.2188747</span>&gt;</span><br></pre></td></tr></table></figure>
<h4 id="2交叉熵误差函数"><a class="markdownIt-Anchor" href="#2交叉熵误差函数"></a> 2.交叉熵误差函数</h4>
<p>熵，在信息论中，用来衡量信息的不确定度。 熵在信息学科中也叫信息熵，或者香农熵。熵越大，代表不确定性越大，信息量也就越大。 某个分布𝑃(𝑖)的熵定义为：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><msub><mo>∑</mo><mi>i</mi></msub><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mi>l</mi><mi>o</mi><msub><mi>g</mi><mn>2</mn></msub><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(P)=-\sum_iP(i)log_2P(i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0497100000000001em;vertical-align:-0.29971000000000003em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mclose">)</span></span></span></span></p>
<ul>
<li>对于确定的分布，熵取得最小值0，不确定性为0</li>
<li>由于𝑃(𝑖) ∈ [0,1], <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mn>2</mn></msub><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">log_2P(i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mclose">)</span></span></span></span> ≤ 0，因此熵𝐻(𝑃)总是大于等于 0</li>
<li>在 TensorFlow 中，可以用<code>tf.math.log</code>来计算熵</li>
</ul>
<p>交叉熵(Cross Entropy)的定义：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>+</mo><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(p||q)=H(p)+D_{KL}(p||q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span></span></span></span></p>
<ul>
<li>其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">D_{KL}(p||q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span></span></span></span>为𝑝与𝑞的 KL 散度(Kullback-Leibler Divergence)：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo>∑</mo><mi>i</mi></msub><mi>p</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><mrow><mi>q</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">D_{KL}(p||q)=\sum_ip(i)log(\frac{p(i)}{q(i)})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span>
<ul>
<li>KL 散度是用于衡量 2 个分布之间距离的指标：𝑝 = 𝑞时，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">D_{KL}(p||q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span></span></span></span>取得最小值 0， 𝑝与𝑞之间的差距越大，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">D_{KL}(p||q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span></span></span></span>也越大</li>
<li>交叉熵和 KL 散度都不是对称的：
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo><mi mathvariant="normal">≠</mi><mi>H</mi><mo stretchy="false">(</mo><mi>q</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(p||q)\neq H(q||p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="rlap"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="inner"><span class="mrel"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault">p</span><span class="mclose">)</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo><mi mathvariant="normal">≠</mi><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>q</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">D_{KL}(p||q)\neq D_{KL}(q||p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="rlap"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="inner"><span class="mrel"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault">p</span><span class="mclose">)</span></span></span></span></li>
</ul>
</li>
</ul>
</li>
<li>当分类问题中 y 的编码分布𝑝采用 One-hot 编码𝒚时： 𝐻(𝑝) = 0</li>
</ul>
<p>分类问题中交叉熵的计算表达式：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><msub><mi>o</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">H(p||q)=D_{KL}(p||q)=-logo_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord">−</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<ul>
<li>𝑖为 One-hot 编码中为 1 的索引号，也是当前输入的真实类别</li>
<li>ℒ只与真实类别𝑖上的概率𝑜𝑖有关， 对应概率𝑜𝑖越大， 𝐻(𝑝||𝑞)越小</li>
<li>当对应类别上的概率为 1 时， 交叉熵𝐻(𝑝||𝑞)取得最小值 0，此时网络输出𝒐与真实标签𝒚完全一致，神经网络取得最优状态</li>
<li>最小化交叉熵损失函数的过程也是最大化正确类别的预测概率的过程</li>
</ul>
<h3 id="7-神经网络类型"><a class="markdownIt-Anchor" href="#7-神经网络类型"></a> 7、神经网络类型</h3>
<p>全连接层是神经网络最基本的网络类型，优点是全连接层前向计算流程相对简单，梯度求导也较简单，缺点是在处理较大特征长度的数据时， 全连接层的参数量往往较大</p>
<h4 id="1卷积神经网络"><a class="markdownIt-Anchor" href="#1卷积神经网络"></a> 1.卷积神经网络</h4>
<p>全连接层在处理高维度的图片、 视频数据时往往出现网络参数量巨大，训练非常困难。通过利用局部相关性和权值共享的思想，Yann Lecun在1986年提出了卷积神经网络(Convolutional Neural Network， 简称 CNN)</p>
<p>其中比较流行的模型：</p>
<ul>
<li>用于图片分类的 AlexNet、 VGG、 GoogLeNet、 ResNet、 DenseNet 等</li>
<li>用于目标识别的 RCNN、 Fast RCNN、 Faster RCNN、 Mask RCNN、 YOLO、 SSD 等</li>
</ul>
<h4 id="2循环神经网络"><a class="markdownIt-Anchor" href="#2循环神经网络"></a> 2.循环神经网络</h4>
<p>除了具有空间结构的图片、 视频等数据外，序列信号也是非常常见的一种数据类型，其中一个最具代表性的序列信号就是文本数据。卷积神经网络由于缺乏 Memory 机制和处理不定长序列信号的能力，并不擅长序列信号的任务。循环神经网络(Recurrent Neural Network，被证明非常擅长处理序列信号</p>
<p>1997年提出的LSTM网络，作为 RNN 的变种，较好地克服了 RNN 缺乏长期记忆、 不擅长处理长序列的问题，在自然语言处理中得到了广泛的应用。基于LSTM 模型， Google 提出了用于机器翻译的 Seq2Seq 模型，并成功商用于谷歌神经机器翻译系统(GNMT)</p>
<p>其他的 RNN 变种还有 GRU、 双向 RNN 等</p>
<h4 id="3注意力机制网络"><a class="markdownIt-Anchor" href="#3注意力机制网络"></a> 3.注意力（机制）网络</h4>
<p>RNN 并不是自然语言处理的最终解决方案，近年来随着注意力机制(Attention Mechanism)的提出，克服了 RNN 训练不稳定、 难以并行化等缺陷，在自然语言处理和图片生成等领域中逐渐崭露头角</p>
<p>2017 年， Google 提出了第一个利用纯注意力机制实现的网络模型Transformer，随后基于 Transformer 模型相继提出了一系列的用于机器翻译的注意力网络模型，如 GPT、 BERT、 GPT-2 等</p>
<p>在其它领域，基于注意力机制，尤其是自注意力(SelfAttention)机制构建的网络也取得了不错的效果，比如基于自注意力机制的 BigGAN 模型等</p>
<h4 id="4图卷积神经网络"><a class="markdownIt-Anchor" href="#4图卷积神经网络"></a> 4.图卷积神经网络</h4>
<p>图片、 文本等数据具有规则的空间、时间结构，称为 Euclidean Data(欧几里德数据)。卷积神经网络和循环神经网络被证明非常擅长处理这种类型的数据。而像类似于社交网络、 通信网络、 蛋白质分子结构等一系列的不规则空间拓扑结构的数据， 它们显得力不从心。 2016 年，基于前人在一阶近似的谱卷积算法上提出了图卷积网络(Graph Convolution Network， GCN)模型。 GCN 算法实现简单，从空间一阶邻居信息聚合的角度也能直观地理解，在半监督任务上取得了不错效果。随后，一系列的网络模型相继被提出，如 GAT， EdgeConv， DeepGCN 等</p>
<h2 id="keras高层接口"><a class="markdownIt-Anchor" href="#keras高层接口"></a> Keras高层接口</h2>
<p>Keras 与 tf.keras 的区别与联系：</p>
<ul>
<li>Keras 可以理解为一套搭建与训练神经网络的高层 API 协议， Keras 本身已经实现了此协议， 安装标准的 Keras 库就可以方便地调用TensorFlow、 CNTK 等后端完成加速计算</li>
<li>在 TensorFlow 中，也实现了一套 Keras 协议，即 tf.keras，它与 TensorFlow 深度融合，且只能基于 TensorFlow 后端运算， 并对TensorFlow 的支持更完美。 对于使用 TensorFlow 的开发者来说， tf.keras 可以理解为一个普通的子模块，与其他子模块，如 tf.math， tf.data 等并没有什么差别。 下文如无特别说明，Keras <strong>均指代 tf.keras</strong>，而不是标准的 Keras 库</li>
</ul>
<h3 id="1-常见功能模块"><a class="markdownIt-Anchor" href="#1-常见功能模块"></a> 1、常见功能模块</h3>
<p>Keras 提供了一系列高层的神经网络相关类和函数，如经典数据集加载函数（进阶操作–&gt;经典数据集加载 章节中讲到过）、 网络层类、 模型容器、 损失函数类、 优化器类、 经典模型类等</p>
<h4 id="1常见网络层类"><a class="markdownIt-Anchor" href="#1常见网络层类"></a> 1.常见网络层类</h4>
<p>对于常见的神经网络层，可以使用张量方式的底层接口函数来实现，这些接口函数一般在<code>tf.nn</code>模块中。</p>
<p>对于常见的<strong>网络层</strong>，我们一般直接使用层方式来完成模型的搭建，在<code>tf.keras.layers</code>命名空间(下文使用 <code>layers</code> 指代 <code>tf.keras.layers</code>)中提供了大量常见网络层的类，如全连接层、 激活函数层、 池化层、 卷积层、 循环神经网络层等。对于这些网络层类，只需要在创建时指定网络层的相关参数， 并调用<code>__call__</code>方法即可完成前向计算。在调用<code>__call__</code>方法时， Keras 会自动调用每个层的前向传播逻辑，这些逻辑一般实现在类的call 函数中。</p>
<p>以 Softmax 层为例， 它既可以使用<code>tf.nn.softmax</code>函数在前向传播逻辑中完成Softmax运算， 也可以通过<code>layers.Softmax(axis)</code>类搭建Softmax网络层，其中<code>axis</code>参数指定进行softmax 运算的维度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 导入 keras 模型，不能使用 import keras，它导入的是标准的 Keras 库</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers <span class="comment"># 导入常见网络层类</span></span><br><span class="line"><span class="comment"># 创建 Softmax 层，并调用__call__方法完成前向计算</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.constant([<span class="number">2.</span>,<span class="number">1.</span>,<span class="number">0.1</span>]) <span class="comment"># 创建输入张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>layer = layers.Softmax(axis=<span class="number">-1</span>) <span class="comment"># 创建 Softmax 层</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = layer(x) <span class="comment"># 调用 softmax 前向计算，输出为 out</span></span><br><span class="line"><span class="comment"># 经过 Softmax 网络层后， 得到概率分布 out 为：</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">2</span>, shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">0.6590012</span>, <span class="number">0.242433</span> , <span class="number">0.0985659</span>], dtype=float32)&gt;</span><br><span class="line"><span class="comment"># 当然，也可以直接通过 tf.nn.softmax()函数完成计算，代码如下：</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = tf.nn.softmax(x) <span class="comment"># 调用 softmax 函数完成前向计算</span></span><br></pre></td></tr></table></figure>
<h4 id="2网络容器"><a class="markdownIt-Anchor" href="#2网络容器"></a> 2.网络容器</h4>
<p>当网络层数变得较深时，手动调用每一层的类实例完成前向传播运算这部分代码显得非常臃肿。可以通过 Keras 提供的网络容器 <code>Sequential</code> 将多个网络层封装成一个大网络模型，只需要调用网络模型的实例一次即可完成数据从第一层到最末层的顺序传播运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 Sequential 容器</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, Sequential</span><br><span class="line">network = Sequential([ <span class="comment"># 封装为一个网络</span></span><br><span class="line">    layers.Dense(<span class="number">3</span>, activation=<span class="literal">None</span>), <span class="comment"># 全连接层，此处不使用激活函数</span></span><br><span class="line">    layers.ReLU(),<span class="comment">#激活函数层</span></span><br><span class="line">    layers.Dense(<span class="number">2</span>, activation=<span class="literal">None</span>), <span class="comment"># 全连接层，此处不使用激活函数</span></span><br><span class="line">    layers.ReLU() <span class="comment">#激活函数层</span></span><br><span class="line">])</span><br><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line">out = network(x) <span class="comment"># 输入从第一层开始， 逐层传播至输出层，并返回输出层的输出</span></span><br></pre></td></tr></table></figure>
<p>Sequential 容器也可以通过<code>add()</code>方法继续追加新的网络层， 实现动态创建网络的功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">layers_num = <span class="number">2</span> <span class="comment"># 堆叠 2 次</span></span><br><span class="line">network = Sequential([]) <span class="comment"># 先创建空的网络容器</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(layers_num):</span><br><span class="line">    network.add(layers.Dense(<span class="number">3</span>)) <span class="comment"># 添加全连接层</span></span><br><span class="line">    network.add(layers.ReLU())<span class="comment"># 添加激活函数层</span></span><br><span class="line">network.build(input_shape=(<span class="number">4</span>, <span class="number">4</span>)) <span class="comment"># 创建网络参数</span></span><br><span class="line">network.summary()</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>在完成网络创建时， 网络层类并没有创建内部权值张量等成员变量，此时通过调用类的<code>build</code>方法并指定输入大小，即可自动创建所有层的内部张量</p>
</li>
<li>
<p>通过Sequential容量封装多个网络层时，每层的参数列表将会自动并入Sequential容器的参数列表中，不需要人为合并网络参数列表</p>
</li>
<li>
<p>Sequential 对象的<code>trainable_variables</code>和<code>variables</code>包含了所有层的待优化张量列表和全部张量列表</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印网络的待优化参数名与 shape</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> p <span class="keyword">in</span> network.trainable_variables:</span><br><span class="line"><span class="meta">... </span>    print(p.name, p.shape) <span class="comment"># 参数名和形状</span></span><br><span class="line">...</span><br><span class="line">dense/kernel:<span class="number">0</span> (<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">dense/bias:<span class="number">0</span> (<span class="number">3</span>,)</span><br><span class="line">dense_1/kernel:<span class="number">0</span> (<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">dense_1/bias:<span class="number">0</span> (<span class="number">3</span>,)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>通过<code>summary()</code>函数可以方便打印出网络结构和参数量，输出：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Model: <span class="string">&quot;sequential&quot;</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param <span class="comment">#</span></span><br><span class="line">=================================================================</span><br><span class="line">dense (Dense)                multiple                  <span class="number">15</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">re_lu (ReLU)                 multiple                  <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              multiple                  <span class="number">12</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">re_lu_1 (ReLU)               multiple                  <span class="number">0</span></span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">27</span></span><br><span class="line">Trainable params: <span class="number">27</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>
<ul>
<li><code>Layer</code>：每层的名字，由 TensorFlow 内部维护，与 Python 的对象名并不一样</li>
<li><code>Param#</code>：层的参数个数</li>
<li><code>Total params</code>：统计出了总的参数量</li>
<li><code>Trainable params</code>：总的待优化参数量</li>
<li><code>Non-trainable params</code>：总的不需要优化的参数量</li>
</ul>
</li>
</ul>
<h3 id="2-模型装配-训练与测试"><a class="markdownIt-Anchor" href="#2-模型装配-训练与测试"></a> 2、模型装配、训练与测试</h3>
<p>在训练网络时，一般的流程是通过前向计算获得网络的输出值， 再通过损失函数计算网络误差，然后通过自动求导工具计算梯度并更新，同时间隔性地测试网络的性能。对于这种常用的训练逻辑，可以直接通过 Keras 提供的模型装配与训练等高层接口实现</p>
<h4 id="1模型装配"><a class="markdownIt-Anchor" href="#1模型装配"></a> 1.模型装配</h4>
<p>在 Keras 中，有 2 个比较特殊的类：</p>
<ul>
<li>keras.Model类：<strong>网络的母类</strong>，除了具有Layer类的功能，还添加了保存模型、加载模型、 训练与测试模型等便捷功能。<u>Sequential也是Model的子类</u>（具有Model类的所有功能）</li>
<li>keras.layers.Layer类：<strong>网络层的母类</strong>，定义了网络层的一些常见功能，如添加权值、 管理权值列表等</li>
</ul>
<p>下面介绍 Model 及其子类的模型装配与训练功能</p>
<p>创建网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 5 层的全连接网络</span></span><br><span class="line">network = Sequential([layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                      layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                      layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                      layers.Dense(<span class="number">32</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                      layers.Dense(<span class="number">10</span>)])</span><br><span class="line">network.build(input_shape=(<span class="number">4</span>, <span class="number">28</span>*<span class="number">28</span>))</span><br><span class="line">network.summary()</span><br></pre></td></tr></table></figure>
<p>通过<code>compile</code>函数指定网络使用的优化器对象、 损失函数类型， 评价指标等设定，这一步称为<strong>装配</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入优化器，损失函数模块</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers,losses</span><br><span class="line"><span class="comment"># 模型装配</span></span><br><span class="line"><span class="comment"># 采用 Adam 优化器，学习率为 0.01;采用交叉熵损失函数，包含 Softmax</span></span><br><span class="line">network.compile(optimizer=optimizers.Adam(lr=<span class="number">0.01</span>),</span><br><span class="line">                loss=losses.CategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">                metrics=[<span class="string">&#x27;accuracy&#x27;</span>] <span class="comment"># 设置测量指标为准确率</span></span><br><span class="line">               )</span><br></pre></td></tr></table></figure>
<h4 id="2模型训练"><a class="markdownIt-Anchor" href="#2模型训练"></a> 2.模型训练</h4>
<p>模型装配完成后，可通过<code>fit()</code>函数送入待训练的数据集和验证用的数据集，实现网络的训练与验证，这一步称为<strong>模型训练</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定训练集为 train_db，验证集为 val_db,训练 5 个 epochs，每 2 个 epoch 验证一次</span></span><br><span class="line"><span class="comment"># 返回训练轨迹信息保存在 history 对象中</span></span><br><span class="line">history = network.fit(train_db, epochs=<span class="number">5</span>, validation_data=val_db,</span><br><span class="line">                      validation_freq=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>train_db：tf.data.Dataset对象，也可以传入Numpy Array类型的数据</p>
</li>
<li>
<p>epochs：指定训练迭代的Epoch数量</p>
</li>
<li>
<p>validation_data：指定用于验证(测试)的数据集</p>
</li>
<li>
<p>validation_freq：验证的频率</p>
</li>
<li>
<p>history：训练过程的数据记录，其中<code>history.history</code>为字典对象，包含了训练过程中的loss、测量指标等记录项</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>history.history <span class="comment"># 打印训练记录</span></span><br><span class="line"><span class="comment"># 历史训练准确率</span></span><br><span class="line">&#123;<span class="string">&#x27;accuracy&#x27;</span>: [<span class="number">0.00011666667</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.010666667</span>, <span class="number">0.02495</span>],</span><br><span class="line"> <span class="string">&#x27;loss&#x27;</span>: [<span class="number">2465719710540.5845</span>, <span class="comment"># 历史训练误差</span></span><br><span class="line">          <span class="number">78167808898516.03</span>,</span><br><span class="line">          <span class="number">404488834518159.6</span>,</span><br><span class="line">          <span class="number">1049151145155144.4</span>,</span><br><span class="line">          <span class="number">1969370184858451.0</span>],</span><br><span class="line"> <span class="string">&#x27;val_accuracy&#x27;</span>: [<span class="number">0.0</span>, <span class="number">0.0</span>], <span class="comment"># 历史验证准确率</span></span><br><span class="line"> <span class="comment"># 历史验证误差</span></span><br><span class="line"> <span class="string">&#x27;val_loss&#x27;</span>: [<span class="number">197178788071657.3</span>, <span class="number">1506234836955706.2</span>]&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><code>fit()</code>函数的运行代表了网络的训练过程，会消耗相当的训练时间，并在训练结束后才返回</p>
<h4 id="3模型测试"><a class="markdownIt-Anchor" href="#3模型测试"></a> 3.模型测试</h4>
<p>关于验证和测试的区别，会在过拟合一章详细阐述，此处可以将验证和测试理解为模型评估的一种方式</p>
<p>通过<code>Model.predict(x)</code>方法即可完成模型的<strong>预测</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载一个 batch 的测试数据</span></span><br><span class="line">x,y = next(iter(db_test))</span><br><span class="line">print(<span class="string">&#x27;predict x:&#x27;</span>, x.shape) <span class="comment"># 打印当前 batch 的形状</span></span><br><span class="line">out = network.predict(x) <span class="comment"># 模型预测，预测结果保存在 out 中</span></span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure>
<p>如果只是简单的测试模型的<strong>性能</strong>，可以通过<code>Model.evaluate(db)</code>循环测试完db数据集上所有样本，并打印出性能指标：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">network.evaluate(db_test) <span class="comment"># 模型测试，测试在 db_test 上的性能表现</span></span><br></pre></td></tr></table></figure>
<h3 id="3-模型保存与加载"><a class="markdownIt-Anchor" href="#3-模型保存与加载"></a> 3、模型保存与加载</h3>
<h4 id="1张量方式"><a class="markdownIt-Anchor" href="#1张量方式"></a> 1.张量方式</h4>
<p>网络的状态主要体现在网络的结构以及网络层内部张量数据上，因此在<u>拥有网络结构源文件</u>的条件下，直接保存网络张量参数到文件系统上是最轻量级的一种方式</p>
<p>通过调用<code>Model.save_weights(path)</code>方法，可将当前的网络参数保存到path文件上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">network.save_weights(<span class="string">&#x27;weights.ckpt&#x27;</span>) <span class="comment"># 保存模型的所有张量数据</span></span><br></pre></td></tr></table></figure>
<p>在需要的时候，先创建好网络对象，然后调用网络对象的<code>load_weights(path)</code>方法即可将指定的模型文件中保存的张量数值写入到当前网络参数中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新创建相同的网络结构</span></span><br><span class="line">new_network = Sequential([layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                          layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                          layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                          layers.Dense(<span class="number">32</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">                          layers.Dense(<span class="number">10</span>)])</span><br><span class="line">new_network.compile(optimizer=optimizers.Adam(lr=<span class="number">0.01</span>),</span><br><span class="line">                    loss=tf.losses.CategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">                    metrics=[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">                   )</span><br><span class="line"><span class="comment"># 从参数文件中读取数据并写入当前网络</span></span><br><span class="line">new_network.load_weights(<span class="string">&#x27;weights.ckpt&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;loaded weights!&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="2网络方式"><a class="markdownIt-Anchor" href="#2网络方式"></a> 2.网络方式</h4>
<p>通过<code>Model.save(path)</code>函数可以将模型的<strong>结构</strong>以及模型的<strong>参数</strong>保存到path文件上，在<u>不需要网络源文件</u>的条件下，通过<code>keras.models.load_model(path)</code>即可恢复网络结构和网络参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构与模型参数到文件</span></span><br><span class="line">network.save(<span class="string">&#x27;model.h5&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;saved total model.&#x27;</span>)</span><br><span class="line"><span class="keyword">del</span> network <span class="comment"># 删除网络对象</span></span><br><span class="line"><span class="comment"># 从文件恢复网络结构与网络参数</span></span><br><span class="line">network = keras.models.load_model(<span class="string">&#x27;model.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="3savedmodel方式"><a class="markdownIt-Anchor" href="#3savedmodel方式"></a> 3.SavedModel方式</h4>
<p>当需要将模型部署到其他平台时，采用SavedModel方式更具有平台无关性。</p>
<p>通过<code>tf.saved_model.save(network, path)</code>即可将模型以SavedModel方式保存到path<strong>目录</strong>中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构与模型参数到文件</span></span><br><span class="line">tf.saved_model.save(network, <span class="string">&#x27;model-savedmodel&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;saving savedmodel.&#x27;</span>)</span><br><span class="line"><span class="keyword">del</span> network <span class="comment"># 删除网络对象</span></span><br></pre></td></tr></table></figure>
<p>通过<code>tf.saved_model.load</code>函数即可恢复出模型对象，我们在恢复出模型实例后，完成测试准确率的计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从文件恢复网络结构与网络参数</span></span><br><span class="line">network = tf.saved_model.load(<span class="string">&#x27;model-savedmodel&#x27;</span>)</span><br><span class="line"><span class="comment"># 准确率计量器</span></span><br><span class="line">acc_meter = metrics.CategoricalAccuracy()</span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> ds_val: <span class="comment"># 遍历测试集</span></span><br><span class="line">    pred = network(x) <span class="comment"># 前向计算</span></span><br><span class="line">    acc_meter.update_state(y_true=y, y_pred=pred) <span class="comment"># 更新准确率统计</span></span><br><span class="line"><span class="comment"># 打印准确率</span></span><br><span class="line">print(<span class="string">&quot;Test Accuracy:%f&quot;</span> % acc_meter.result())</span><br></pre></td></tr></table></figure>
<h3 id="4-自定义网络"><a class="markdownIt-Anchor" href="#4-自定义网络"></a> 4、自定义网络</h3>
<p>对于需要创建自定义逻辑的网络层，可以通过<strong>自定义类</strong>来实现</p>
<ul>
<li>在创建自定义<strong>网络层类</strong>时，需要继承自 layers.Layer 基类</li>
<li>创建自定义的<strong>网络类</strong>时，需要继承自 keras.Model 基类</li>
</ul>
<h4 id="1自定义网络层"><a class="markdownIt-Anchor" href="#1自定义网络层"></a> 1.自定义网络层</h4>
<p>对于自定义的网络层， 需要实现初始化<code>__init__</code>方法和前向传播逻辑<code>call</code>方法</p>
<p>以某个具体的自定义网络层为例，假设需要一个没有偏置向量的全连接层，同时固定激活函数为 ReLU 函数：</p>
<ol>
<li>
<p>首先创建类，并继承自 Layer 基类。创建初始化方法，并调用母类的初始化函数。由于是全连接层， 因此需要设置两个参数：输入特征的长度inp_dim和输出特征的长度outp_dim，并通过<code>self.add_variable(name, shape)</code>创建 shape 大小，名字为 name 的张量𝑾，并设置为需要优化</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDense</span>(<span class="params">layers.Layer</span>):</span></span><br><span class="line">    <span class="comment"># 自定义网络层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, inp_dim, outp_dim</span>):</span></span><br><span class="line">        super(MyDense, self).__init__() <span class="comment"># 调用母类的初始化函数</span></span><br><span class="line">        <span class="comment"># 创建权值张量并添加到类管理列表中，设置为需要优化</span></span><br><span class="line">        self.kernel = self.add_variable(<span class="string">&#x27;w&#x27;</span>, [inp_dim, outp_dim],</span><br><span class="line">                                        trainable=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 此外，通过 tf.Variable 创建的类成员也会自动加入类参数列表</span></span><br><span class="line">        <span class="comment"># self.kernel = tf.Variable(tf.random.normal([inp_dim, outp_dim]),</span></span><br><span class="line">        <span class="comment">#                           trainable=False)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>self.add_variable会返回张量𝑾的 Python 引用</li>
<li>变量名 name 由TensorFlow 内部维护， 使用的比较少</li>
<li>trainable：创建的张量是否需要优化</li>
</ul>
</li>
<li>
<p>设计自定义类的前向运算逻辑。对于本例，只需要完成𝑶 = 𝑿@𝑾矩阵运算，并通过固定的ReLU激活函数即可</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, training=None</span>):</span></span><br><span class="line">    <span class="comment"># 实现自定义类的前向计算逻辑</span></span><br><span class="line">    <span class="comment"># X@W</span></span><br><span class="line">    out = inputs @ self.kernel</span><br><span class="line">    <span class="comment"># 执行激活函数运算</span></span><br><span class="line">    out = tf.nn.relu(out)</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<ul>
<li>inputs：输入， 由用户在调用时传入</li>
<li>training：用于指定模型的状态：
<ul>
<li>True：执行训练模式</li>
<li>False：执行测试模式，默认参数为 None，即测试模式</li>
</ul>
</li>
<li>由于全连接层的训练模式和测试模式逻辑一致，此处不需要额外处理。对于部份测试模式和训练模式不一致的网络层，需要根据 training 参数来设计需要执行的逻辑</li>
</ul>
</li>
</ol>
<p>此时可以实例化 MyDense 类，并查看其参数列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = MyDense(<span class="number">4</span>,<span class="number">3</span>) <span class="comment"># 创建输入为 4，输出为 3 节点的自定义层</span></span><br><span class="line">net.variables,net.trainable_variables <span class="comment"># 查看自定义层的参数列表</span></span><br></pre></td></tr></table></figure>
<h4 id="2自定义网络"><a class="markdownIt-Anchor" href="#2自定义网络"></a> 2.自定义网络</h4>
<p>自定义网络类可以和其他标准类一样，通过 Sequential 容器方便地封装成一个网络模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">network = Sequential([MyDense(<span class="number">784</span>, <span class="number">256</span>), <span class="comment"># 使用自定义的层</span></span><br><span class="line">                      MyDense(<span class="number">256</span>, <span class="number">128</span>),</span><br><span class="line">                      MyDense(<span class="number">128</span>, <span class="number">64</span>),</span><br><span class="line">                      MyDense(<span class="number">64</span>, <span class="number">32</span>),</span><br><span class="line">                      MyDense(<span class="number">32</span>, <span class="number">10</span>)])</span><br><span class="line">network.build(input_shape=(<span class="literal">None</span>, <span class="number">28</span>*<span class="number">28</span>))</span><br><span class="line">network.summary()</span><br></pre></td></tr></table></figure>
<p>通过堆叠自定义网络层类，可以实现 5 层的全连接层网络，每层全连接层无偏置张量，同时激活函数固定地使用 ReLU 函数</p>
<hr>
<p>Sequential 容器适合于数据按序从第一层传播到第二层，再从第二层传播到第三层，以此规律传播的网络模型。对于复杂的网络结构，例如第三层的输入不仅是第二层的输出，还有第一层的输出，此时使用自定义网络更加灵活：</p>
<ol>
<li>
<p>创建自定义网络类，首先创建类， 并继承自 Model 基类，分别创建对应的网络层对象：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="comment"># 自定义网络类，继承自 Model 基类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(MyModel, self).__init__() <span class="comment"># 调用母类的初始化函数</span></span><br><span class="line">        <span class="comment"># 完成网络内需要的网络层的创建工作</span></span><br><span class="line">        self.fc1 = MyDense(<span class="number">28</span>*<span class="number">28</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc2 = MyDense(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc3 = MyDense(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.fc4 = MyDense(<span class="number">64</span>, <span class="number">32</span>)</span><br><span class="line">        self.fc5 = MyDense(<span class="number">32</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>实现自定义网络的前向运算逻辑：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, training=None</span>):</span></span><br><span class="line">    <span class="comment"># 自定义前向运算逻辑</span></span><br><span class="line">    x = self.fc1(inputs)</span><br><span class="line">    x = self.fc2(x)</span><br><span class="line">    x = self.fc3(x)</span><br><span class="line">    x = self.fc4(x)</span><br><span class="line">    x = self.fc5(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="5-模型乐园"><a class="markdownIt-Anchor" href="#5-模型乐园"></a> 5、模型乐园</h3>
<p>对于常用的网络模型，如 ResNet、 VGG 等，不需要手动创建网络，可以直接从<code>keras.applications</code>子模块中通过一行代码即可创建并使用这些经典模型，同时还可以通过设置 weights 参数加载预训练的网络参数</p>
<h4 id="1加载模型"><a class="markdownIt-Anchor" href="#1加载模型"></a> 1.加载模型</h4>
<p>暂无，等待施工</p>
<h3 id="6-测量工具"><a class="markdownIt-Anchor" href="#6-测量工具"></a> 6、测量工具</h3>
<p>Keras 提供了一些常用的测量工具，位于<code>keras.metrics</code>模块中，专门用于统计训练过程中常用的指标数据。Keras 的测量工具的使用方法一般有 4 个主要步骤：</p>
<ul>
<li>新建测量器</li>
<li>写入数据</li>
<li>读取统计数据</li>
<li>清零测量器</li>
</ul>
<h4 id="1新建测量器"><a class="markdownIt-Anchor" href="#1新建测量器"></a> 1.新建测量器</h4>
<p>在<code>keras.metrics</code>模块中，提供了较多的常用测量器类， 如统计平均值的 <code>Mean</code> 类，统计准确率的 <code>Accuracy</code> 类，统计余弦相似度的 <code>CosineSimilarity</code> 类等</p>
<p>例子：统计误差值</p>
<p>在前向运算时，会得到每一个 Batch 的平均误差，但是希望统计每个Step的平均误差，因此选择使用Mean测量器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建平均测量器，适合 Loss 数据</span></span><br><span class="line">loss_meter = metrics.Mean()</span><br></pre></td></tr></table></figure>
<h4 id="2写入数据"><a class="markdownIt-Anchor" href="#2写入数据"></a> 2.写入数据</h4>
<p>通过测量器的<code>update_state</code>函数可以写入新的数据，测量器会根据自身逻辑记录并处理采样数据。例如，在每个 Step 结束时采集一次 loss 值，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 记录采样的数据，通过 float()函数将张量转换为普通数值</span></span><br><span class="line">loss_meter.update_state(float(loss))</span><br></pre></td></tr></table></figure>
<ul>
<li>放置在每个 Batch 运算结束后即可， 测量器会自动根据采样的数据来统计平均值</li>
</ul>
<h4 id="3读取统计信息"><a class="markdownIt-Anchor" href="#3读取统计信息"></a> 3.读取统计信息</h4>
<p>在采样多次数据后，可以选择在需要的地方调用测量器的<code>result()</code>函数，来获取统计值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印统计期间的平均 loss</span></span><br><span class="line">print(step, <span class="string">&#x27;loss:&#x27;</span>, loss_meter.result())</span><br></pre></td></tr></table></figure>
<h4 id="4清除状态"><a class="markdownIt-Anchor" href="#4清除状态"></a> 4.清除状态</h4>
<p>测量器会统计<strong>所有历史记录</strong>的数据，因此在启动新一轮统计时，有必要清除历史状态。通过<code>reset_states()</code>即可实现清除状态功能</p>
<p>例如，在每次读取完平均误差后， 清零统计信息，以便下一轮统计的开始</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># 打印统计的平均 loss</span></span><br><span class="line">    print(step, <span class="string">&#x27;loss:&#x27;</span>, loss_meter.result())</span><br><span class="line">    loss_meter.reset_states() <span class="comment"># 打印完后， 清零测量器</span></span><br></pre></td></tr></table></figure>
<h4 id="5准确率统计实战"><a class="markdownIt-Anchor" href="#5准确率统计实战"></a> 5.准确率统计实战</h4>
<p>新建准确率测量器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">acc_meter = metrics.Accuracy() <span class="comment"># 创建准确率测量器</span></span><br></pre></td></tr></table></figure>
<p>Accuracy 类的 update_state函数的参数为预测值和真实值，而不是当前 Batch 的准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [b, 784] =&gt; [b, 10]，网络输出值</span></span><br><span class="line">out = network(x)</span><br><span class="line"><span class="comment"># [b, 10] =&gt; [b]，经过 argmax 后计算预测值</span></span><br><span class="line">pred = tf.argmax(out, axis=<span class="number">1</span>)</span><br><span class="line">pred = tf.cast(pred, dtype=tf.int32)</span><br><span class="line"><span class="comment"># 根据预测值与真实值写入测量器</span></span><br><span class="line">acc_meter.update_state(y, pred)</span><br></pre></td></tr></table></figure>
<p>在统计完测试集所有 Batch 的预测值后， 打印统计的平均准确率， 并清零测量器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取统计结果</span></span><br><span class="line">print(step, <span class="string">&#x27;Evaluate Acc:&#x27;</span>, acc_meter.result().numpy())</span><br><span class="line">acc_meter.reset_states() <span class="comment"># 清零测量器</span></span><br></pre></td></tr></table></figure>
<h3 id="7-可视化"><a class="markdownIt-Anchor" href="#7-可视化"></a> 7、可视化</h3>
<p>TensorFlow 提供了一个可视化工具<code>TensorBoard</code>。原理是通过将监控数据写入到文件系统， 并利用Web后端监控对应的文件目录， 从而可以允许用户从远程查看网络的监控数据。</p>
<p>TensorBoard 的使用需要模型代码和浏览器相互配合。在使用 TensorBoard 之前，需要安装 TensorBoard 库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 TensorBoard</span></span><br><span class="line">pip install tensorboard</span><br></pre></td></tr></table></figure>
<h4 id="1模型端"><a class="markdownIt-Anchor" href="#1模型端"></a> 1.模型端</h4>
<p>创建写入监控数据的<code>Summary</code>类， 并在需要的时候写入监控数据即可。</p>
<p>首先通过<code>tf.summary.create_file_writer</code>创建监控对象类实例，并指定监控数据的写入<u>目录</u></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建监控类，监控数据将写入 log_dir 目录</span></span><br><span class="line">summary_writer = tf.summary.create_file_writer(log_dir)</span><br></pre></td></tr></table></figure>
<hr>
<p>例子：监控误差数据和可视化数据</p>
<p>在前向计算完成后，对于误差这种<strong>标量</strong>数据， 我们通过<code>tf.summary.scalar</code>函数记录监控数据，并指定时间戳 step 参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> summary_writer.as_default(): <span class="comment"># 写入环境</span></span><br><span class="line">    <span class="comment"># 当前时间戳 step 上的数据为 loss，写入到名为 train-loss 数据库中</span></span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;train-loss&#x27;</span>, float(loss), step=step)</span><br></pre></td></tr></table></figure>
<ul>
<li>step：类似于每个数据对应的时间刻度信息（可以理解为数据曲线的x坐标），不宜重复。</li>
<li>每类数据通过字符串名字来区分，同类的数据需要写入相同名字的数据库中</li>
</ul>
<p>对于<strong>图片</strong>类型的数据， 可以通过<code>tf.summary.image</code>函数监控多个图片的张量数据，并通过设置max_outputs参数来选择最多显示的图片数量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> summary_writer.as_default():<span class="comment"># 写入环境</span></span><br><span class="line">    <span class="comment"># 写入测试准确率</span></span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;test-acc&#x27;</span>, float(total_correct/total), step=step)</span><br><span class="line">    <span class="comment"># 可视化测试用的图片，设置最多可视化 9 张图片</span></span><br><span class="line">    tf.summary.image(<span class="string">&quot;val-onebyone-images:&quot;</span>, val_images, max_outputs=<span class="number">9</span>, step=step)</span><br></pre></td></tr></table></figure>
<h4 id="2浏览器端"><a class="markdownIt-Anchor" href="#2浏览器端"></a> 2.浏览器端</h4>
<p>打开 Web 后端：通过在 cmd 终端运行<code>tensorboard --logdir path</code>指定 Web 后端监控的文件目录 path， 即可打开 Web 后端监控进程</p>
<p>之后打开浏览器，输入网址<code>http://localhost:6006</code>(也可通过 IP 地址远程访问， 具体端口号可能会变动，可查看命令提示) 即可监控网络训练进度</p>
<p>除了监控标量数据和图片数据外， TensorBoard 还支持通过<code>tf.summary.histogram</code>查看张量数据的直方图分布，以及通过<code>tf.summary.text</code>打印文本信息等功能</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> summary_writer.as_default():</span><br><span class="line">    <span class="comment"># 当前时间戳 step 上的数据为 loss，写入到 ID 位 train-loss 对象中</span></span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;train-loss&#x27;</span>, float(loss), step=step)</span><br><span class="line">    <span class="comment"># 可视化真实标签的直方图分布</span></span><br><span class="line">    tf.summary.histogram(<span class="string">&#x27;y-hist&#x27;</span>,y, step=step)</span><br><span class="line">    <span class="comment"># 查看文本信息</span></span><br><span class="line">    tf.summary.text(<span class="string">&#x27;loss-text&#x27;</span>,str(float(loss)))</span><br></pre></td></tr></table></figure>
<p>实际上，除了 TensorBoard 外，Visdom 工具同样可以方便可视化数据。Visdom 可以直接接受PyTorch 的张量类型的数据，但不能直接接受 TensorFlow 的张量类型数据，需要转换为Numpy 数组</p>
<h2 id="过拟合"><a class="markdownIt-Anchor" href="#过拟合"></a> 过拟合</h2>
<p>机器学习的主要目的是从训练集上学习到数据的真实模型， 从而能够在未见过的测试集上也能够表现良好，我们把这种能力叫做<strong>泛化能力</strong></p>
<h3 id="1-模型的容量"><a class="markdownIt-Anchor" href="#1-模型的容量"></a> 1、模型的容量</h3>
<p>通俗地讲，模型的容量或表达能力，是指模型拟合复杂函数的能力。一种体现模型容量的指标为模型的假设空间(Hypothesis Space)大小，即模型可以表示的函数集的大小。</p>
<p>假设空间越大越完备， 从假设空间中搜索出逼近真实模型的函数也就越有可能； 反之，如果假设空间非常受限，就很难从中找到逼近真实模型的函数</p>
<p>实际上，较大的假设空间并不一定能搜索出更好的函数模型。 由于观测误差的存在，较大的假设空间中可能包含了大量表达能力过强的函数， 能够将训练样本的观测误差也学习进来，从而伤害了模型的泛化能力。挑选合适容量的学习模型是一个很大的难题</p>
<h3 id="2-过拟合与欠拟合"><a class="markdownIt-Anchor" href="#2-过拟合与欠拟合"></a> 2、过拟合与欠拟合</h3>
<ul>
<li>过拟合(Overfitting)：当模型的容量过大时，网络模型除了学习到训练集数据的模态之外，还把额外的观测误差也学习进来，导致学习的模型在训练集上面表现较好，但是在未见的样本上表现不佳，也就是模型泛化能力偏弱</li>
<li>欠拟合(Underfitting)：当模型的容量过小时，模型不能够很好地学习到训练集数据的模态，导致训练集上表现不佳，同时在未见的样本上表现也不佳</li>
</ul>
<p>那么如何去选择模型的容量？</p>
<ul>
<li>
<p>统计学习理论中的 VC 维度(Vapnik-Chervonenkis 维度)是一个应用比较广泛的度量函数容量的方法。但是该方法却很少应用到深度学习中去，一部分原因是神经网络过于复杂，很难去确定网络结构背后的数学模型的 VC 维度</p>
</li>
<li>
<p>可以根据奥卡姆剃刀原理(Occam’s razor)来指导神经网络的设计和训练。即“切勿浪费较多东西，去做‘用较少的东西，同样可以做好的事情’。”。也就是说，如果两层的神经网络结构能够很好的表达真实模型，那么三层的神经网络也能够很好的表达，但是我们应该优先选择使用更简单的两层神经网络，因为它的参数量更少，更容易训练，也更容易通过较少的训练样本获得不错的泛化误差</p>
</li>
</ul>
<h4 id="1欠拟合"><a class="markdownIt-Anchor" href="#1欠拟合"></a> 1.欠拟合</h4>
<p>当我们发现当前的模型在训练集上误差一直维持较高的状态，很难优化减少，同时在测试集上也表现不佳时，我们可以考虑是否出现了欠拟合的现象</p>
<p>可以通过增加神经网络的层数、增大中间维度的大小等手段， 比较好的解决欠拟合的问题。</p>
<p>在实际使用过程中，更多的是出现过拟合现象</p>
<h4 id="2过拟合"><a class="markdownIt-Anchor" href="#2过拟合"></a> 2.过拟合</h4>
<p>现代深度神经网络中过拟合现象非常容易出现，主要是因为神经网络的表达能力非常强， 训练集样本数不够</p>
<h3 id="3-数据集划分"><a class="markdownIt-Anchor" href="#3-数据集划分"></a> 3、数据集划分</h3>
<p>前面我们介绍了数据集需要划分为训练集(Train set)和测试集(Test set)，但是为了挑选模型超参数和检测过拟合现象，一般需要将原来的训练集再次切分为新的训练集和验证集(Validation set)，即数据集需要切分为训练集、验证集和测试集 3 个子集</p>
<h4 id="1验证集与超参数"><a class="markdownIt-Anchor" href="#1验证集与超参数"></a> 1.验证集与超参数</h4>
<p>验证集：选择模型的超参数(模型选择， Model selection)，功能包括：</p>
<ul>
<li>根据验证集的性能表现来调整学习率、 权值衰减系数、 训练次数等</li>
<li>根据验证集的性能表现来重新调整网络拓扑结构</li>
<li>根据验证集的性能表现判断是否过拟合和欠拟合</li>
</ul>
<p>训练集、验证集和测试集可以按着自定义的比例来划分，比如常见的 60%-20%-20%的划分</p>
<p>验证集与测试集的区别：</p>
<ul>
<li>算法设计人员可以根据<strong>验证集</strong>的表现来调整模型的各种超参数的设置，提升模型的<strong>泛化能力</strong>（测试泛化性能）</li>
<li>测试集的表现不能用来反馈模型的调整，否则测试集将和验证集的功能重合， 因此在测试集上的性能表现将<strong>无法</strong>代表模型的泛化能力</li>
</ul>
<h4 id="2提前停止"><a class="markdownIt-Anchor" href="#2提前停止"></a> 2.提前停止</h4>
<p>一般把对训练集中的一个<strong>Batch</strong>运算更新一次叫做一个<strong>Step</strong>， 对训练集的所有样本循环迭代一次叫做一个<strong>Epoch</strong>。验证集可以在数次 Step 或数次 Epoch 后使用，计算模型的验证性能（一般建议几个 Epoch 后进行一次验证运算）</p>
<ul>
<li>训练时，一般关注的指标有训练误差、 训练准确率等</li>
<li>验证时，也有验证误差和验证准确率等</li>
<li>测试时，也有测试误差和测试准确率等</li>
</ul>
<p>通过观测<u>训练准确率</u>和<u>验证准确率</u>可以大致推断模型是否出现过拟合和欠拟合</p>
<ul>
<li>过拟合：如果模型的训练误差较低，训练准确率较高，但是验证误差较高，验证准确率较低
<ul>
<li>解决方法：可以从新设计网络模型的容量，如降低网络的层数、降低网络的参数量、 添加正则化手段、 添加假设空间的约束等，使得模型的实际容量降低</li>
</ul>
</li>
<li>欠拟合：如果训练集和验证集上面的误差都较高，准确率较低
<ul>
<li>解决方法：尝试增大网络的容量，如加深网络的层数、 增加网络的参数量，尝试更复杂的网络结构</li>
</ul>
</li>
</ul>
<p>实际上， 由于网络的实际容量可以随着训练的进行发生改变，因此在相同的网络设定下，随着训练的进行， 可能观测到不同的过拟合、 欠拟合状况</p>
<ul>
<li>在训练的前期，随着训练的进行，模型的训练准确率和测试准确率都呈现增大的趋势，此时并没有出现过拟合现象</li>
<li>在训练后期，即使是相同网络结构下， 由于模型的实际容量发生改变，我们观察到了过拟合的现象，具体表现为<u>训练准确度继续改善</u>，但是泛化能力变弱(<u>测试准确率减低</u>)</li>
</ul>
<p>记录模型的验证准确率，并监控验证准确率的变化， 当发现验证准确率连续𝑛个 Epoch 没有下降时，可以预测可能已经达到了最适合的 Epoch 附近，从而<strong>提前终止</strong>训练</p>
<h3 id="4-模型设计"><a class="markdownIt-Anchor" href="#4-模型设计"></a> 4、模型设计</h3>
<p>对于神经网络来说，网络的层数和参数量是网络容量很重要的参考指标</p>
<ul>
<li>减少网络的层数， 减少每层中网络参数量的规模， 可以有效降低网络的容量</li>
<li>如果发现模型欠拟合，需要增大网络的容量，可以通过增加层数，增大每层的参数量等方式实现</li>
</ul>
<h3 id="5-正则化"><a class="markdownIt-Anchor" href="#5-正则化"></a> 5、正则化</h3>
<p>待补充</p>
<h3 id="6-dropout"><a class="markdownIt-Anchor" href="#6-dropout"></a> 6、Dropout</h3>
<p>Dropout 通过随机断开神经网络的连接，减少每次训练时实际参与计算的模型的参数量；但是在测试时， Dropout 会恢复所有的连接，保证模型测试时获得最好的性能</p>
<p>在 TensorFlow 中，可以通过<code>tf.nn.dropout(x, rate)</code>函数实现某条连接的 Dropout 功能，其中 rate 参数设置断开的概率值𝑝</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加 dropout 操作，断开概率为 0.5</span></span><br><span class="line">x = tf.nn.dropout(x, rate=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
<p>也可以将 Dropout 作为一个网络层使用， 在网络中间插入一个 Dropout 层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加 Dropout 层，断开概率为 0.5</span></span><br><span class="line">model.add(layers.Dropout(rate=<span class="number">0.5</span>))</span><br></pre></td></tr></table></figure>
<p>随着 Dropout 层的增加，网络模型训练时的实际容量减少，泛化能力变强</p>
<h3 id="7-数据增强"><a class="markdownIt-Anchor" href="#7-数据增强"></a> 7、数据增强</h3>
<p>增加数据集规模是解决过拟合最重要的途径。在有限的数据集上，通过数据增强技术可以增加训练的样本数量，获得一定程度上的性能提升</p>
<p>数据增强(Data Augmentation)是指在维持样本标签不变的条件下，根据先验知识改变样本的特征， 使得新产生的样本也符合或者近似符合数据的真实分布</p>
<p>以图片数据为例。数据集中的图片大小往往是不一致的，为了方便神经网络处理，需要将图片缩放到某个固定的大小，如缩放后的固定224 × 224大小的图片。对于图中的人物图片， 根据先验知识，我们知道旋转、缩放、 平移、裁剪、改变视角、 遮挡某局部区域都不会改变图片的主体类别标签，因此针对图片数据，可以有多种数据增强方式</p>
<p>TensorFlow 中提供了常用图片的处理函数， 位于<code>tf.image</code>子模块中。通过<code>tf.image.resize</code>函数可以实现图片的缩放功能</p>
<p>将图片从文件系统读取进来后，即可进行图片数据增强操作。通过预处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params">x,y</span>):</span></span><br><span class="line"><span class="comment"># 预处理函数</span></span><br><span class="line"><span class="comment"># x: 图片的路径， y：图片的数字编码</span></span><br><span class="line">x = tf.io.read_file(x)</span><br><span class="line">x = tf.image.decode_jpeg(x, channels=<span class="number">3</span>) <span class="comment"># RGBA</span></span><br><span class="line"><span class="comment"># 图片缩放到 244x244 大小，这个大小根据网络设定自行调整</span></span><br><span class="line">x = tf.image.resize(x, [<span class="number">244</span>, <span class="number">244</span>])</span><br></pre></td></tr></table></figure>
<h4 id="1旋转"><a class="markdownIt-Anchor" href="#1旋转"></a> 1.旋转</h4>
<p>通过<code>tf.image.rot90(x, k=1)</code>可以实现图片按逆时针方式旋转 k 个 90 度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图片逆时针旋转 180 度</span></span><br><span class="line">x = tf.image.rot90(x,<span class="number">2</span>)  </span><br></pre></td></tr></table></figure>
<h4 id="2翻转"><a class="markdownIt-Anchor" href="#2翻转"></a> 2.翻转</h4>
<p>图片的翻转分为沿水平轴翻转和竖直轴翻转，可以通过<code>tf.image.random_flip_left_right</code>和<code>tf.image.random_flip_up_down</code>实现图片在水平方向和竖直方向的随机翻转操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机水平翻转</span></span><br><span class="line">x = tf.image.random_flip_left_right(x)</span><br><span class="line"><span class="comment"># 随机竖直翻转</span></span><br><span class="line">x = tf.image.random_flip_up_down(x)</span><br></pre></td></tr></table></figure>
<h4 id="3裁剪"><a class="markdownIt-Anchor" href="#3裁剪"></a> 3.裁剪</h4>
<p>通过在原图的左右或者上下方向去掉部分边缘像素，可以保持图片主体不变，同时获得新的图片样本。在实际裁剪时，一般先将图片缩放到略大于网络输入尺寸的大小， 再裁剪到合适大小</p>
<p>如网络的输入大小为224 × 224，那么可以先通过 resize 函数将图片缩放到244 × 244大小，再随机裁剪到224 × 224大小：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图片先缩放到稍大尺寸</span></span><br><span class="line">x = tf.image.resize(x, [<span class="number">244</span>, <span class="number">244</span>])</span><br><span class="line"><span class="comment"># 再随机裁剪到合适尺寸</span></span><br><span class="line">x = tf.image.random_crop(x, [<span class="number">224</span>,<span class="number">224</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<h4 id="4生成数据"><a class="markdownIt-Anchor" href="#4生成数据"></a> 4.生成数据</h4>
<p>通过生成模型在原有数据上进行训练， 学习到真实数据的分布，从而利用生成模型获得新的样本，这种方式也可以在一定程度上提升网络性能。 如通过条件生成对抗网络(Conditional GAN,简称 CGAN)可以生成带标签的样本数据</p>
<h4 id="5其他方式"><a class="markdownIt-Anchor" href="#5其他方式"></a> 5.其他方式</h4>
<p>除了上述介绍的典型图片数据增强方式以外，可以根据先验知识，在不改变图片标签信息的条件下，任意变换图片数据，获得新的图片。如在原图上叠加高斯噪声、通过改变图片的观察视角后、在原图上随机遮挡部分区域等</p>
<h3 id="8-过拟合问题"><a class="markdownIt-Anchor" href="#8-过拟合问题"></a> 8、过拟合问题</h3>
<h4 id="1数据集构建"><a class="markdownIt-Anchor" href="#1数据集构建"></a> 1.数据集构建</h4>
<p>我们使用的数据集样本特性向量长度为 2， 标签为 0 或 1，分别代表了两种类别。借助于<code>scikit-learn</code>库中提供的<code>make_moons</code>工具， 我们可以生成任意多数据的训练集。首先安装 scikit-learn 库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip 安装 scikit-learn 库</span></span><br><span class="line">pip install -U scikit-learn</span><br></pre></td></tr></table></figure>
<p>为了演示过拟合现象，采样1000个样本数据，同时添加标准差为 0.25 的高斯噪声数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入数据集生成工具</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"><span class="comment"># 从 moon 分布中随机采样 1000 个点，并切分为训练集-测试集</span></span><br><span class="line">X, y = make_moons(n_samples = N_SAMPLES, noise=<span class="number">0.25</span>, random_state=<span class="number">100</span>)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y,</span><br><span class="line">test_size = TEST_SIZE, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>
<p>编写make_plot函数，方便根据样本的坐标 X 和样本的标签 y 绘制出数据的分布图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_plot</span>(<span class="params">X, y, plot_name, file_name, XX=None, YY=None, preds=None</span>):</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    <span class="comment"># sns.set_style(&quot;whitegrid&quot;)</span></span><br><span class="line">    axes = plt.gca()</span><br><span class="line">    axes.set_xlim([x_min,x_max])</span><br><span class="line">    axes.set_ylim([y_min,y_max])</span><br><span class="line">    axes.set(xlabel=<span class="string">&quot;$x_1$&quot;</span>, ylabel=<span class="string">&quot;$x_2$&quot;</span>)</span><br><span class="line">    <span class="comment"># 根据网络输出绘制预测曲面</span></span><br><span class="line">    <span class="keyword">if</span>(XX <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> YY <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> preds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">        plt.contourf(XX, YY, preds.reshape(XX.shape), <span class="number">25</span>, alpha = <span class="number">0.08</span>,</span><br><span class="line">                     cmap=cm.Spectral)</span><br><span class="line">        plt.contour(XX, YY, preds.reshape(XX.shape), levels=[<span class="number">.5</span>],</span><br><span class="line">                    cmap=<span class="string">&quot;Greys&quot;</span>,</span><br><span class="line">                    vmin=<span class="number">0</span>, vmax=<span class="number">.6</span>)</span><br><span class="line">    <span class="comment"># 绘制正负样本</span></span><br><span class="line">    markers = [<span class="string">&#x27;o&#x27;</span> <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">else</span> <span class="string">&#x27;s&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> y.ravel()]</span><br><span class="line">    mscatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y.ravel(), s=<span class="number">20</span>,</span><br><span class="line">             cmap=plt.cm.Spectral, edgecolors=<span class="string">&#x27;none&#x27;</span>, m=markers)</span><br><span class="line">    <span class="comment"># 保存矢量图</span></span><br><span class="line">    plt.savefig(OUTPUT_DIR+<span class="string">&#x27;/&#x27;</span>+file_name)</span><br></pre></td></tr></table></figure>
<p>绘制出采样的 1000 个样本分布：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制数据集分布</span></span><br><span class="line">make_plot(X, y, <span class="literal">None</span>, <span class="string">&quot;dataset.svg&quot;</span>)  </span><br></pre></td></tr></table></figure>
<h4 id="2网络层数的影响"><a class="markdownIt-Anchor" href="#2网络层数的影响"></a> 2.网络层数的影响</h4>
<p>为了探讨不同的网络深度下的过拟合程度，我们共进行了 5 次训练实验。在𝑛 ∈ [0,4]时，构建网络层数为𝑛 + 2层的全连接层网络，并通过 Adam 优化器训练 500 个 Epoch，获得网络在训练集上的分隔曲线</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">5</span>): <span class="comment"># 构建 5 种不同层数的网络</span></span><br><span class="line">    model = Sequential()<span class="comment"># 创建容器</span></span><br><span class="line">    <span class="comment"># 创建第一层</span></span><br><span class="line">    model.add(Dense(<span class="number">8</span>, input_dim=<span class="number">2</span>,activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(n): <span class="comment"># 添加 n 层，共 n+2 层</span></span><br><span class="line">        model.add(Dense(<span class="number">32</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)) <span class="comment"># 创建最末层</span></span><br><span class="line">    model.compile(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">                  metrics=[<span class="string">&#x27;accuracy&#x27;</span>]) <span class="comment"># 模型装配与训练</span></span><br><span class="line">    history = model.fit(X_train, y_train, epochs=N_EPOCHS, verbose=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 绘制不同层数的网络决策边界曲线</span></span><br><span class="line">    preds = model.predict_classes(np.c_[XX.ravel(), YY.ravel()])</span><br><span class="line">    title = <span class="string">&quot;网络层数(&#123;&#125;)&quot;</span>.format(n)</span><br><span class="line">    file = <span class="string">&quot;网络容量%f.png&quot;</span>%(<span class="number">2</span>+n*<span class="number">1</span>)</span><br><span class="line">    make_plot(X_train, y_train, title, file, XX, YY, preds)</span><br></pre></td></tr></table></figure>
<h4 id="3dropout的影响"><a class="markdownIt-Anchor" href="#3dropout的影响"></a> 3.Dropout的影响</h4>
<p>待添加</p>
<h4 id="4正则化的影响"><a class="markdownIt-Anchor" href="#4正则化的影响"></a> 4.正则化的影响</h4>
<p>待添加</p>
<h2 id="卷积神经网络"><a class="markdownIt-Anchor" href="#卷积神经网络"></a> 卷积神经网络</h2>
<h3 id="1-全连接层的问题"><a class="markdownIt-Anchor" href="#1-全连接层的问题"></a> 1、全连接层的问题</h3>
<p>全连接层较高的内存占用量严重限制了神经网络朝着更大规模、更深层数方向的发展</p>
<h4 id="1局部相关性"><a class="markdownIt-Anchor" href="#1局部相关性"></a> 1.局部相关性</h4>
<p>网络的每个输出节点都与所有的输入节点相连接， 用于提取所有输入节点的特征信息，这种稠密的连接方式是全连接层参数量大、 计算代价高的根本原因。 全连接层也称为稠密连接层(Dense Layer)</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200511214046062.png" alt="全连接示意图"></p>
<p>基于距离的重要性分布假设特性称为<strong>局部相关性</strong>，只关注和自己距离较近的部分节点，而忽略距离较远的节点。 在这种重要性分布假设下，全连接层的连接模式变成了下图所示的状态，输出节点𝑗只与以𝑗为中心的局部区域(感受野)相连接，与其它像素无连接</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200511214621069.png" alt="局部链接的网络层示意图"></p>
<p>其中和自己距离较近的部分节点形成的窗口称为<strong>感受野</strong>(Receptive Field)，表征了每个像素对于中心像素的重要性分布情况，网格内的像素才会被考虑，网格外的像素对于中心像素会被忽略</p>
<h4 id="2劝值共享"><a class="markdownIt-Anchor" href="#2劝值共享"></a> 2.劝值共享</h4>
<p>如下图所示，在计算左上角位置的输出像素时，使用权值矩阵：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>11</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>12</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>13</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>21</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>22</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>23</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>31</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>32</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>33</mn></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">W=\begin{bmatrix}
w_{11} &amp; w_{12} &amp; w_{13}\\ 
w_{21} &amp; w_{22} &amp; w_{23}\\ 
w_{31} &amp; w_{32} &amp; w_{33}
\end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.60004em;vertical-align:-1.55002em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>与对应感受野内部的像素相乘累加， 作为左上角像素的输出值；在计算右下方感受野区域时，共享权值参数𝑾，即使用相同的权值参数𝑾相乘累加，得到右下角像素的输出值，此时网络层的参数量9个，且与输入、输出节点数无关</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200511215857765.png" alt="劝值共享矩阵示意图"></p>
<p>通过运用局部相关性和权值共享的思想，成功把网络的参数量减少到𝑘 × 𝑘(准确地说，是在单输入通道、 单卷积核的条件下)。这种共享权值的“局部连接层”网络其实就是卷积神经网络</p>
<h4 id="3卷积运算"><a class="markdownIt-Anchor" href="#3卷积运算"></a> 3.卷积运算</h4>
<p>略</p>
<h3 id="2-卷积神经网络"><a class="markdownIt-Anchor" href="#2-卷积神经网络"></a> 2、卷积神经网络</h3>
<p>卷积神经网络通过充分利用局部相关性和权值共享的思想，大大地减少了网络的参数量， 从而提高训练效率，更容易实现超大规模的深层网络</p>
<p>以图片数据为例，卷积层接受高、 宽分别为ℎ、 𝑤，通道数为𝑐𝑖𝑛的输入特征图𝑿，在𝑐𝑜𝑢𝑡个高、 宽都为𝑘，通道数为𝑐𝑖𝑛的卷积核作用下，生成高、 宽分别为ℎ′、 𝑤′，通道数为𝑐𝑜𝑢𝑡的特征图输出。需要注意的是，卷积核的高宽可以不等，为了简化讨论，这里仅讨论高宽都为𝑘的情况</p>
<h4 id="1单通道输入和单卷积核"><a class="markdownIt-Anchor" href="#1单通道输入和单卷积核"></a> 1.单通道输入和单卷积核</h4>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200512112621597.png" alt="示意图"></p>
<p>完成第一个感受野区域的特征提取后，感受野窗口向右移动一个步长单位(Strides， 记为𝑠， 默认为 1)</p>
<h4 id="2多通道输入和单卷积核"><a class="markdownIt-Anchor" href="#2多通道输入和单卷积核"></a> 2.多通道输入和单卷积核</h4>
<p>在多通道输入的情况下， 卷积核的通道数需要和输入𝑿的通道数量相匹配， 卷积核的第𝑖个通道和𝑿的第𝑖个通道运算，得到第𝑖个中间矩阵，此时可以视为单通道输入与单卷积核的情况， 所有通道的中间矩阵对应元素再次相加， 作为最终输出</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200512112150245.png" alt="多通道输入和单卷积核"></p>
<p>整个的计算示意图如下所示， 输入的每个通道处的感受野均与卷积核的对应通道相乘累加，得到与通道数量相等的中间变量，这些中间变量全部相加即得到当前位置的输出值。 输入通道的通道数量决定了卷积核的通道数。 一个卷积核只能得到一个输出矩阵，无论输入𝑿的通道数量</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200512112452481.png" alt="多通道输入和单卷积核计算示意图"></p>
<h4 id="3多通道输入-多卷积核"><a class="markdownIt-Anchor" href="#3多通道输入-多卷积核"></a> 3.多通道输入、多卷积核</h4>
<p>一般来说，一个卷积核只能完成某种逻辑的特征提取，当需要同时提取多种逻辑特征时， 可以通过增加多个卷积核来得到多种特征，提高神经网络的表达能力，这就是多通道输入、 多卷积核的情况</p>
<p>当出现多卷积核时， 第𝑖 (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">i \in [1,n]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69862em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mclose">]</span></span></span></span>， 𝑛为卷积核个数)个卷积核与输入𝑿运算得到第𝑖个输出矩阵(也称为输出张量𝑶的通道𝑖）， 最后全部的输出矩阵在通道维度上进行拼接(Stack 操作，创建输出通道数的新维度)，产生输出张量𝑶， 𝑶包含了𝑛个通道数</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200512113018813.png" alt="多卷积核示意图"></p>
<ul>
<li>每个卷积核的大小𝑘、步长𝑠、填充设定等都是统一设置（保证输出的每个通道大小一致）</li>
</ul>
<h4 id="4步长"><a class="markdownIt-Anchor" href="#4步长"></a> 4.步长</h4>
<p>感受野密度的控制手段一般是通过移动步长(Strides)实现的</p>
<p>步长是指感受野窗口每次移动的长度单位，对于2D输入来说，分为沿𝑥(向右)方向和𝑦(向下)方向的移动长度</p>
<ul>
<li>当步长设计的较小时，感受野以较小幅度移动窗口，有利于提取到更多的特征信息，输出张量的尺寸也更大</li>
<li>当步长设计的较大时， 感受野以较大幅度移动窗口，有利于减少计算代价， 过滤冗余信息，输出张量的尺寸也更小</li>
</ul>
<h4 id="5填充"><a class="markdownIt-Anchor" href="#5填充"></a> 5.填充</h4>
<p>在网络模型设计时，有时希望输出𝑶的高宽能够与输入𝑿的高宽相同， 从而方便网络参数的设计、 残差连接等</p>
<p>方法：通过在原输入𝑿的高和宽维度上面进行填充(Padding)若干无效元素操作，得到增大的输入𝑿′。 通过精心设计填充单元的数量， 在𝑿′上面进行卷积运算得到输出𝑶的高宽可以和原输入𝑿相等，甚至更大</p>
<p>卷积神经层的输出尺寸<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><msup><mi>h</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><msup><mi>w</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><msub><mi>c</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[b,h&#x27;,w&#x27;,c_{out}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.001892em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>由卷积核的数量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>c</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">c_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，卷积核的大小𝑘，步长𝑠，填充数𝑝(只考虑上下填充数量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mi>h</mi></msub></mrow><annotation encoding="application/x-tex">p_h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>相同，左右填充数量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">p_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>相同的情况)以及输入𝑿的高宽ℎ/𝑤共同决定， 它们之间的数学关系可以表达为：</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>h</mi><mo mathvariant="normal">′</mo></msup><mo>=</mo><mrow><mo fence="true">⌊</mo><mfrac><mrow><mi>h</mi><mo>+</mo><mn>2</mn><mo>⋅</mo><msub><mi>p</mi><mi>h</mi></msub><mo>−</mo><mi>k</mi></mrow><mi>s</mi></mfrac><mo fence="true">⌋</mo></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">h&#x27;=\left \lfloor \frac{h+2\cdot p_h-k}{s} \right \rfloor+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.751892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.80002em;vertical-align:-0.65002em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">⌊</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mbin mtight">+</span><span class="mord mtight">2</span><span class="mbin mtight">⋅</span><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">⌋</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span></p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>w</mi><mo mathvariant="normal">′</mo></msup><mo>=</mo><mrow><mo fence="true">⌊</mo><mfrac><mrow><mi>w</mi><mo>+</mo><mn>2</mn><mo>⋅</mo><msub><mi>p</mi><mi>w</mi></msub><mo>−</mo><mi>k</mi></mrow><mi>s</mi></mfrac><mo fence="true">⌋</mo></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">w&#x27;=\left \lfloor \frac{w+2\cdot p_w-k}{s} \right \rfloor+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.751892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.80002em;vertical-align:-0.65002em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">⌊</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="mbin mtight">+</span><span class="mord mtight">2</span><span class="mbin mtight">⋅</span><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">⌋</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span></p>
<ul>
<li>其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mi>h</mi></msub></mrow><annotation encoding="application/x-tex">p_h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">p_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>分别表示高、宽方向的填充数量</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo fence="true">⌊</mo><mo fence="true">⌋</mo></mrow><annotation encoding="application/x-tex">\left \lfloor \right \rfloor</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">⌊</span><span class="mclose delimcenter" style="top:0em;">⌋</span></span></span></span></span>表示向下取整</li>
</ul>
<p>在 TensorFlow 中， 在𝑠 = 1 时， 如果希望输出𝑶和输入𝑿高、 宽相等， 只需要简单地设置参数<code>padding=”SAME”</code>即可使 TensorFlow 自动计算 padding 数量</p>
<h3 id="3-卷积层实现"><a class="markdownIt-Anchor" href="#3-卷积层实现"></a> 3、卷积层实现</h3>
<p>在 TensorFlow 中，既可以通过自定义权值的底层实现方式搭建神经网络，也可以直接调用现成的卷积层类的高层方式快速搭建复杂网络</p>
<h4 id="1自定义权值"><a class="markdownIt-Anchor" href="#1自定义权值"></a> 1.自定义权值</h4>
<p>通过<code>tf.nn.conv2d</code>函数可以方便地实现 2D 卷积运算。<code>tf.nn.conv2d</code>基于输入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mo>:</mo><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi><mo separator="true">,</mo><msub><mi>c</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">X:[b,h,w,c_{in}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>和卷积核<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi><mo>:</mo><mo stretchy="false">[</mo><mi>k</mi><mo separator="true">,</mo><mi>k</mi><mo separator="true">,</mo><msub><mi>c</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>c</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">W:[k,k,c_{in},c_{out}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>进行卷积运算， 得到输出<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>:</mo><mo stretchy="false">[</mo><mi>b</mi><mo separator="true">,</mo><msup><mi>h</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><msup><mi>w</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><msub><mi>c</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">O:[b,h&#x27;,w&#x27;,c_{out}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.001892em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>c</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">c_{in}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：输入通道数</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>c</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">c_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：卷积核的数量，即输出特征图的通道数</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>：卷积核宽高</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>：图片数量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>]) <span class="comment"># 模拟输入， 3 通道，高宽为 5</span></span><br><span class="line"><span class="comment"># 需要根据[k,k,cin,cout]格式创建 W 张量， 4 个 3x3 大小卷积核</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>w = tf.random.normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="comment"># 步长为 1, padding 为 0,</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = tf.nn.conv2d(x,w,strides=<span class="number">1</span>,padding=[[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line"><span class="comment"># 输出张量的 shape</span></span><br><span class="line">TensorShape([<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>padding参数格式：<code>padding=[[0,0],[上,下],[左,右],[0,0]]</code></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上下左右各填充一个单位：</span></span><br><span class="line">padding=[[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>特别地， 通过设置参数<code>padding='SAME'</code>、<code>strides=1</code>可以直接得到输入、 输出同大小的卷积层</p>
</li>
<li>
<p>当<code>strides&gt;1</code>时， 设置<code>padding='SAME'</code>将使得输出高、宽将成<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mn>1</mn><mrow><mi>s</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>d</mi><mi>e</mi><mi>s</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{strides}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">s</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>倍地减少</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.random.normal([<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>w = tf.random.normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="comment"># 高宽先 padding 成可以整除 3 的最小整数 6，然后 6 按 3 倍减少，得到 2x2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = tf.nn.conv2d(x,w,strides=<span class="number">3</span>,padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line">TensorShape([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>卷积神经网络层与全连接层一样，可以设置网络带偏置向量。<code>tf.nn.conv2d</code>函数是没有实现偏置向量计算的， 添加偏置需要手动累加偏置张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据[cout]格式创建偏置向量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.zeros([<span class="number">4</span>])</span><br><span class="line"><span class="comment"># 在卷积输出上叠加偏置向量，它会自动 broadcasting 为[b,h&#x27;,w&#x27;,cout]</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = out + b</span><br></pre></td></tr></table></figure>
<h4 id="2卷积层类"><a class="markdownIt-Anchor" href="#2卷积层类"></a> 2.卷积层类</h4>
<p>通过卷积层类<code>layers.Conv2D</code>可以直接调用类实例完成卷积层的前向计算（TensorFlow中，API的首字母大写的对象一般表示类，全部小写的一般表示函数）。使用类方式会自动创建（在创建类时或build时）需要的权值张量和偏置向量等， 用户不需要记忆卷积核张量的定义格式</p>
<p>在新建卷积层类时，只需要指定卷积核数量参数<code>filters</code>，卷积核大小<code>kernel_size</code>， 步长<code>strides</code>，填充 <code>padding</code> 等即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建了 4 个3 × 3大小的卷积核的卷积层，步长为 1，padding 方案为&#x27;SAME&#x27;</span></span><br><span class="line">layer = layers.Conv2D(<span class="number">4</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>,padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>如果卷积核高宽不等，步长行列方向不等，此时需要将kernel_size参数设计为元组格式<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>h</mi></msub><mo separator="true">,</mo><msub><mi>k</mi><mi>w</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(k_h,k_w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，strides参数设计为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>h</mi></msub><mo separator="true">,</mo><msub><mi>s</mi><mi>w</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(s_h,s_w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 4 个3 × 4大小的卷积核，竖直方向移动步长𝑠ℎ = 2，水平方向移动步长𝑠𝑤 = 1：</span></span><br><span class="line">layer = layers.Conv2D(<span class="number">4</span>,kernel_size=(<span class="number">3</span>,<span class="number">4</span>),strides=(<span class="number">2</span>,<span class="number">1</span>),padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>创建完成后，通过调用实例(的<code>__call__</code>方法)即可完成前向计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建卷积层类</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>layer = layers.Conv2D(<span class="number">4</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>,padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = layer(x) <span class="comment"># 前向计算</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out.shape <span class="comment"># 输出张量的 shape</span></span><br><span class="line">TensorShape([<span class="number">2</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<p>在类<code>Conv2D</code>中，可以通过类成员<code>trainable_variables</code>直接返回𝑾和𝒃的列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回所有待优化张量列表</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>layer.trainable_variables</span><br><span class="line">[&lt;tf.Variable <span class="string">&#x27;conv2d/kernel:0&#x27;</span> shape=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>) dtype=float32, numpy=</span><br><span class="line"> array([[[[ <span class="number">0.13485974</span>, <span class="number">-0.22861657</span>, <span class="number">0.01000655</span>, <span class="number">0.11988598</span>],</span><br><span class="line">          [ <span class="number">0.12811887</span>, <span class="number">0.20501086</span>, <span class="number">-0.29820845</span>, <span class="number">-0.19579397</span>],</span><br><span class="line">          [ <span class="number">0.00858489</span>, <span class="number">-0.24469738</span>, <span class="number">-0.08591779</span>, <span class="number">-0.27885547</span>]], ...</span><br><span class="line"> &lt;tf.Variable <span class="string">&#x27;conv2d/bias:0&#x27;</span> shape=(<span class="number">4</span>,) dtype=float32, numpy=array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>], dtype=float32)&gt;]</span><br></pre></td></tr></table></figure>
<ul>
<li>可以直接调用类实例<code>layer.kernel</code>、<code>layer.bias</code>名访问𝑾和𝒃张量</li>
</ul>
<h3 id="4-lenet-5实战"><a class="markdownIt-Anchor" href="#4-lenet-5实战"></a> 4、LeNet-5实战</h3>
<p>1990 年代， Yann LeCun 等人提出了用于手写数字和机器打印字符图片识别的神经网络，被命名为 LeNet-5 [4]。 LeNet-5 的提出，使得卷积神经网络在当时能够成功被商用，广泛应用在邮政编码、支票号码识别等任务中</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200513020542762.png" alt="LeNet-5网络结构"></p>
<ul>
<li>接受32*32大小的数字、字符图片，经过第一个卷积层得到[b,28,28,6]形状的张量，经过一个向下采样层，张量尺寸缩小到[b,14,14,6]</li>
<li>经过第二个卷积层，得到[b,10,10,16]形状的张量，同样经过下采样层，张量尺寸缩小到[b,5,5,16]</li>
<li>在进入全连接层之前，先将张量打成[b,400]的张量</li>
<li>送入输出节点数分别为120、84的2个全连接层，得到[b,84]的张量</li>
<li>最后通过Gaussian connections层</li>
</ul>
<p>在上述基础上进行少许调整，使得它更容易在现代深度学习框架上实现：</p>
<p><img src="/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/image-20200513021054068.png" alt="调整后的网络结构"></p>
<ul>
<li>将输入𝑿形状由32 × 32调整为28 × 28</li>
<li>将 2 个下采样层实现为最大池化层(降低特征图的高、宽，后续会介绍)</li>
<li>利用全连接层替换掉Gaussian connections层</li>
</ul>
<p>加载MNIST数据集：进阶操作 -&gt; 7、经典数据集加载</p>
<p>创建网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Sequential</span><br><span class="line">network = Sequential([ <span class="comment"># 网络容器</span></span><br><span class="line">    layers.Conv2D(<span class="number">6</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>), <span class="comment"># 第一个卷积层, 6 个 3x3 卷积核</span></span><br><span class="line">    layers.MaxPooling2D(pool_size=<span class="number">2</span>,strides=<span class="number">2</span>), <span class="comment"># 高宽各减半的池化层</span></span><br><span class="line">    layers.ReLU(), <span class="comment"># 激活函数</span></span><br><span class="line">    layers.Conv2D(<span class="number">16</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>), <span class="comment"># 第二个卷积层, 16 个 3x3 卷积核</span></span><br><span class="line">    layers.MaxPooling2D(pool_size=<span class="number">2</span>,strides=<span class="number">2</span>), <span class="comment"># 高宽各减半的池化层</span></span><br><span class="line">    layers.ReLU(), <span class="comment"># 激活函数</span></span><br><span class="line">    layers.Flatten(), <span class="comment"># 打平层，方便全连接层处理</span></span><br><span class="line">    layers.Dense(<span class="number">120</span>, activation=<span class="string">&#x27;relu&#x27;</span>), <span class="comment"># 全连接层， 120 个节点</span></span><br><span class="line">    layers.Dense(<span class="number">84</span>, activation=<span class="string">&#x27;relu&#x27;</span>), <span class="comment"># 全连接层， 84 节点</span></span><br><span class="line">    layers.Dense(<span class="number">10</span>) <span class="comment"># 全连接层， 10 个节点</span></span><br><span class="line">])</span><br><span class="line"><span class="comment"># build 一次网络模型，给输入 X 的形状，其中 4 为随意给的 batchsz</span></span><br><span class="line">network.build(input_shape=(<span class="number">4</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 统计网络信息</span></span><br><span class="line">network.summary()</span><br></pre></td></tr></table></figure>
<p>网络信息如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Model: <span class="string">&quot;sequential&quot;</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line">conv2d (Conv2D)              multiple                  <span class="number">60</span>        </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d (MaxPooling2D) multiple                  <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">re_lu (ReLU)                 multiple                  <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_1 (Conv2D)            multiple                  <span class="number">880</span>       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d_1 (MaxPooling2 multiple                  <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">re_lu_1 (ReLU)               multiple                  <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten (Flatten)            multiple                  <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                multiple                  <span class="number">48120</span>     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              multiple                  <span class="number">10164</span>     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_2 (Dense)              multiple                  <span class="number">850</span>       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">60</span>,<span class="number">074</span></span><br><span class="line">Trainable params: <span class="number">60</span>,<span class="number">074</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>
<ul>
<li>显著降低网络参数量，同时增加网络深度</li>
</ul>
<h3 id="5-表示学习"><a class="markdownIt-Anchor" href="#5-表示学习"></a> 5、表示学习</h3>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">spaceman</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://nu-ll.github.io/2020/01/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow2/">http://nu-ll.github.io/2020/01/03/深度学习与TensorFlow2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://NU-LL.github.io" target="_blank">spaceman</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/TensorFlow2/">TensorFlow2</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/01/10/python/"><img class="prev-cover" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Python</div></div></a></div><div class="next-post pull-right"><a href="/2019/11/12/%E5%9F%BA%E4%BA%8ESTM32L476%E7%9A%84IAP%E5%8D%87%E7%BA%A7/"><img class="next-cover" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">基于STM32L476的IAP升级</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/05/03/TensorFlow2/" title="TensorFlow2入门与实践"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2020-05-03</div><div class="title">TensorFlow2入门与实践</div></div></a></div></div></div></article></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By spaceman</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><script type="text/javascript" src="https://api.uixsj.cn/hitokoto/w.php?code=js"></script><div id="xsjhitokoto"><script>xsjhitokoto()</script></div> <iframe scrolling="no" src="https://tianqiapi.com/api.php?style=tx&color=eee" frameborder="0" allowtransparency="false" align="middle" height="20"></iframe></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> {preloader.endLoading()})</script><div class="js-pjax"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" async="async" mobile="false"></script></div></body></html>